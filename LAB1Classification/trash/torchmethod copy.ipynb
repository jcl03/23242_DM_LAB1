{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Load the dataset\n",
    "url = 'https://p19-bot-sign-sg.ciciai.com/tos-alisg-i-b2l6bve69y-sg/ca602a4dd2e04e6ea717f6548ca51fa6.csv~tplv-b2l6bve69y-image.image?rk3s=68e6b6b5&x-expires=1719744177&x-signature=sdxk9fyq5FsGMaG9rN5e5sU%2FFhk%3D'\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Handling Missing Values\n",
    "data.fillna(data.mean(), inplace=True)\n",
    "\n",
    "# Encoding Categorical Variables\n",
    "label_encoder = LabelEncoder()\n",
    "data['Sex'] = label_encoder.fit_transform(data['Sex'])\n",
    "\n",
    "# Normalizing Numerical Features\n",
    "numerical_cols = data.columns\n",
    "scaler = StandardScaler()\n",
    "data[numerical_cols] = scaler.fit_transform(data[numerical_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#load data into gpu\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data = data.to(device)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "\n",
    "X = data.drop(columns=['Diabetes'])\n",
    "y = data['Diabetes']\n",
    "y = y.apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***LOGISTIC REGRESSION***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.float32).view(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the model (logistic regresion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the Logistic Regression model using PyTorch\n",
    "#running on gpu\n",
    "\n",
    "\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "# Initialize the model, loss function, and optimizer\n",
    "input_dim = X.shape[1]\n",
    "lr_model = LogisticRegressionModel(input_dim)\n",
    "lr_model.to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(lr_model.parameters(), lr=0.01)\n",
    "\n",
    "# Define number of epochs\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def train_model(model, optimizer, criterion, X_train, y_train):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test)\n",
    "        predicted = outputs.round()\n",
    "        y_test_numpy = y_test.numpy()\n",
    "        predicted_numpy = predicted.numpy()\n",
    "        y_prob = outputs.numpy()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = accuracy_score(y_test_numpy, predicted_numpy)\n",
    "        \n",
    "        # Calculate precision\n",
    "        precision = precision_score(y_test_numpy, predicted_numpy)\n",
    "        \n",
    "        # Calculate sensitivity (recall)\n",
    "        recall = recall_score(y_test_numpy, predicted_numpy)\n",
    "        \n",
    "        # Calculate F1 score\n",
    "        f1 = f1_score(y_test_numpy, predicted_numpy)\n",
    "        \n",
    "        # Calculate confusion matrix\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test_numpy, predicted_numpy).ravel()\n",
    "\n",
    "        # Calculate ROC AUC\n",
    "        roc_auc = roc_auc_score(y_test_numpy, y_prob)\n",
    "        \n",
    "        return accuracy, precision, recall, f1, roc_auc, tn, fp, fn, tp\n",
    "\n",
    "# Perform 100 evaluations using bootstrapping\n",
    "num_evaluations = 100\n",
    "metrics = []\n",
    "best_accuracy = 0\n",
    "best_model_state = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.6818\n",
      "Average Precision: 0.6939\n",
      "Average Sensitivity (Recall): 0.6545\n",
      "Average F1 Score: 0.6707\n",
      "Average ROC AUC: 0.7502\n",
      "Average Confusion Matrix: TP=4624, TN=5016, FP=2058, FN=2441\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "\n",
    "# Perform 100 evaluations using bootstrapping\n",
    "num_evaluations = 100\n",
    "metrics = []\n",
    "\n",
    "for i in range(num_evaluations):\n",
    "    # Bootstrap sampling\n",
    "    X_resampled, y_resampled = resample(X_tensor, y_tensor, n_samples=len(X_tensor), random_state=None)\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=None)\n",
    "    \n",
    "    # Reinitialize the model parameters for each iteration\n",
    "    lr_model = LogisticRegressionModel(input_dim)\n",
    "    optimizer = optim.SGD(lr_model.parameters(), lr=0.01)\n",
    "    \n",
    "    # Train the model\n",
    "    train_model(lr_model, optimizer, criterion, X_train, y_train)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    metrics.append(evaluate_model(lr_model, X_test, y_test))\n",
    "    \n",
    "    # Save the model\n",
    "    torch.save(lr_model.state_dict(), f'logistic_regression_model_{i}.pth')\n",
    "\n",
    "# Calculate average metrics\n",
    "metrics_array = np.array(metrics)\n",
    "mean_metrics = np.mean(metrics_array, axis=0)\n",
    "\n",
    "# Print the average results\n",
    "print(f'Average Accuracy: {mean_metrics[0]:.4f}')\n",
    "print(f'Average Precision: {mean_metrics[1]:.4f}')\n",
    "print(f'Average Sensitivity (Recall): {mean_metrics[2]:.4f}')\n",
    "print(f'Average F1 Score: {mean_metrics[3]:.4f}')\n",
    "print(f'Average ROC AUC: {mean_metrics[4]:.4f}')\n",
    "print(f'Average Confusion Matrix: TP={mean_metrics[8]:.0f}, TN={mean_metrics[5]:.0f}, FP={mean_metrics[6]:.0f}, FN={mean_metrics[7]:.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.6769 (95% CI: 0.6225 - 0.7153)\n",
      "Average Precision: 0.6883 (95% CI: 0.6299 - 0.7364)\n",
      "Average Sensitivity (Recall): 0.6531 (95% CI: 0.4968 - 0.8094)\n",
      "Average F1 Score: 0.6663 (95% CI: 0.5717 - 0.7351)\n",
      "Average ROC AUC: 0.7463 (95% CI: 0.6894 - 0.7897)\n",
      "Average Confusion Matrix: TP=4614 (95% CI: 3535 - 5749), TN=4957 (95% CI: 4091 - 5711), FP=2116 (95% CI: 1416 - 3019), FN=2451 (95% CI: 1353 - 3586)\n"
     ]
    }
   ],
   "source": [
    "# Calculate mean and 95% confidence intervals for each metric\n",
    "mean_metrics = np.mean(metrics_array, axis=0)\n",
    "ci_lower = np.percentile(metrics_array, 2.5, axis=0)\n",
    "ci_upper = np.percentile(metrics_array, 97.5, axis=0)\n",
    "\n",
    "# Print the average results with confidence intervals\n",
    "print(f'Average Accuracy: {mean_metrics[0]:.4f} (95% CI: {ci_lower[0]:.4f} - {ci_upper[0]:.4f})')\n",
    "print(f'Average Precision: {mean_metrics[1]:.4f} (95% CI: {ci_lower[1]:.4f} - {ci_upper[1]:.4f})')\n",
    "print(f'Average Sensitivity (Recall): {mean_metrics[2]:.4f} (95% CI: {ci_lower[2]:.4f} - {ci_upper[2]:.4f})')\n",
    "print(f'Average F1 Score: {mean_metrics[3]:.4f} (95% CI: {ci_lower[3]:.4f} - {ci_upper[3]:.4f})')\n",
    "print(f'Average ROC AUC: {mean_metrics[4]:.4f} (95% CI: {ci_lower[4]:.4f} - {ci_upper[4]:.4f})')\n",
    "print(f'Average Confusion Matrix: TP={mean_metrics[8]:.0f} (95% CI: {ci_lower[8]:.0f} - {ci_upper[8]:.0f}), TN={mean_metrics[5]:.0f} (95% CI: {ci_lower[5]:.0f} - {ci_upper[5]:.0f}), FP={mean_metrics[6]:.0f} (95% CI: {ci_lower[6]:.0f} - {ci_upper[6]:.0f}), FN={mean_metrics[7]:.0f} (95% CI: {ci_lower[7]:.0f} - {ci_upper[7]:.0f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***SVM MODEL***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVMModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SVMModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "# Initialize the models, loss functions, and optimizers\n",
    "input_dim = X_train.shape[1]\n",
    "svm_model = SVMModel(input_dim)\n",
    "criterion_svm = nn.HingeEmbeddingLoss()\n",
    "optimizer_svm = optim.SGD(svm_model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m svm_model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      4\u001b[0m optimizer_svm\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m----> 5\u001b[0m outputs \u001b[38;5;241m=\u001b[39m svm_model(\u001b[43mX_train_tensor\u001b[49m)\n\u001b[0;32m      6\u001b[0m outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msign(outputs)\n\u001b[0;32m      7\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion_svm(outputs, \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m y_train_tensor \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Convert 0/1 to -1/1\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_tensor' is not defined"
     ]
    }
   ],
   "source": [
    "# Training the SVM model\n",
    "for epoch in range(num_epochs):\n",
    "    svm_model.train()\n",
    "    optimizer_svm.zero_grad()\n",
    "    outputs = svm_model(X_train_tensor)\n",
    "    outputs = torch.sign(outputs)\n",
    "    loss = criterion_svm(outputs, 2 * y_train_tensor - 1)  # Convert 0/1 to -1/1\n",
    "    loss.backward()\n",
    "    optimizer_svm.step()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'SVM - Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "# Save the model\n",
    "torch.save(svm_model.state_dict(), 'svm_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evalutaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM - Accuracy: 0.4611\n",
      "SVM - Precision: 0.4412\n",
      "SVM - Sensitivity (Recall): 0.3040\n",
      "SVM - F1 Score: 0.3600\n",
      "SVM - Confusion Matrix: TP=2143, TN=4376, FP=2714, FN=4906\n",
      "SVM - ROC AUC: 0.4209\n"
     ]
    }
   ],
   "source": [
    "svm_model.eval()\n",
    "with torch.no_grad():\n",
    "    svm_outputs = svm_model(X_test_tensor)\n",
    "    svm_pred = (svm_outputs >= 0).float().numpy().flatten()  # Convert to binary 0 and 1\n",
    "    svm_prob = (svm_outputs - svm_outputs.min()) / (svm_outputs.max() - svm_outputs.min())  # Normalize to [0, 1]\n",
    "    svm_prob = svm_prob.numpy().flatten()\n",
    "\n",
    "def evaluate_model(y_test, y_pred, y_prob):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_prob)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    return accuracy, precision, recall, f1, roc_auc, tn, fp, fn, tp\n",
    "\n",
    "# Evaluate the SVM model\n",
    "svm_accuracy, svm_precision, svm_recall, svm_f1, svm_roc_auc, svm_tn, svm_fp, svm_fn, svm_tp = evaluate_model(y_test, svm_pred, svm_prob)\n",
    "\n",
    "# Print the results\n",
    "print(f'SVM - Accuracy: {svm_accuracy:.4f}')\n",
    "print(f'SVM - Precision: {svm_precision:.4f}')\n",
    "print(f'SVM - Sensitivity (Recall): {svm_recall:.4f}')\n",
    "print(f'SVM - F1 Score: {svm_f1:.4f}')\n",
    "print(f'SVM - Confusion Matrix: TP={svm_tp}, TN={svm_tn}, FP={svm_fp}, FN={svm_fn}')\n",
    "print(f'SVM - ROC AUC: {svm_roc_auc:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***compare 2 model***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Cross-Validation Scores: [0.73810029 0.74757762 0.74713538 0.75166219 0.75060122]\n",
      "SVM Cross-Validation Scores: [0.74121225 0.74658745 0.74840854 0.75194511 0.74968171]\n"
     ]
    }
   ],
   "source": [
    "#perform cross validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "# Define a function to perform cross-validation\n",
    "def cross_validate(model, X, y, cv=5):\n",
    "    scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
    "    return scores\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Perform cross-validation for Logistic Regression\n",
    "lr_scores = cross_validate(LogisticRegression(), X, y)\n",
    "print(f'Logistic Regression Cross-Validation Scores: {lr_scores}')\n",
    "\n",
    "# Perform cross-validation for SVM\n",
    "svm_scores = cross_validate(SVC(), X, y)\n",
    "print(f'SVM Cross-Validation Scores: {svm_scores}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[128], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m     margin_of_error \u001b[38;5;241m=\u001b[39m se \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1.96\u001b[39m  \u001b[38;5;66;03m# For 95% confidence interval\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mean, margin_of_error\n\u001b[1;32m---> 14\u001b[0m logistic_ci \u001b[38;5;241m=\u001b[39m {metric: \u001b[43mconfidence_interval\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlogistic_metrics\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i, metric \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroc_auc\u001b[39m\u001b[38;5;124m'\u001b[39m])}\n\u001b[0;32m     15\u001b[0m svm_ci \u001b[38;5;241m=\u001b[39m {metric: confidence_interval([svm_metrics[i]]) \u001b[38;5;28;01mfor\u001b[39;00m i, metric \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroc_auc\u001b[39m\u001b[38;5;124m'\u001b[39m])}\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogistic Regression Confidence Intervals:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[128], line 9\u001b[0m, in \u001b[0;36mconfidence_interval\u001b[1;34m(data, confidence)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconfidence_interval\u001b[39m(data, confidence\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m):\n\u001b[0;32m      8\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data)\n\u001b[1;32m----> 9\u001b[0m     mean \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mmean(data)\n\u001b[0;32m     10\u001b[0m     se \u001b[38;5;241m=\u001b[39m sem(data)\n\u001b[0;32m     11\u001b[0m     margin_of_error \u001b[38;5;241m=\u001b[39m se \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1.96\u001b[39m  \u001b[38;5;66;03m# For 95% confidence interval\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Confidence Intervals\n",
    "from scipy.stats import sem\n",
    "import numpy as np\n",
    "\n",
    "logistic_metrics = [lr_accuracy, lr_precision, lr_recall, lr_f1, lr_roc_auc, lr_tn, lr_fp, lr_fn, lr_tp]\n",
    "svm_metrics = [svm_accuracy, svm_precision, svm_recall, svm_f1, svm_roc_auc, svm_tn, svm_fp, svm_fn, svm_tp]\n",
    "\n",
    "def confidence_interval(data, confidence=0.95):\n",
    "    n = len(data)\n",
    "    mean = np.mean(data)\n",
    "    se = sem(data)\n",
    "    margin_of_error = se * 1.96  # For 95% confidence interval\n",
    "    return mean, margin_of_error\n",
    "\n",
    "logistic_ci = {metric: confidence_interval([logistic_metrics[i]]) for i, metric in enumerate(['accuracy', 'precision', 'recall', 'f1', 'roc_auc'])}\n",
    "svm_ci = {metric: confidence_interval([svm_metrics[i]]) for i, metric in enumerate(['accuracy', 'precision', 'recall', 'f1', 'roc_auc'])}\n",
    "\n",
    "print('Logistic Regression Confidence Intervals:')\n",
    "print(logistic_ci)\n",
    "print('SVM Confidence Intervals:')\n",
    "print(svm_ci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "lr_prob = lr_model(X_test_tensor).detach().numpy().flatten()\n",
    "\n",
    "\n",
    "\n",
    "logistic_fpr, logistic_tpr, _ = roc_curve(y_test, lr_prob)\n",
    "svm_fpr, svm_tpr, _ = roc_curve(y_test, svm_prob)\n",
    "\n",
    "# Plot ROC Curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(logistic_fpr, logistic_tpr, label=f'Logistic Regression (AUC = {logistic_metrics[4]:.2f})')\n",
    "plt.plot(svm_fpr, svm_tpr, label=f'SVM (AUC = {svm_metrics[4]:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performnce Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logistic_metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Assuming logistic_metrics, logistic_ci, svm_metrics, svm_ci are predefined\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Creating the DataFrame\u001b[39;00m\n\u001b[0;32m      7\u001b[0m comparison_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMetric\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrecision\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSensitivity (Recall)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF1 Score\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mROC AUC\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrue Positive (TP)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrue Negative (TN)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFalse Positive (FP)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFalse Negative (FN)\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogistic Regression\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mlogistic_metrics\u001b[49m,\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogistic Regression CI\u001b[39m\u001b[38;5;124m'\u001b[39m: [logistic_ci[metric] \u001b[38;5;28;01mfor\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroc_auc\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m+\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSVM\u001b[39m\u001b[38;5;124m'\u001b[39m: svm_metrics,\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSVM CI\u001b[39m\u001b[38;5;124m'\u001b[39m: [svm_ci[metric] \u001b[38;5;28;01mfor\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroc_auc\u001b[39m\u001b[38;5;124m'\u001b[39m]]  \u001b[38;5;241m+\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m4\u001b[39m\n\u001b[0;32m     13\u001b[0m })\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Set the Metric column as the index for better readability\u001b[39;00m\n\u001b[0;32m     16\u001b[0m comparison_df\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMetric\u001b[39m\u001b[38;5;124m'\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'logistic_metrics' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming logistic_metrics, logistic_ci, svm_metrics, svm_ci are predefined\n",
    "\n",
    "\n",
    "# Creating the DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Sensitivity (Recall)', 'F1 Score', 'ROC AUC', 'True Positive (TP)', 'True Negative (TN)', 'False Positive (FP)', 'False Negative (FN)'],\n",
    "    'Logistic Regression': logistic_metrics,\n",
    "    'Logistic Regression CI': [logistic_ci[metric] for metric in ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']] + [None]*4,\n",
    "    'SVM': svm_metrics,\n",
    "    'SVM CI': [svm_ci[metric] for metric in ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']]  + [None]*4\n",
    "})\n",
    "\n",
    "# Set the Metric column as the index for better readability\n",
    "comparison_df.set_index('Metric', inplace=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(comparison_df.to_markdown())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Iteration 1/100\n",
      "X_train shape: torch.Size([56553, 17]), y_train shape: torch.Size([56553, 1])\n",
      "X_test shape: torch.Size([14139, 17]), y_test shape: torch.Size([14139, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "all elements of target should be between 0 and 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 117\u001b[0m\n\u001b[0;32m    114\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(lr_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m    120\u001b[0m metrics\u001b[38;5;241m.\u001b[39mappend(evaluate_model(lr_model, X_test, y_test))\n",
      "Cell \u001b[1;32mIn[4], line 61\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, optimizer, criterion, X_train, y_train)\u001b[0m\n\u001b[0;32m     59\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     60\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(X_train)\n\u001b[1;32m---> 61\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Debug statement\u001b[39;00m\n\u001b[0;32m     63\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\jiach\\.conda\\envs\\env1\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jiach\\.conda\\envs\\env1\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jiach\\.conda\\envs\\env1\\lib\\site-packages\\torch\\nn\\modules\\loss.py:618\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jiach\\.conda\\envs\\env1\\lib\\site-packages\\torch\\nn\\functional.py:3154\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3151\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m   3152\u001b[0m     weight \u001b[38;5;241m=\u001b[39m weight\u001b[38;5;241m.\u001b[39mexpand(new_size)\n\u001b[1;32m-> 3154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_enum\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: all elements of target should be between 0 and 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Check if CUDA is available\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load the dataset\n",
    "url = 'https://p19-bot-sign-sg.ciciai.com/tos-alisg-i-b2l6bve69y-sg/ca602a4dd2e04e6ea717f6548ca51fa6.csv~tplv-b2l6bve69y-image.image?rk3s=68e6b6b5&x-expires=1719744177&x-signature=sdxk9fyq5FsGMaG9rN5e5sU%2FFhk%3D'\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Handling Missing Values\n",
    "data.fillna(data.mean(), inplace=True)\n",
    "\n",
    "# Encoding Categorical Variables\n",
    "label_encoder = LabelEncoder()\n",
    "data['Sex'] = label_encoder.fit_transform(data['Sex'])\n",
    "\n",
    "# Normalizing Numerical Features\n",
    "numerical_cols = data.columns\n",
    "scaler = StandardScaler()\n",
    "data[numerical_cols] = scaler.fit_transform(data[numerical_cols])\n",
    "\n",
    "# Set the device to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Convert the data to PyTorch tensors and move them to the GPU\n",
    "X = torch.tensor(data.drop(columns=['Diabetes']).values, dtype=torch.float32).to(device)\n",
    "y = torch.tensor(data['Diabetes'].values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "# Define the Logistic Regression model using PyTorch\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "input_dim = X.shape[1]\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Define number of epochs\n",
    "num_epochs = 100\n",
    "\n",
    "# Train the model\n",
    "def train_model(model, optimizer, criterion, X_train, y_train):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')  # Debug statement\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test)\n",
    "        predicted = outputs.round()\n",
    "        y_test_numpy = y_test.cpu().numpy()  # Move to CPU for evaluation\n",
    "        predicted_numpy = predicted.cpu().numpy()  # Move to CPU for evaluation\n",
    "        y_prob = outputs.cpu().numpy()  # Move to CPU for evaluation\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = accuracy_score(y_test_numpy, predicted_numpy)\n",
    "        \n",
    "        # Calculate precision\n",
    "        precision = precision_score(y_test_numpy, predicted_numpy)\n",
    "        \n",
    "        # Calculate sensitivity (recall)\n",
    "        recall = recall_score(y_test_numpy, predicted_numpy)\n",
    "        \n",
    "        # Calculate F1 score\n",
    "        f1 = f1_score(y_test_numpy, predicted_numpy)\n",
    "        \n",
    "        # Calculate confusion matrix\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test_numpy, predicted_numpy).ravel()\n",
    "\n",
    "        # Calculate ROC AUC\n",
    "        roc_auc = roc_auc_score(y_test_numpy, y_prob)\n",
    "        \n",
    "        return accuracy, precision, recall, f1, roc_auc, tn, fp, fn, tp\n",
    "\n",
    "# Perform 100 evaluations using bootstrapping\n",
    "num_evaluations = 100\n",
    "metrics = []\n",
    "\n",
    "for i in range(num_evaluations):\n",
    "    # Bootstrap sampling\n",
    "    X_resampled, y_resampled = resample(X, y, n_samples=len(X), random_state=None)\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=None)\n",
    "    \n",
    "    # Debug statements to check data\n",
    "    print(f'Iteration {i+1}/{num_evaluations}')\n",
    "    print(f'X_train shape: {X_train.shape}, y_train shape: {y_train.shape}')\n",
    "    print(f'X_test shape: {X_test.shape}, y_test shape: {y_test.shape}')\n",
    "    \n",
    "    # Reinitialize the model parameters for each iteration\n",
    "    lr_model = LogisticRegressionModel(input_dim).to(device)\n",
    "    optimizer = optim.SGD(lr_model.parameters(), lr=0.01)\n",
    "    \n",
    "    # Train the model\n",
    "    train_model(lr_model, optimizer, criterion, X_train, y_train)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    metrics.append(evaluate_model(lr_model, X_test, y_test))\n",
    "    \n",
    "    # Save the model\n",
    "    torch.save(lr_model.state_dict(), f'logistic_regression_model_{i}.pth')\n",
    "\n",
    "# Calculate average metrics\n",
    "metrics_array = np.array(metrics)\n",
    "mean_metrics = np.mean(metrics_array, axis=0)\n",
    "\n",
    "# Print the average results\n",
    "print(f'Average Accuracy: {mean_metrics[0]:.4f}')\n",
    "print(f'Average Precision: {mean_metrics[1]:.4f}')\n",
    "print(f'Average Sensitivity (Recall): {mean_metrics[2]:.4f}')\n",
    "print(f'Average F1 Score: {mean_metrics[3]:.4f}')\n",
    "print(f'Average ROC AUC: {mean_metrics[4]:.4f}')\n",
    "print(f'Average Confusion Matrix: TP={mean_metrics[8]:.0f}, TN={mean_metrics[5]:.0f}, FP={mean_metrics[6]:.0f}, FN={mean_metrics[7]:.0f}')\n",
    "\n",
    "# Calculate mean and 95% confidence intervals for each metric\n",
    "ci_lower = np.percentile(metrics_array, 2.5, axis=0)\n",
    "ci_upper = np.percentile(metrics_array, 97.5, axis=0)\n",
    "\n",
    "# Print the average results with confidence intervals\n",
    "print(f'Average Accuracy: {mean_metrics[0]:.4f} (95% CI: {ci_lower[0]:.4f} - {ci_upper[0]:.4f})')\n",
    "print(f'Average Precision: {mean_metrics[1]:.4f} (95% CI: {ci_lower[1]:.4f} - {ci_upper[1]:.4f})')\n",
    "print(f'Average Sensitivity (Recall): {mean_metrics[2]:.4f} (95% CI: {ci_lower[2]:.4f} - {ci_upper[2]:.4f})')\n",
    "print(f'Average F1 Score: {mean_metrics[3]:.4f} (95% CI: {ci_lower[3]:.4f} - {ci_upper[3]:.4f})')\n",
    "print(f'Average ROC AUC: {mean_metrics[4]:.4f} (95% CI: {ci_lower[4]:.4f} - {ci_upper[4]:.4f})')\n",
    "print(f'Average Confusion Matrix: TP={mean_metrics[8]:.0f} (95% CI: {ci_lower[8]:.0f} - {ci_upper[8]:.0f}), TN={mean_metrics[5]:.0f} (95% CI: {ci_lower[5]:.0f} - {ci_upper[5]:.0f}), FP={mean_metrics[6]:.0f} (95% CI: {ci_lower[6]:.0f} - {ci_upper[6]:.0f}), FN={mean_metrics[7]:.0f} (95% CI: {ci_lower[7]:.0f} - {ci_upper[7]:.0f})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.7392\n",
      "Epoch [20/100], Loss: 0.6562\n",
      "Epoch [30/100], Loss: 0.6066\n",
      "Epoch [40/100], Loss: 0.5753\n",
      "Epoch [50/100], Loss: 0.5558\n",
      "Epoch [60/100], Loss: 0.5432\n",
      "Epoch [70/100], Loss: 0.5350\n",
      "Epoch [80/100], Loss: 0.5296\n",
      "Epoch [90/100], Loss: 0.5260\n",
      "Epoch [100/100], Loss: 0.5236\n",
      "Epoch [10/100], Loss: 0.7476\n",
      "Epoch [20/100], Loss: 0.6613\n",
      "Epoch [30/100], Loss: 0.6112\n",
      "Epoch [40/100], Loss: 0.5799\n",
      "Epoch [50/100], Loss: 0.5592\n",
      "Epoch [60/100], Loss: 0.5451\n",
      "Epoch [70/100], Loss: 0.5355\n",
      "Epoch [80/100], Loss: 0.5290\n",
      "Epoch [90/100], Loss: 0.5248\n",
      "Epoch [100/100], Loss: 0.5220\n",
      "Epoch [10/100], Loss: 0.8490\n",
      "Epoch [20/100], Loss: 0.7287\n",
      "Epoch [30/100], Loss: 0.6531\n",
      "Epoch [40/100], Loss: 0.6063\n",
      "Epoch [50/100], Loss: 0.5770\n",
      "Epoch [60/100], Loss: 0.5586\n",
      "Epoch [70/100], Loss: 0.5470\n",
      "Epoch [80/100], Loss: 0.5393\n",
      "Epoch [90/100], Loss: 0.5342\n",
      "Epoch [100/100], Loss: 0.5304\n",
      "Epoch [10/100], Loss: 1.1575\n",
      "Epoch [20/100], Loss: 0.9439\n",
      "Epoch [30/100], Loss: 0.7923\n",
      "Epoch [40/100], Loss: 0.6904\n",
      "Epoch [50/100], Loss: 0.6256\n",
      "Epoch [60/100], Loss: 0.5869\n",
      "Epoch [70/100], Loss: 0.5640\n",
      "Epoch [80/100], Loss: 0.5500\n",
      "Epoch [90/100], Loss: 0.5410\n",
      "Epoch [100/100], Loss: 0.5348\n",
      "Epoch [10/100], Loss: 0.6682\n",
      "Epoch [20/100], Loss: 0.6172\n",
      "Epoch [30/100], Loss: 0.5826\n",
      "Epoch [40/100], Loss: 0.5597\n",
      "Epoch [50/100], Loss: 0.5441\n",
      "Epoch [60/100], Loss: 0.5340\n",
      "Epoch [70/100], Loss: 0.5275\n",
      "Epoch [80/100], Loss: 0.5233\n",
      "Epoch [90/100], Loss: 0.5205\n",
      "Epoch [100/100], Loss: 0.5185\n",
      "Epoch [10/100], Loss: 0.6268\n",
      "Epoch [20/100], Loss: 0.5827\n",
      "Epoch [30/100], Loss: 0.5577\n",
      "Epoch [40/100], Loss: 0.5430\n",
      "Epoch [50/100], Loss: 0.5336\n",
      "Epoch [60/100], Loss: 0.5272\n",
      "Epoch [70/100], Loss: 0.5228\n",
      "Epoch [80/100], Loss: 0.5197\n",
      "Epoch [90/100], Loss: 0.5176\n",
      "Epoch [100/100], Loss: 0.5162\n",
      "Epoch [10/100], Loss: 0.6252\n",
      "Epoch [20/100], Loss: 0.5881\n",
      "Epoch [30/100], Loss: 0.5658\n",
      "Epoch [40/100], Loss: 0.5515\n",
      "Epoch [50/100], Loss: 0.5424\n",
      "Epoch [60/100], Loss: 0.5361\n",
      "Epoch [70/100], Loss: 0.5313\n",
      "Epoch [80/100], Loss: 0.5273\n",
      "Epoch [90/100], Loss: 0.5241\n",
      "Epoch [100/100], Loss: 0.5214\n",
      "Epoch [10/100], Loss: 0.6140\n",
      "Epoch [20/100], Loss: 0.5716\n",
      "Epoch [30/100], Loss: 0.5460\n",
      "Epoch [40/100], Loss: 0.5318\n",
      "Epoch [50/100], Loss: 0.5237\n",
      "Epoch [60/100], Loss: 0.5191\n",
      "Epoch [70/100], Loss: 0.5161\n",
      "Epoch [80/100], Loss: 0.5142\n",
      "Epoch [90/100], Loss: 0.5129\n",
      "Epoch [100/100], Loss: 0.5121\n",
      "Epoch [10/100], Loss: 0.9045\n",
      "Epoch [20/100], Loss: 0.7347\n",
      "Epoch [30/100], Loss: 0.6349\n",
      "Epoch [40/100], Loss: 0.5830\n",
      "Epoch [50/100], Loss: 0.5566\n",
      "Epoch [60/100], Loss: 0.5424\n",
      "Epoch [70/100], Loss: 0.5342\n",
      "Epoch [80/100], Loss: 0.5291\n",
      "Epoch [90/100], Loss: 0.5257\n",
      "Epoch [100/100], Loss: 0.5234\n",
      "Epoch [10/100], Loss: 0.6032\n",
      "Epoch [20/100], Loss: 0.5633\n",
      "Epoch [30/100], Loss: 0.5414\n",
      "Epoch [40/100], Loss: 0.5283\n",
      "Epoch [50/100], Loss: 0.5206\n",
      "Epoch [60/100], Loss: 0.5162\n",
      "Epoch [70/100], Loss: 0.5136\n",
      "Epoch [80/100], Loss: 0.5120\n",
      "Epoch [90/100], Loss: 0.5110\n",
      "Epoch [100/100], Loss: 0.5104\n",
      "Epoch [10/100], Loss: 0.6865\n",
      "Epoch [20/100], Loss: 0.6195\n",
      "Epoch [30/100], Loss: 0.5771\n",
      "Epoch [40/100], Loss: 0.5520\n",
      "Epoch [50/100], Loss: 0.5381\n",
      "Epoch [60/100], Loss: 0.5300\n",
      "Epoch [70/100], Loss: 0.5248\n",
      "Epoch [80/100], Loss: 0.5213\n",
      "Epoch [90/100], Loss: 0.5189\n",
      "Epoch [100/100], Loss: 0.5171\n",
      "Epoch [10/100], Loss: 0.7942\n",
      "Epoch [20/100], Loss: 0.6969\n",
      "Epoch [30/100], Loss: 0.6394\n",
      "Epoch [40/100], Loss: 0.6030\n",
      "Epoch [50/100], Loss: 0.5780\n",
      "Epoch [60/100], Loss: 0.5601\n",
      "Epoch [70/100], Loss: 0.5473\n",
      "Epoch [80/100], Loss: 0.5384\n",
      "Epoch [90/100], Loss: 0.5321\n",
      "Epoch [100/100], Loss: 0.5276\n",
      "Epoch [10/100], Loss: 0.7946\n",
      "Epoch [20/100], Loss: 0.6837\n",
      "Epoch [30/100], Loss: 0.6239\n",
      "Epoch [40/100], Loss: 0.5890\n",
      "Epoch [50/100], Loss: 0.5662\n",
      "Epoch [60/100], Loss: 0.5509\n",
      "Epoch [70/100], Loss: 0.5404\n",
      "Epoch [80/100], Loss: 0.5330\n",
      "Epoch [90/100], Loss: 0.5278\n",
      "Epoch [100/100], Loss: 0.5240\n",
      "Epoch [10/100], Loss: 0.7049\n",
      "Epoch [20/100], Loss: 0.6257\n",
      "Epoch [30/100], Loss: 0.5784\n",
      "Epoch [40/100], Loss: 0.5516\n",
      "Epoch [50/100], Loss: 0.5367\n",
      "Epoch [60/100], Loss: 0.5281\n",
      "Epoch [70/100], Loss: 0.5229\n",
      "Epoch [80/100], Loss: 0.5195\n",
      "Epoch [90/100], Loss: 0.5173\n",
      "Epoch [100/100], Loss: 0.5158\n",
      "Epoch [10/100], Loss: 0.7449\n",
      "Epoch [20/100], Loss: 0.6666\n",
      "Epoch [30/100], Loss: 0.6208\n",
      "Epoch [40/100], Loss: 0.5922\n",
      "Epoch [50/100], Loss: 0.5739\n",
      "Epoch [60/100], Loss: 0.5617\n",
      "Epoch [70/100], Loss: 0.5528\n",
      "Epoch [80/100], Loss: 0.5456\n",
      "Epoch [90/100], Loss: 0.5396\n",
      "Epoch [100/100], Loss: 0.5348\n",
      "Epoch [10/100], Loss: 0.7070\n",
      "Epoch [20/100], Loss: 0.6241\n",
      "Epoch [30/100], Loss: 0.5785\n",
      "Epoch [40/100], Loss: 0.5522\n",
      "Epoch [50/100], Loss: 0.5373\n",
      "Epoch [60/100], Loss: 0.5287\n",
      "Epoch [70/100], Loss: 0.5233\n",
      "Epoch [80/100], Loss: 0.5198\n",
      "Epoch [90/100], Loss: 0.5175\n",
      "Epoch [100/100], Loss: 0.5161\n",
      "Epoch [10/100], Loss: 0.7671\n",
      "Epoch [20/100], Loss: 0.6827\n",
      "Epoch [30/100], Loss: 0.6220\n",
      "Epoch [40/100], Loss: 0.5805\n",
      "Epoch [50/100], Loss: 0.5544\n",
      "Epoch [60/100], Loss: 0.5388\n",
      "Epoch [70/100], Loss: 0.5298\n",
      "Epoch [80/100], Loss: 0.5246\n",
      "Epoch [90/100], Loss: 0.5214\n",
      "Epoch [100/100], Loss: 0.5193\n",
      "Epoch [10/100], Loss: 0.7317\n",
      "Epoch [20/100], Loss: 0.6465\n",
      "Epoch [30/100], Loss: 0.6044\n",
      "Epoch [40/100], Loss: 0.5806\n",
      "Epoch [50/100], Loss: 0.5650\n",
      "Epoch [60/100], Loss: 0.5539\n",
      "Epoch [70/100], Loss: 0.5454\n",
      "Epoch [80/100], Loss: 0.5389\n",
      "Epoch [90/100], Loss: 0.5338\n",
      "Epoch [100/100], Loss: 0.5299\n",
      "Epoch [10/100], Loss: 1.2330\n",
      "Epoch [20/100], Loss: 0.9736\n",
      "Epoch [30/100], Loss: 0.7898\n",
      "Epoch [40/100], Loss: 0.6811\n",
      "Epoch [50/100], Loss: 0.6228\n",
      "Epoch [60/100], Loss: 0.5903\n",
      "Epoch [70/100], Loss: 0.5704\n",
      "Epoch [80/100], Loss: 0.5572\n",
      "Epoch [90/100], Loss: 0.5478\n",
      "Epoch [100/100], Loss: 0.5409\n",
      "Epoch [10/100], Loss: 0.6121\n",
      "Epoch [20/100], Loss: 0.5594\n",
      "Epoch [30/100], Loss: 0.5341\n",
      "Epoch [40/100], Loss: 0.5214\n",
      "Epoch [50/100], Loss: 0.5153\n",
      "Epoch [60/100], Loss: 0.5124\n",
      "Epoch [70/100], Loss: 0.5111\n",
      "Epoch [80/100], Loss: 0.5106\n",
      "Epoch [90/100], Loss: 0.5103\n",
      "Epoch [100/100], Loss: 0.5103\n",
      "Epoch [10/100], Loss: 0.7071\n",
      "Epoch [20/100], Loss: 0.6219\n",
      "Epoch [30/100], Loss: 0.5763\n",
      "Epoch [40/100], Loss: 0.5536\n",
      "Epoch [50/100], Loss: 0.5415\n",
      "Epoch [60/100], Loss: 0.5339\n",
      "Epoch [70/100], Loss: 0.5287\n",
      "Epoch [80/100], Loss: 0.5249\n",
      "Epoch [90/100], Loss: 0.5221\n",
      "Epoch [100/100], Loss: 0.5200\n",
      "Epoch [10/100], Loss: 0.9755\n",
      "Epoch [20/100], Loss: 0.8267\n",
      "Epoch [30/100], Loss: 0.7250\n",
      "Epoch [40/100], Loss: 0.6566\n",
      "Epoch [50/100], Loss: 0.6116\n",
      "Epoch [60/100], Loss: 0.5818\n",
      "Epoch [70/100], Loss: 0.5613\n",
      "Epoch [80/100], Loss: 0.5470\n",
      "Epoch [90/100], Loss: 0.5369\n",
      "Epoch [100/100], Loss: 0.5298\n",
      "Epoch [10/100], Loss: 0.6833\n",
      "Epoch [20/100], Loss: 0.6104\n",
      "Epoch [30/100], Loss: 0.5746\n",
      "Epoch [40/100], Loss: 0.5521\n",
      "Epoch [50/100], Loss: 0.5367\n",
      "Epoch [60/100], Loss: 0.5266\n",
      "Epoch [70/100], Loss: 0.5204\n",
      "Epoch [80/100], Loss: 0.5168\n",
      "Epoch [90/100], Loss: 0.5147\n",
      "Epoch [100/100], Loss: 0.5135\n",
      "Epoch [10/100], Loss: 0.6633\n",
      "Epoch [20/100], Loss: 0.6059\n",
      "Epoch [30/100], Loss: 0.5779\n",
      "Epoch [40/100], Loss: 0.5595\n",
      "Epoch [50/100], Loss: 0.5460\n",
      "Epoch [60/100], Loss: 0.5364\n",
      "Epoch [70/100], Loss: 0.5292\n",
      "Epoch [80/100], Loss: 0.5240\n",
      "Epoch [90/100], Loss: 0.5203\n",
      "Epoch [100/100], Loss: 0.5176\n",
      "Epoch [10/100], Loss: 0.8820\n",
      "Epoch [20/100], Loss: 0.7401\n",
      "Epoch [30/100], Loss: 0.6474\n",
      "Epoch [40/100], Loss: 0.5924\n",
      "Epoch [50/100], Loss: 0.5617\n",
      "Epoch [60/100], Loss: 0.5442\n",
      "Epoch [70/100], Loss: 0.5334\n",
      "Epoch [80/100], Loss: 0.5266\n",
      "Epoch [90/100], Loss: 0.5220\n",
      "Epoch [100/100], Loss: 0.5190\n",
      "Epoch [10/100], Loss: 0.7002\n",
      "Epoch [20/100], Loss: 0.6375\n",
      "Epoch [30/100], Loss: 0.6015\n",
      "Epoch [40/100], Loss: 0.5780\n",
      "Epoch [50/100], Loss: 0.5622\n",
      "Epoch [60/100], Loss: 0.5510\n",
      "Epoch [70/100], Loss: 0.5424\n",
      "Epoch [80/100], Loss: 0.5357\n",
      "Epoch [90/100], Loss: 0.5305\n",
      "Epoch [100/100], Loss: 0.5265\n",
      "Epoch [10/100], Loss: 0.7290\n",
      "Epoch [20/100], Loss: 0.6543\n",
      "Epoch [30/100], Loss: 0.6138\n",
      "Epoch [40/100], Loss: 0.5877\n",
      "Epoch [50/100], Loss: 0.5693\n",
      "Epoch [60/100], Loss: 0.5555\n",
      "Epoch [70/100], Loss: 0.5449\n",
      "Epoch [80/100], Loss: 0.5368\n",
      "Epoch [90/100], Loss: 0.5307\n",
      "Epoch [100/100], Loss: 0.5261\n",
      "Epoch [10/100], Loss: 0.8215\n",
      "Epoch [20/100], Loss: 0.7131\n",
      "Epoch [30/100], Loss: 0.6431\n",
      "Epoch [40/100], Loss: 0.5966\n",
      "Epoch [50/100], Loss: 0.5683\n",
      "Epoch [60/100], Loss: 0.5516\n",
      "Epoch [70/100], Loss: 0.5407\n",
      "Epoch [80/100], Loss: 0.5332\n",
      "Epoch [90/100], Loss: 0.5278\n",
      "Epoch [100/100], Loss: 0.5238\n",
      "Epoch [10/100], Loss: 0.7038\n",
      "Epoch [20/100], Loss: 0.6217\n",
      "Epoch [30/100], Loss: 0.5753\n",
      "Epoch [40/100], Loss: 0.5501\n",
      "Epoch [50/100], Loss: 0.5364\n",
      "Epoch [60/100], Loss: 0.5289\n",
      "Epoch [70/100], Loss: 0.5243\n",
      "Epoch [80/100], Loss: 0.5211\n",
      "Epoch [90/100], Loss: 0.5188\n",
      "Epoch [100/100], Loss: 0.5170\n",
      "Epoch [10/100], Loss: 0.7110\n",
      "Epoch [20/100], Loss: 0.6299\n",
      "Epoch [30/100], Loss: 0.5819\n",
      "Epoch [40/100], Loss: 0.5555\n",
      "Epoch [50/100], Loss: 0.5408\n",
      "Epoch [60/100], Loss: 0.5317\n",
      "Epoch [70/100], Loss: 0.5260\n",
      "Epoch [80/100], Loss: 0.5223\n",
      "Epoch [90/100], Loss: 0.5199\n",
      "Epoch [100/100], Loss: 0.5183\n",
      "Epoch [10/100], Loss: 1.0484\n",
      "Epoch [20/100], Loss: 0.8448\n",
      "Epoch [30/100], Loss: 0.7174\n",
      "Epoch [40/100], Loss: 0.6482\n",
      "Epoch [50/100], Loss: 0.6098\n",
      "Epoch [60/100], Loss: 0.5856\n",
      "Epoch [70/100], Loss: 0.5685\n",
      "Epoch [80/100], Loss: 0.5558\n",
      "Epoch [90/100], Loss: 0.5462\n",
      "Epoch [100/100], Loss: 0.5388\n",
      "Epoch [10/100], Loss: 0.7777\n",
      "Epoch [20/100], Loss: 0.6743\n",
      "Epoch [30/100], Loss: 0.6133\n",
      "Epoch [40/100], Loss: 0.5789\n",
      "Epoch [50/100], Loss: 0.5580\n",
      "Epoch [60/100], Loss: 0.5444\n",
      "Epoch [70/100], Loss: 0.5351\n",
      "Epoch [80/100], Loss: 0.5288\n",
      "Epoch [90/100], Loss: 0.5246\n",
      "Epoch [100/100], Loss: 0.5217\n",
      "Epoch [10/100], Loss: 0.7057\n",
      "Epoch [20/100], Loss: 0.6271\n",
      "Epoch [30/100], Loss: 0.5907\n",
      "Epoch [40/100], Loss: 0.5702\n",
      "Epoch [50/100], Loss: 0.5555\n",
      "Epoch [60/100], Loss: 0.5445\n",
      "Epoch [70/100], Loss: 0.5365\n",
      "Epoch [80/100], Loss: 0.5308\n",
      "Epoch [90/100], Loss: 0.5267\n",
      "Epoch [100/100], Loss: 0.5237\n",
      "Epoch [10/100], Loss: 0.6626\n",
      "Epoch [20/100], Loss: 0.6049\n",
      "Epoch [30/100], Loss: 0.5774\n",
      "Epoch [40/100], Loss: 0.5592\n",
      "Epoch [50/100], Loss: 0.5460\n",
      "Epoch [60/100], Loss: 0.5367\n",
      "Epoch [70/100], Loss: 0.5301\n",
      "Epoch [80/100], Loss: 0.5254\n",
      "Epoch [90/100], Loss: 0.5221\n",
      "Epoch [100/100], Loss: 0.5196\n",
      "Epoch [10/100], Loss: 0.7847\n",
      "Epoch [20/100], Loss: 0.6774\n",
      "Epoch [30/100], Loss: 0.6174\n",
      "Epoch [40/100], Loss: 0.5821\n",
      "Epoch [50/100], Loss: 0.5598\n",
      "Epoch [60/100], Loss: 0.5454\n",
      "Epoch [70/100], Loss: 0.5362\n",
      "Epoch [80/100], Loss: 0.5303\n",
      "Epoch [90/100], Loss: 0.5262\n",
      "Epoch [100/100], Loss: 0.5233\n",
      "Epoch [10/100], Loss: 0.7899\n",
      "Epoch [20/100], Loss: 0.6683\n",
      "Epoch [30/100], Loss: 0.6060\n",
      "Epoch [40/100], Loss: 0.5756\n",
      "Epoch [50/100], Loss: 0.5588\n",
      "Epoch [60/100], Loss: 0.5479\n",
      "Epoch [70/100], Loss: 0.5402\n",
      "Epoch [80/100], Loss: 0.5346\n",
      "Epoch [90/100], Loss: 0.5303\n",
      "Epoch [100/100], Loss: 0.5268\n",
      "Epoch [10/100], Loss: 0.6104\n",
      "Epoch [20/100], Loss: 0.5625\n",
      "Epoch [30/100], Loss: 0.5408\n",
      "Epoch [40/100], Loss: 0.5286\n",
      "Epoch [50/100], Loss: 0.5222\n",
      "Epoch [60/100], Loss: 0.5191\n",
      "Epoch [70/100], Loss: 0.5175\n",
      "Epoch [80/100], Loss: 0.5166\n",
      "Epoch [90/100], Loss: 0.5162\n",
      "Epoch [100/100], Loss: 0.5160\n",
      "Epoch [10/100], Loss: 0.7001\n",
      "Epoch [20/100], Loss: 0.6319\n",
      "Epoch [30/100], Loss: 0.5931\n",
      "Epoch [40/100], Loss: 0.5686\n",
      "Epoch [50/100], Loss: 0.5518\n",
      "Epoch [60/100], Loss: 0.5402\n",
      "Epoch [70/100], Loss: 0.5324\n",
      "Epoch [80/100], Loss: 0.5273\n",
      "Epoch [90/100], Loss: 0.5239\n",
      "Epoch [100/100], Loss: 0.5216\n",
      "Epoch [10/100], Loss: 0.9195\n",
      "Epoch [20/100], Loss: 0.7572\n",
      "Epoch [30/100], Loss: 0.6627\n",
      "Epoch [40/100], Loss: 0.6094\n",
      "Epoch [50/100], Loss: 0.5770\n",
      "Epoch [60/100], Loss: 0.5559\n",
      "Epoch [70/100], Loss: 0.5421\n",
      "Epoch [80/100], Loss: 0.5332\n",
      "Epoch [90/100], Loss: 0.5274\n",
      "Epoch [100/100], Loss: 0.5235\n",
      "Epoch [10/100], Loss: 0.6708\n",
      "Epoch [20/100], Loss: 0.6121\n",
      "Epoch [30/100], Loss: 0.5742\n",
      "Epoch [40/100], Loss: 0.5514\n",
      "Epoch [50/100], Loss: 0.5386\n",
      "Epoch [60/100], Loss: 0.5307\n",
      "Epoch [70/100], Loss: 0.5256\n",
      "Epoch [80/100], Loss: 0.5223\n",
      "Epoch [90/100], Loss: 0.5201\n",
      "Epoch [100/100], Loss: 0.5187\n",
      "Epoch [10/100], Loss: 0.6893\n",
      "Epoch [20/100], Loss: 0.6220\n",
      "Epoch [30/100], Loss: 0.5797\n",
      "Epoch [40/100], Loss: 0.5549\n",
      "Epoch [50/100], Loss: 0.5399\n",
      "Epoch [60/100], Loss: 0.5302\n",
      "Epoch [70/100], Loss: 0.5236\n",
      "Epoch [80/100], Loss: 0.5192\n",
      "Epoch [90/100], Loss: 0.5164\n",
      "Epoch [100/100], Loss: 0.5145\n",
      "Epoch [10/100], Loss: 0.6677\n",
      "Epoch [20/100], Loss: 0.6140\n",
      "Epoch [30/100], Loss: 0.5762\n",
      "Epoch [40/100], Loss: 0.5515\n",
      "Epoch [50/100], Loss: 0.5367\n",
      "Epoch [60/100], Loss: 0.5281\n",
      "Epoch [70/100], Loss: 0.5229\n",
      "Epoch [80/100], Loss: 0.5195\n",
      "Epoch [90/100], Loss: 0.5172\n",
      "Epoch [100/100], Loss: 0.5154\n",
      "Epoch [10/100], Loss: 0.7620\n",
      "Epoch [20/100], Loss: 0.6718\n",
      "Epoch [30/100], Loss: 0.6187\n",
      "Epoch [40/100], Loss: 0.5862\n",
      "Epoch [50/100], Loss: 0.5649\n",
      "Epoch [60/100], Loss: 0.5510\n",
      "Epoch [70/100], Loss: 0.5419\n",
      "Epoch [80/100], Loss: 0.5356\n",
      "Epoch [90/100], Loss: 0.5311\n",
      "Epoch [100/100], Loss: 0.5278\n",
      "Epoch [10/100], Loss: 0.9519\n",
      "Epoch [20/100], Loss: 0.7647\n",
      "Epoch [30/100], Loss: 0.6502\n",
      "Epoch [40/100], Loss: 0.5898\n",
      "Epoch [50/100], Loss: 0.5596\n",
      "Epoch [60/100], Loss: 0.5434\n",
      "Epoch [70/100], Loss: 0.5335\n",
      "Epoch [80/100], Loss: 0.5271\n",
      "Epoch [90/100], Loss: 0.5225\n",
      "Epoch [100/100], Loss: 0.5192\n",
      "Epoch [10/100], Loss: 0.8386\n",
      "Epoch [20/100], Loss: 0.7304\n",
      "Epoch [30/100], Loss: 0.6542\n",
      "Epoch [40/100], Loss: 0.6024\n",
      "Epoch [50/100], Loss: 0.5694\n",
      "Epoch [60/100], Loss: 0.5492\n",
      "Epoch [70/100], Loss: 0.5375\n",
      "Epoch [80/100], Loss: 0.5307\n",
      "Epoch [90/100], Loss: 0.5266\n",
      "Epoch [100/100], Loss: 0.5239\n",
      "Epoch [10/100], Loss: 0.6482\n",
      "Epoch [20/100], Loss: 0.5972\n",
      "Epoch [30/100], Loss: 0.5632\n",
      "Epoch [40/100], Loss: 0.5419\n",
      "Epoch [50/100], Loss: 0.5294\n",
      "Epoch [60/100], Loss: 0.5225\n",
      "Epoch [70/100], Loss: 0.5187\n",
      "Epoch [80/100], Loss: 0.5166\n",
      "Epoch [90/100], Loss: 0.5154\n",
      "Epoch [100/100], Loss: 0.5147\n",
      "Epoch [10/100], Loss: 0.7824\n",
      "Epoch [20/100], Loss: 0.6785\n",
      "Epoch [30/100], Loss: 0.6129\n",
      "Epoch [40/100], Loss: 0.5744\n",
      "Epoch [50/100], Loss: 0.5521\n",
      "Epoch [60/100], Loss: 0.5386\n",
      "Epoch [70/100], Loss: 0.5301\n",
      "Epoch [80/100], Loss: 0.5246\n",
      "Epoch [90/100], Loss: 0.5209\n",
      "Epoch [100/100], Loss: 0.5185\n",
      "Epoch [10/100], Loss: 1.0309\n",
      "Epoch [20/100], Loss: 0.8492\n",
      "Epoch [30/100], Loss: 0.7353\n",
      "Epoch [40/100], Loss: 0.6659\n",
      "Epoch [50/100], Loss: 0.6225\n",
      "Epoch [60/100], Loss: 0.5942\n",
      "Epoch [70/100], Loss: 0.5747\n",
      "Epoch [80/100], Loss: 0.5604\n",
      "Epoch [90/100], Loss: 0.5496\n",
      "Epoch [100/100], Loss: 0.5412\n",
      "Epoch [10/100], Loss: 0.8402\n",
      "Epoch [20/100], Loss: 0.7231\n",
      "Epoch [30/100], Loss: 0.6526\n",
      "Epoch [40/100], Loss: 0.6078\n",
      "Epoch [50/100], Loss: 0.5788\n",
      "Epoch [60/100], Loss: 0.5597\n",
      "Epoch [70/100], Loss: 0.5467\n",
      "Epoch [80/100], Loss: 0.5377\n",
      "Epoch [90/100], Loss: 0.5314\n",
      "Epoch [100/100], Loss: 0.5270\n",
      "Epoch [10/100], Loss: 0.7282\n",
      "Epoch [20/100], Loss: 0.6509\n",
      "Epoch [30/100], Loss: 0.6013\n",
      "Epoch [40/100], Loss: 0.5694\n",
      "Epoch [50/100], Loss: 0.5489\n",
      "Epoch [60/100], Loss: 0.5355\n",
      "Epoch [70/100], Loss: 0.5268\n",
      "Epoch [80/100], Loss: 0.5213\n",
      "Epoch [90/100], Loss: 0.5178\n",
      "Epoch [100/100], Loss: 0.5156\n",
      "Epoch [10/100], Loss: 0.6340\n",
      "Epoch [20/100], Loss: 0.5848\n",
      "Epoch [30/100], Loss: 0.5572\n",
      "Epoch [40/100], Loss: 0.5397\n",
      "Epoch [50/100], Loss: 0.5298\n",
      "Epoch [60/100], Loss: 0.5238\n",
      "Epoch [70/100], Loss: 0.5201\n",
      "Epoch [80/100], Loss: 0.5178\n",
      "Epoch [90/100], Loss: 0.5164\n",
      "Epoch [100/100], Loss: 0.5155\n",
      "Epoch [10/100], Loss: 0.6081\n",
      "Epoch [20/100], Loss: 0.5696\n",
      "Epoch [30/100], Loss: 0.5465\n",
      "Epoch [40/100], Loss: 0.5326\n",
      "Epoch [50/100], Loss: 0.5240\n",
      "Epoch [60/100], Loss: 0.5189\n",
      "Epoch [70/100], Loss: 0.5161\n",
      "Epoch [80/100], Loss: 0.5146\n",
      "Epoch [90/100], Loss: 0.5138\n",
      "Epoch [100/100], Loss: 0.5133\n",
      "Epoch [10/100], Loss: 0.7934\n",
      "Epoch [20/100], Loss: 0.6928\n",
      "Epoch [30/100], Loss: 0.6385\n",
      "Epoch [40/100], Loss: 0.6038\n",
      "Epoch [50/100], Loss: 0.5801\n",
      "Epoch [60/100], Loss: 0.5638\n",
      "Epoch [70/100], Loss: 0.5524\n",
      "Epoch [80/100], Loss: 0.5441\n",
      "Epoch [90/100], Loss: 0.5377\n",
      "Epoch [100/100], Loss: 0.5327\n",
      "Epoch [10/100], Loss: 0.7949\n",
      "Epoch [20/100], Loss: 0.6870\n",
      "Epoch [30/100], Loss: 0.6197\n",
      "Epoch [40/100], Loss: 0.5773\n",
      "Epoch [50/100], Loss: 0.5516\n",
      "Epoch [60/100], Loss: 0.5366\n",
      "Epoch [70/100], Loss: 0.5278\n",
      "Epoch [80/100], Loss: 0.5226\n",
      "Epoch [90/100], Loss: 0.5193\n",
      "Epoch [100/100], Loss: 0.5173\n",
      "Epoch [10/100], Loss: 0.6117\n",
      "Epoch [20/100], Loss: 0.5670\n",
      "Epoch [30/100], Loss: 0.5424\n",
      "Epoch [40/100], Loss: 0.5293\n",
      "Epoch [50/100], Loss: 0.5220\n",
      "Epoch [60/100], Loss: 0.5180\n",
      "Epoch [70/100], Loss: 0.5158\n",
      "Epoch [80/100], Loss: 0.5146\n",
      "Epoch [90/100], Loss: 0.5140\n",
      "Epoch [100/100], Loss: 0.5137\n",
      "Epoch [10/100], Loss: 0.7340\n",
      "Epoch [20/100], Loss: 0.6563\n",
      "Epoch [30/100], Loss: 0.6056\n",
      "Epoch [40/100], Loss: 0.5718\n",
      "Epoch [50/100], Loss: 0.5498\n",
      "Epoch [60/100], Loss: 0.5358\n",
      "Epoch [70/100], Loss: 0.5272\n",
      "Epoch [80/100], Loss: 0.5221\n",
      "Epoch [90/100], Loss: 0.5190\n",
      "Epoch [100/100], Loss: 0.5170\n",
      "Epoch [10/100], Loss: 0.6310\n",
      "Epoch [20/100], Loss: 0.5828\n",
      "Epoch [30/100], Loss: 0.5534\n",
      "Epoch [40/100], Loss: 0.5363\n",
      "Epoch [50/100], Loss: 0.5263\n",
      "Epoch [60/100], Loss: 0.5206\n",
      "Epoch [70/100], Loss: 0.5173\n",
      "Epoch [80/100], Loss: 0.5154\n",
      "Epoch [90/100], Loss: 0.5143\n",
      "Epoch [100/100], Loss: 0.5137\n",
      "Epoch [10/100], Loss: 0.8555\n",
      "Epoch [20/100], Loss: 0.7191\n",
      "Epoch [30/100], Loss: 0.6362\n",
      "Epoch [40/100], Loss: 0.5872\n",
      "Epoch [50/100], Loss: 0.5593\n",
      "Epoch [60/100], Loss: 0.5439\n",
      "Epoch [70/100], Loss: 0.5351\n",
      "Epoch [80/100], Loss: 0.5294\n",
      "Epoch [90/100], Loss: 0.5254\n",
      "Epoch [100/100], Loss: 0.5226\n",
      "Epoch [10/100], Loss: 0.6670\n",
      "Epoch [20/100], Loss: 0.6015\n",
      "Epoch [30/100], Loss: 0.5635\n",
      "Epoch [40/100], Loss: 0.5404\n",
      "Epoch [50/100], Loss: 0.5286\n",
      "Epoch [60/100], Loss: 0.5230\n",
      "Epoch [70/100], Loss: 0.5201\n",
      "Epoch [80/100], Loss: 0.5185\n",
      "Epoch [90/100], Loss: 0.5175\n",
      "Epoch [100/100], Loss: 0.5168\n",
      "Epoch [10/100], Loss: 0.6812\n",
      "Epoch [20/100], Loss: 0.5963\n",
      "Epoch [30/100], Loss: 0.5526\n",
      "Epoch [40/100], Loss: 0.5332\n",
      "Epoch [50/100], Loss: 0.5248\n",
      "Epoch [60/100], Loss: 0.5206\n",
      "Epoch [70/100], Loss: 0.5180\n",
      "Epoch [80/100], Loss: 0.5162\n",
      "Epoch [90/100], Loss: 0.5150\n",
      "Epoch [100/100], Loss: 0.5141\n",
      "Epoch [10/100], Loss: 0.7321\n",
      "Epoch [20/100], Loss: 0.6520\n",
      "Epoch [30/100], Loss: 0.6078\n",
      "Epoch [40/100], Loss: 0.5799\n",
      "Epoch [50/100], Loss: 0.5604\n",
      "Epoch [60/100], Loss: 0.5466\n",
      "Epoch [70/100], Loss: 0.5369\n",
      "Epoch [80/100], Loss: 0.5301\n",
      "Epoch [90/100], Loss: 0.5251\n",
      "Epoch [100/100], Loss: 0.5214\n",
      "Epoch [10/100], Loss: 0.6345\n",
      "Epoch [20/100], Loss: 0.5772\n",
      "Epoch [30/100], Loss: 0.5482\n",
      "Epoch [40/100], Loss: 0.5346\n",
      "Epoch [50/100], Loss: 0.5274\n",
      "Epoch [60/100], Loss: 0.5228\n",
      "Epoch [70/100], Loss: 0.5197\n",
      "Epoch [80/100], Loss: 0.5174\n",
      "Epoch [90/100], Loss: 0.5156\n",
      "Epoch [100/100], Loss: 0.5142\n",
      "Epoch [10/100], Loss: 0.6350\n",
      "Epoch [20/100], Loss: 0.5876\n",
      "Epoch [30/100], Loss: 0.5554\n",
      "Epoch [40/100], Loss: 0.5349\n",
      "Epoch [50/100], Loss: 0.5231\n",
      "Epoch [60/100], Loss: 0.5170\n",
      "Epoch [70/100], Loss: 0.5141\n",
      "Epoch [80/100], Loss: 0.5128\n",
      "Epoch [90/100], Loss: 0.5123\n",
      "Epoch [100/100], Loss: 0.5121\n",
      "Epoch [10/100], Loss: 0.7935\n",
      "Epoch [20/100], Loss: 0.7001\n",
      "Epoch [30/100], Loss: 0.6454\n",
      "Epoch [40/100], Loss: 0.6083\n",
      "Epoch [50/100], Loss: 0.5824\n",
      "Epoch [60/100], Loss: 0.5649\n",
      "Epoch [70/100], Loss: 0.5527\n",
      "Epoch [80/100], Loss: 0.5435\n",
      "Epoch [90/100], Loss: 0.5366\n",
      "Epoch [100/100], Loss: 0.5314\n",
      "Epoch [10/100], Loss: 0.7753\n",
      "Epoch [20/100], Loss: 0.6821\n",
      "Epoch [30/100], Loss: 0.6286\n",
      "Epoch [40/100], Loss: 0.5940\n",
      "Epoch [50/100], Loss: 0.5698\n",
      "Epoch [60/100], Loss: 0.5528\n",
      "Epoch [70/100], Loss: 0.5408\n",
      "Epoch [80/100], Loss: 0.5325\n",
      "Epoch [90/100], Loss: 0.5268\n",
      "Epoch [100/100], Loss: 0.5230\n",
      "Epoch [10/100], Loss: 0.6775\n",
      "Epoch [20/100], Loss: 0.6000\n",
      "Epoch [30/100], Loss: 0.5701\n",
      "Epoch [40/100], Loss: 0.5556\n",
      "Epoch [50/100], Loss: 0.5457\n",
      "Epoch [60/100], Loss: 0.5382\n",
      "Epoch [70/100], Loss: 0.5323\n",
      "Epoch [80/100], Loss: 0.5278\n",
      "Epoch [90/100], Loss: 0.5241\n",
      "Epoch [100/100], Loss: 0.5211\n",
      "Epoch [10/100], Loss: 0.7443\n",
      "Epoch [20/100], Loss: 0.6661\n",
      "Epoch [30/100], Loss: 0.6173\n",
      "Epoch [40/100], Loss: 0.5836\n",
      "Epoch [50/100], Loss: 0.5608\n",
      "Epoch [60/100], Loss: 0.5459\n",
      "Epoch [70/100], Loss: 0.5363\n",
      "Epoch [80/100], Loss: 0.5300\n",
      "Epoch [90/100], Loss: 0.5257\n",
      "Epoch [100/100], Loss: 0.5224\n",
      "Epoch [10/100], Loss: 0.9443\n",
      "Epoch [20/100], Loss: 0.7882\n",
      "Epoch [30/100], Loss: 0.6895\n",
      "Epoch [40/100], Loss: 0.6304\n",
      "Epoch [50/100], Loss: 0.5936\n",
      "Epoch [60/100], Loss: 0.5693\n",
      "Epoch [70/100], Loss: 0.5532\n",
      "Epoch [80/100], Loss: 0.5424\n",
      "Epoch [90/100], Loss: 0.5349\n",
      "Epoch [100/100], Loss: 0.5296\n",
      "Epoch [10/100], Loss: 0.6134\n",
      "Epoch [20/100], Loss: 0.5718\n",
      "Epoch [30/100], Loss: 0.5462\n",
      "Epoch [40/100], Loss: 0.5310\n",
      "Epoch [50/100], Loss: 0.5233\n",
      "Epoch [60/100], Loss: 0.5192\n",
      "Epoch [70/100], Loss: 0.5165\n",
      "Epoch [80/100], Loss: 0.5148\n",
      "Epoch [90/100], Loss: 0.5138\n",
      "Epoch [100/100], Loss: 0.5132\n",
      "Epoch [10/100], Loss: 0.6233\n",
      "Epoch [20/100], Loss: 0.5719\n",
      "Epoch [30/100], Loss: 0.5424\n",
      "Epoch [40/100], Loss: 0.5279\n",
      "Epoch [50/100], Loss: 0.5212\n",
      "Epoch [60/100], Loss: 0.5181\n",
      "Epoch [70/100], Loss: 0.5166\n",
      "Epoch [80/100], Loss: 0.5159\n",
      "Epoch [90/100], Loss: 0.5155\n",
      "Epoch [100/100], Loss: 0.5154\n",
      "Epoch [10/100], Loss: 0.5886\n",
      "Epoch [20/100], Loss: 0.5545\n",
      "Epoch [30/100], Loss: 0.5371\n",
      "Epoch [40/100], Loss: 0.5268\n",
      "Epoch [50/100], Loss: 0.5202\n",
      "Epoch [60/100], Loss: 0.5165\n",
      "Epoch [70/100], Loss: 0.5143\n",
      "Epoch [80/100], Loss: 0.5129\n",
      "Epoch [90/100], Loss: 0.5120\n",
      "Epoch [100/100], Loss: 0.5115\n",
      "Epoch [10/100], Loss: 0.6267\n",
      "Epoch [20/100], Loss: 0.5775\n",
      "Epoch [30/100], Loss: 0.5519\n",
      "Epoch [40/100], Loss: 0.5385\n",
      "Epoch [50/100], Loss: 0.5304\n",
      "Epoch [60/100], Loss: 0.5249\n",
      "Epoch [70/100], Loss: 0.5209\n",
      "Epoch [80/100], Loss: 0.5179\n",
      "Epoch [90/100], Loss: 0.5157\n",
      "Epoch [100/100], Loss: 0.5141\n",
      "Epoch [10/100], Loss: 0.8652\n",
      "Epoch [20/100], Loss: 0.7342\n",
      "Epoch [30/100], Loss: 0.6556\n",
      "Epoch [40/100], Loss: 0.6088\n",
      "Epoch [50/100], Loss: 0.5801\n",
      "Epoch [60/100], Loss: 0.5621\n",
      "Epoch [70/100], Loss: 0.5506\n",
      "Epoch [80/100], Loss: 0.5429\n",
      "Epoch [90/100], Loss: 0.5373\n",
      "Epoch [100/100], Loss: 0.5329\n",
      "Epoch [10/100], Loss: 0.9445\n",
      "Epoch [20/100], Loss: 0.7910\n",
      "Epoch [30/100], Loss: 0.6871\n",
      "Epoch [40/100], Loss: 0.6211\n",
      "Epoch [50/100], Loss: 0.5813\n",
      "Epoch [60/100], Loss: 0.5574\n",
      "Epoch [70/100], Loss: 0.5425\n",
      "Epoch [80/100], Loss: 0.5331\n",
      "Epoch [90/100], Loss: 0.5272\n",
      "Epoch [100/100], Loss: 0.5234\n",
      "Epoch [10/100], Loss: 1.0317\n",
      "Epoch [20/100], Loss: 0.8175\n",
      "Epoch [30/100], Loss: 0.6834\n",
      "Epoch [40/100], Loss: 0.6111\n",
      "Epoch [50/100], Loss: 0.5739\n",
      "Epoch [60/100], Loss: 0.5537\n",
      "Epoch [70/100], Loss: 0.5417\n",
      "Epoch [80/100], Loss: 0.5340\n",
      "Epoch [90/100], Loss: 0.5288\n",
      "Epoch [100/100], Loss: 0.5251\n",
      "Epoch [10/100], Loss: 0.6831\n",
      "Epoch [20/100], Loss: 0.6121\n",
      "Epoch [30/100], Loss: 0.5749\n",
      "Epoch [40/100], Loss: 0.5531\n",
      "Epoch [50/100], Loss: 0.5394\n",
      "Epoch [60/100], Loss: 0.5305\n",
      "Epoch [70/100], Loss: 0.5246\n",
      "Epoch [80/100], Loss: 0.5203\n",
      "Epoch [90/100], Loss: 0.5174\n",
      "Epoch [100/100], Loss: 0.5153\n",
      "Epoch [10/100], Loss: 0.8818\n",
      "Epoch [20/100], Loss: 0.7351\n",
      "Epoch [30/100], Loss: 0.6509\n",
      "Epoch [40/100], Loss: 0.6028\n",
      "Epoch [50/100], Loss: 0.5747\n",
      "Epoch [60/100], Loss: 0.5578\n",
      "Epoch [70/100], Loss: 0.5470\n",
      "Epoch [80/100], Loss: 0.5395\n",
      "Epoch [90/100], Loss: 0.5340\n",
      "Epoch [100/100], Loss: 0.5299\n",
      "Epoch [10/100], Loss: 0.6714\n",
      "Epoch [20/100], Loss: 0.6061\n",
      "Epoch [30/100], Loss: 0.5670\n",
      "Epoch [40/100], Loss: 0.5451\n",
      "Epoch [50/100], Loss: 0.5329\n",
      "Epoch [60/100], Loss: 0.5257\n",
      "Epoch [70/100], Loss: 0.5211\n",
      "Epoch [80/100], Loss: 0.5181\n",
      "Epoch [90/100], Loss: 0.5161\n",
      "Epoch [100/100], Loss: 0.5147\n",
      "Epoch [10/100], Loss: 0.7121\n",
      "Epoch [20/100], Loss: 0.6192\n",
      "Epoch [30/100], Loss: 0.5737\n",
      "Epoch [40/100], Loss: 0.5501\n",
      "Epoch [50/100], Loss: 0.5361\n",
      "Epoch [60/100], Loss: 0.5277\n",
      "Epoch [70/100], Loss: 0.5229\n",
      "Epoch [80/100], Loss: 0.5201\n",
      "Epoch [90/100], Loss: 0.5185\n",
      "Epoch [100/100], Loss: 0.5177\n",
      "Epoch [10/100], Loss: 0.7893\n",
      "Epoch [20/100], Loss: 0.6839\n",
      "Epoch [30/100], Loss: 0.6215\n",
      "Epoch [40/100], Loss: 0.5851\n",
      "Epoch [50/100], Loss: 0.5630\n",
      "Epoch [60/100], Loss: 0.5489\n",
      "Epoch [70/100], Loss: 0.5394\n",
      "Epoch [80/100], Loss: 0.5325\n",
      "Epoch [90/100], Loss: 0.5273\n",
      "Epoch [100/100], Loss: 0.5233\n",
      "Epoch [10/100], Loss: 0.6617\n",
      "Epoch [20/100], Loss: 0.5937\n",
      "Epoch [30/100], Loss: 0.5620\n",
      "Epoch [40/100], Loss: 0.5441\n",
      "Epoch [50/100], Loss: 0.5323\n",
      "Epoch [60/100], Loss: 0.5252\n",
      "Epoch [70/100], Loss: 0.5211\n",
      "Epoch [80/100], Loss: 0.5185\n",
      "Epoch [90/100], Loss: 0.5168\n",
      "Epoch [100/100], Loss: 0.5156\n",
      "Epoch [10/100], Loss: 0.5595\n",
      "Epoch [20/100], Loss: 0.5314\n",
      "Epoch [30/100], Loss: 0.5192\n",
      "Epoch [40/100], Loss: 0.5153\n",
      "Epoch [50/100], Loss: 0.5139\n",
      "Epoch [60/100], Loss: 0.5133\n",
      "Epoch [70/100], Loss: 0.5130\n",
      "Epoch [80/100], Loss: 0.5129\n",
      "Epoch [90/100], Loss: 0.5129\n",
      "Epoch [100/100], Loss: 0.5128\n",
      "Epoch [10/100], Loss: 0.5809\n",
      "Epoch [20/100], Loss: 0.5501\n",
      "Epoch [30/100], Loss: 0.5308\n",
      "Epoch [40/100], Loss: 0.5207\n",
      "Epoch [50/100], Loss: 0.5158\n",
      "Epoch [60/100], Loss: 0.5136\n",
      "Epoch [70/100], Loss: 0.5126\n",
      "Epoch [80/100], Loss: 0.5122\n",
      "Epoch [90/100], Loss: 0.5120\n",
      "Epoch [100/100], Loss: 0.5120\n",
      "Epoch [10/100], Loss: 0.7099\n",
      "Epoch [20/100], Loss: 0.6212\n",
      "Epoch [30/100], Loss: 0.5716\n",
      "Epoch [40/100], Loss: 0.5469\n",
      "Epoch [50/100], Loss: 0.5338\n",
      "Epoch [60/100], Loss: 0.5259\n",
      "Epoch [70/100], Loss: 0.5208\n",
      "Epoch [80/100], Loss: 0.5174\n",
      "Epoch [90/100], Loss: 0.5150\n",
      "Epoch [100/100], Loss: 0.5135\n",
      "Epoch [10/100], Loss: 0.8200\n",
      "Epoch [20/100], Loss: 0.6758\n",
      "Epoch [30/100], Loss: 0.6037\n",
      "Epoch [40/100], Loss: 0.5719\n",
      "Epoch [50/100], Loss: 0.5555\n",
      "Epoch [60/100], Loss: 0.5442\n",
      "Epoch [70/100], Loss: 0.5354\n",
      "Epoch [80/100], Loss: 0.5290\n",
      "Epoch [90/100], Loss: 0.5246\n",
      "Epoch [100/100], Loss: 0.5216\n",
      "Epoch [10/100], Loss: 0.7570\n",
      "Epoch [20/100], Loss: 0.6851\n",
      "Epoch [30/100], Loss: 0.6438\n",
      "Epoch [40/100], Loss: 0.6143\n",
      "Epoch [50/100], Loss: 0.5919\n",
      "Epoch [60/100], Loss: 0.5744\n",
      "Epoch [70/100], Loss: 0.5609\n",
      "Epoch [80/100], Loss: 0.5506\n",
      "Epoch [90/100], Loss: 0.5428\n",
      "Epoch [100/100], Loss: 0.5369\n",
      "Epoch [10/100], Loss: 0.6494\n",
      "Epoch [20/100], Loss: 0.5929\n",
      "Epoch [30/100], Loss: 0.5603\n",
      "Epoch [40/100], Loss: 0.5420\n",
      "Epoch [50/100], Loss: 0.5315\n",
      "Epoch [60/100], Loss: 0.5253\n",
      "Epoch [70/100], Loss: 0.5217\n",
      "Epoch [80/100], Loss: 0.5198\n",
      "Epoch [90/100], Loss: 0.5187\n",
      "Epoch [100/100], Loss: 0.5181\n",
      "Epoch [10/100], Loss: 0.6933\n",
      "Epoch [20/100], Loss: 0.6291\n",
      "Epoch [30/100], Loss: 0.5905\n",
      "Epoch [40/100], Loss: 0.5635\n",
      "Epoch [50/100], Loss: 0.5451\n",
      "Epoch [60/100], Loss: 0.5335\n",
      "Epoch [70/100], Loss: 0.5263\n",
      "Epoch [80/100], Loss: 0.5218\n",
      "Epoch [90/100], Loss: 0.5189\n",
      "Epoch [100/100], Loss: 0.5170\n",
      "Epoch [10/100], Loss: 0.6843\n",
      "Epoch [20/100], Loss: 0.6260\n",
      "Epoch [30/100], Loss: 0.5861\n",
      "Epoch [40/100], Loss: 0.5610\n",
      "Epoch [50/100], Loss: 0.5459\n",
      "Epoch [60/100], Loss: 0.5365\n",
      "Epoch [70/100], Loss: 0.5305\n",
      "Epoch [80/100], Loss: 0.5265\n",
      "Epoch [90/100], Loss: 0.5238\n",
      "Epoch [100/100], Loss: 0.5219\n",
      "Epoch [10/100], Loss: 0.7333\n",
      "Epoch [20/100], Loss: 0.6441\n",
      "Epoch [30/100], Loss: 0.6012\n",
      "Epoch [40/100], Loss: 0.5768\n",
      "Epoch [50/100], Loss: 0.5599\n",
      "Epoch [60/100], Loss: 0.5479\n",
      "Epoch [70/100], Loss: 0.5399\n",
      "Epoch [80/100], Loss: 0.5344\n",
      "Epoch [90/100], Loss: 0.5305\n",
      "Epoch [100/100], Loss: 0.5275\n",
      "Epoch [10/100], Loss: 0.6404\n",
      "Epoch [20/100], Loss: 0.5987\n",
      "Epoch [30/100], Loss: 0.5723\n",
      "Epoch [40/100], Loss: 0.5552\n",
      "Epoch [50/100], Loss: 0.5433\n",
      "Epoch [60/100], Loss: 0.5346\n",
      "Epoch [70/100], Loss: 0.5286\n",
      "Epoch [80/100], Loss: 0.5245\n",
      "Epoch [90/100], Loss: 0.5217\n",
      "Epoch [100/100], Loss: 0.5198\n",
      "Epoch [10/100], Loss: 0.5640\n",
      "Epoch [20/100], Loss: 0.5404\n",
      "Epoch [30/100], Loss: 0.5272\n",
      "Epoch [40/100], Loss: 0.5201\n",
      "Epoch [50/100], Loss: 0.5163\n",
      "Epoch [60/100], Loss: 0.5140\n",
      "Epoch [70/100], Loss: 0.5128\n",
      "Epoch [80/100], Loss: 0.5123\n",
      "Epoch [90/100], Loss: 0.5121\n",
      "Epoch [100/100], Loss: 0.5121\n",
      "Epoch [10/100], Loss: 0.7049\n",
      "Epoch [20/100], Loss: 0.6229\n",
      "Epoch [30/100], Loss: 0.5714\n",
      "Epoch [40/100], Loss: 0.5423\n",
      "Epoch [50/100], Loss: 0.5283\n",
      "Epoch [60/100], Loss: 0.5221\n",
      "Epoch [70/100], Loss: 0.5193\n",
      "Epoch [80/100], Loss: 0.5179\n",
      "Epoch [90/100], Loss: 0.5174\n",
      "Epoch [100/100], Loss: 0.5171\n",
      "Epoch [10/100], Loss: 0.7494\n",
      "Epoch [20/100], Loss: 0.6419\n",
      "Epoch [30/100], Loss: 0.5844\n",
      "Epoch [40/100], Loss: 0.5548\n",
      "Epoch [50/100], Loss: 0.5391\n",
      "Epoch [60/100], Loss: 0.5301\n",
      "Epoch [70/100], Loss: 0.5247\n",
      "Epoch [80/100], Loss: 0.5214\n",
      "Epoch [90/100], Loss: 0.5192\n",
      "Epoch [100/100], Loss: 0.5178\n",
      "Epoch [10/100], Loss: 0.6349\n",
      "Epoch [20/100], Loss: 0.5799\n",
      "Epoch [30/100], Loss: 0.5501\n",
      "Epoch [40/100], Loss: 0.5330\n",
      "Epoch [50/100], Loss: 0.5233\n",
      "Epoch [60/100], Loss: 0.5181\n",
      "Epoch [70/100], Loss: 0.5155\n",
      "Epoch [80/100], Loss: 0.5142\n",
      "Epoch [90/100], Loss: 0.5136\n",
      "Epoch [100/100], Loss: 0.5133\n",
      "Epoch [10/100], Loss: 0.7056\n",
      "Epoch [20/100], Loss: 0.6449\n",
      "Epoch [30/100], Loss: 0.6047\n",
      "Epoch [40/100], Loss: 0.5756\n",
      "Epoch [50/100], Loss: 0.5548\n",
      "Epoch [60/100], Loss: 0.5409\n",
      "Epoch [70/100], Loss: 0.5318\n",
      "Epoch [80/100], Loss: 0.5258\n",
      "Epoch [90/100], Loss: 0.5218\n",
      "Epoch [100/100], Loss: 0.5190\n",
      "Epoch [10/100], Loss: 0.6308\n",
      "Epoch [20/100], Loss: 0.5854\n",
      "Epoch [30/100], Loss: 0.5548\n",
      "Epoch [40/100], Loss: 0.5356\n",
      "Epoch [50/100], Loss: 0.5250\n",
      "Epoch [60/100], Loss: 0.5197\n",
      "Epoch [70/100], Loss: 0.5170\n",
      "Epoch [80/100], Loss: 0.5157\n",
      "Epoch [90/100], Loss: 0.5150\n",
      "Epoch [100/100], Loss: 0.5147\n",
      "Epoch [10/100], Loss: 0.7743\n",
      "Epoch [20/100], Loss: 0.6777\n",
      "Epoch [30/100], Loss: 0.6117\n",
      "Epoch [40/100], Loss: 0.5690\n",
      "Epoch [50/100], Loss: 0.5440\n",
      "Epoch [60/100], Loss: 0.5298\n",
      "Epoch [70/100], Loss: 0.5220\n",
      "Epoch [80/100], Loss: 0.5176\n",
      "Epoch [90/100], Loss: 0.5150\n",
      "Epoch [100/100], Loss: 0.5133\n",
      "Epoch [10/100], Loss: 0.7519\n",
      "Epoch [20/100], Loss: 0.6360\n",
      "Epoch [30/100], Loss: 0.5792\n",
      "Epoch [40/100], Loss: 0.5540\n",
      "Epoch [50/100], Loss: 0.5412\n",
      "Epoch [60/100], Loss: 0.5333\n",
      "Epoch [70/100], Loss: 0.5279\n",
      "Epoch [80/100], Loss: 0.5239\n",
      "Epoch [90/100], Loss: 0.5210\n",
      "Epoch [100/100], Loss: 0.5189\n",
      "Epoch [10/100], Loss: 0.9667\n",
      "Epoch [20/100], Loss: 0.8005\n",
      "Epoch [30/100], Loss: 0.6957\n",
      "Epoch [40/100], Loss: 0.6336\n",
      "Epoch [50/100], Loss: 0.5970\n",
      "Epoch [60/100], Loss: 0.5749\n",
      "Epoch [70/100], Loss: 0.5611\n",
      "Epoch [80/100], Loss: 0.5520\n",
      "Epoch [90/100], Loss: 0.5455\n",
      "Epoch [100/100], Loss: 0.5406\n",
      "Average Accuracy: 0.7415\n",
      "Average Precision: 0.7304\n",
      "Average Sensitivity (Recall): 0.7633\n",
      "Average F1 Score: 0.7464\n",
      "Average ROC AUC: 0.8182\n",
      "Average Confusion Matrix: TP=5380, TN=5104, FP=1986, FN=1669\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Assume data is already preprocessed\n",
    "# Preprocessing code here...\n",
    "\n",
    "# Sampling\n",
    "X = data.drop(columns=['Diabetes'])\n",
    "y = data['Diabetes']\n",
    "y = y.apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert data to TensorFlow tensors\n",
    "X_train_tensor = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "X_test_tensor = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
    "y_train_tensor = tf.convert_to_tensor(y_train.values, dtype=tf.float32)\n",
    "y_test_tensor = tf.convert_to_tensor(y_test.values, dtype=tf.float32)\n",
    "\n",
    "# Define the Logistic Regression model using TensorFlow\n",
    "class LogisticRegressionModel(tf.keras.Model):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return self.linear(inputs)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "input_dim = X_train.shape[1]\n",
    "lr_model = LogisticRegressionModel(input_dim)\n",
    "binary_crossentropy = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "# Define the training function\n",
    "def train_model(X_train, y_train, epochs=100):\n",
    "    for epoch in range(epochs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = lr_model(X_train)\n",
    "            loss = binary_crossentropy(y_train, logits)\n",
    "            regularization_loss = tf.reduce_sum(lr_model.losses)\n",
    "            total_loss = loss + regularization_loss\n",
    "        \n",
    "        gradients = tape.gradient(total_loss, lr_model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, lr_model.trainable_variables))\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {total_loss.numpy():.4f}')\n",
    "\n",
    "# Define the evaluation function\n",
    "def evaluate_model(X_test, y_test):\n",
    "    logits = lr_model(X_test)\n",
    "    predictions = tf.round(logits)\n",
    "    predictions = tf.squeeze(predictions)\n",
    "    y_test_numpy = y_test.numpy()\n",
    "    predictions_numpy = predictions.numpy()\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test_numpy, predictions_numpy)\n",
    "    \n",
    "    # Calculate precision\n",
    "    precision = precision_score(y_test_numpy, predictions_numpy)\n",
    "    \n",
    "    # Calculate sensitivity (recall)\n",
    "    recall = recall_score(y_test_numpy, predictions_numpy)\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(y_test_numpy, predictions_numpy)\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test_numpy, predictions_numpy).ravel()\n",
    "\n",
    "    # Calculate ROC AUC\n",
    "    probabilities = logits.numpy()\n",
    "    roc_auc = roc_auc_score(y_test_numpy, probabilities)\n",
    "    \n",
    "    return accuracy, precision, recall, f1, roc_auc, tn, fp, fn, tp\n",
    "\n",
    "# Perform 100 evaluations using bootstrapping\n",
    "num_evaluations = 100\n",
    "metrics = []\n",
    "\n",
    "for _ in range(num_evaluations):\n",
    "    # Bootstrap sampling\n",
    "    X_resampled, y_resampled = resample(X_train, y_train, n_samples=len(X_train), random_state=None)\n",
    "    \n",
    "    # Convert resampled data to TensorFlow tensors\n",
    "    X_resampled_tensor = tf.convert_to_tensor(X_resampled, dtype=tf.float32)\n",
    "    y_resampled_tensor = tf.convert_to_tensor(y_resampled.values, dtype=tf.float32)\n",
    "    \n",
    "    # Reinitialize the model, optimizer, and loss function for each iteration\n",
    "    lr_model = LogisticRegressionModel(input_dim)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "    \n",
    "    # Train the model\n",
    "    train_model(X_resampled_tensor, y_resampled_tensor)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    metrics.append(evaluate_model(X_test_tensor, y_test_tensor))\n",
    "\n",
    "# Calculate average metrics\n",
    "metrics_array = np.array(metrics)\n",
    "mean_metrics = np.mean(metrics_array, axis=0)\n",
    "\n",
    "# Print the average results\n",
    "print(f'Average Accuracy: {mean_metrics[0]:.4f}')\n",
    "print(f'Average Precision: {mean_metrics[1]:.4f}')\n",
    "print(f'Average Sensitivity (Recall): {mean_metrics[2]:.4f}')\n",
    "print(f'Average F1 Score: {mean_metrics[3]:.4f}')\n",
    "print(f'Average ROC AUC: {mean_metrics[4]:.4f}')\n",
    "print(f'Average Confusion Matrix: TP={mean_metrics[8]:.0f}, TN={mean_metrics[5]:.0f}, FP={mean_metrics[6]:.0f}, FN={mean_metrics[7]:.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 1.2291\n",
      "Epoch [20/100], Loss: 0.9861\n",
      "Epoch [30/100], Loss: 0.8280\n",
      "Epoch [40/100], Loss: 0.7406\n",
      "Epoch [50/100], Loss: 0.6943\n",
      "Epoch [60/100], Loss: 0.6669\n",
      "Epoch [70/100], Loss: 0.6477\n",
      "Epoch [80/100], Loss: 0.6336\n",
      "Epoch [90/100], Loss: 0.6229\n",
      "Epoch [100/100], Loss: 0.6148\n",
      "Epoch [10/100], Loss: 1.2249\n",
      "Epoch [20/100], Loss: 0.9788\n",
      "Epoch [30/100], Loss: 0.8273\n",
      "Epoch [40/100], Loss: 0.7500\n",
      "Epoch [50/100], Loss: 0.7014\n",
      "Epoch [60/100], Loss: 0.6680\n",
      "Epoch [70/100], Loss: 0.6450\n",
      "Epoch [80/100], Loss: 0.6289\n",
      "Epoch [90/100], Loss: 0.6177\n",
      "Epoch [100/100], Loss: 0.6099\n",
      "Epoch [10/100], Loss: 0.8937\n",
      "Epoch [20/100], Loss: 0.7719\n",
      "Epoch [30/100], Loss: 0.6982\n",
      "Epoch [40/100], Loss: 0.6559\n",
      "Epoch [50/100], Loss: 0.6310\n",
      "Epoch [60/100], Loss: 0.6144\n",
      "Epoch [70/100], Loss: 0.6033\n",
      "Epoch [80/100], Loss: 0.5964\n",
      "Epoch [90/100], Loss: 0.5924\n",
      "Epoch [100/100], Loss: 0.5901\n",
      "Epoch [10/100], Loss: 0.8356\n",
      "Epoch [20/100], Loss: 0.7373\n",
      "Epoch [30/100], Loss: 0.6816\n",
      "Epoch [40/100], Loss: 0.6450\n",
      "Epoch [50/100], Loss: 0.6224\n",
      "Epoch [60/100], Loss: 0.6101\n",
      "Epoch [70/100], Loss: 0.6031\n",
      "Epoch [80/100], Loss: 0.5992\n",
      "Epoch [90/100], Loss: 0.5974\n",
      "Epoch [100/100], Loss: 0.5966\n",
      "Epoch [10/100], Loss: 1.1723\n",
      "Epoch [20/100], Loss: 0.9181\n",
      "Epoch [30/100], Loss: 0.7399\n",
      "Epoch [40/100], Loss: 0.6656\n",
      "Epoch [50/100], Loss: 0.6336\n",
      "Epoch [60/100], Loss: 0.6172\n",
      "Epoch [70/100], Loss: 0.6062\n",
      "Epoch [80/100], Loss: 0.5993\n",
      "Epoch [90/100], Loss: 0.5943\n",
      "Epoch [100/100], Loss: 0.5910\n",
      "Epoch [10/100], Loss: 1.2955\n",
      "Epoch [20/100], Loss: 1.0561\n",
      "Epoch [30/100], Loss: 0.8953\n",
      "Epoch [40/100], Loss: 0.8101\n",
      "Epoch [50/100], Loss: 0.7577\n",
      "Epoch [60/100], Loss: 0.7205\n",
      "Epoch [70/100], Loss: 0.6908\n",
      "Epoch [80/100], Loss: 0.6673\n",
      "Epoch [90/100], Loss: 0.6488\n",
      "Epoch [100/100], Loss: 0.6344\n",
      "Epoch [10/100], Loss: 0.9443\n",
      "Epoch [20/100], Loss: 0.7976\n",
      "Epoch [30/100], Loss: 0.7154\n",
      "Epoch [40/100], Loss: 0.6782\n",
      "Epoch [50/100], Loss: 0.6551\n",
      "Epoch [60/100], Loss: 0.6378\n",
      "Epoch [70/100], Loss: 0.6249\n",
      "Epoch [80/100], Loss: 0.6152\n",
      "Epoch [90/100], Loss: 0.6082\n",
      "Epoch [100/100], Loss: 0.6035\n",
      "Epoch [10/100], Loss: 0.9353\n",
      "Epoch [20/100], Loss: 0.8011\n",
      "Epoch [30/100], Loss: 0.7233\n",
      "Epoch [40/100], Loss: 0.6703\n",
      "Epoch [50/100], Loss: 0.6346\n",
      "Epoch [60/100], Loss: 0.6150\n",
      "Epoch [70/100], Loss: 0.6046\n",
      "Epoch [80/100], Loss: 0.5988\n",
      "Epoch [90/100], Loss: 0.5956\n",
      "Epoch [100/100], Loss: 0.5938\n",
      "Epoch [10/100], Loss: 1.0819\n",
      "Epoch [20/100], Loss: 0.9080\n",
      "Epoch [30/100], Loss: 0.8067\n",
      "Epoch [40/100], Loss: 0.7486\n",
      "Epoch [50/100], Loss: 0.7070\n",
      "Epoch [60/100], Loss: 0.6739\n",
      "Epoch [70/100], Loss: 0.6478\n",
      "Epoch [80/100], Loss: 0.6282\n",
      "Epoch [90/100], Loss: 0.6148\n",
      "Epoch [100/100], Loss: 0.6058\n",
      "Epoch [10/100], Loss: 1.2764\n",
      "Epoch [20/100], Loss: 1.0117\n",
      "Epoch [30/100], Loss: 0.8442\n",
      "Epoch [40/100], Loss: 0.7510\n",
      "Epoch [50/100], Loss: 0.6992\n",
      "Epoch [60/100], Loss: 0.6687\n",
      "Epoch [70/100], Loss: 0.6499\n",
      "Epoch [80/100], Loss: 0.6375\n",
      "Epoch [90/100], Loss: 0.6282\n",
      "Epoch [100/100], Loss: 0.6211\n",
      "Epoch [10/100], Loss: 0.9341\n",
      "Epoch [20/100], Loss: 0.7879\n",
      "Epoch [30/100], Loss: 0.7111\n",
      "Epoch [40/100], Loss: 0.6674\n",
      "Epoch [50/100], Loss: 0.6392\n",
      "Epoch [60/100], Loss: 0.6216\n",
      "Epoch [70/100], Loss: 0.6102\n",
      "Epoch [80/100], Loss: 0.6030\n",
      "Epoch [90/100], Loss: 0.5983\n",
      "Epoch [100/100], Loss: 0.5953\n",
      "Epoch [10/100], Loss: 1.3006\n",
      "Epoch [20/100], Loss: 0.9968\n",
      "Epoch [30/100], Loss: 0.7825\n",
      "Epoch [40/100], Loss: 0.6767\n",
      "Epoch [50/100], Loss: 0.6408\n",
      "Epoch [60/100], Loss: 0.6278\n",
      "Epoch [70/100], Loss: 0.6195\n",
      "Epoch [80/100], Loss: 0.6134\n",
      "Epoch [90/100], Loss: 0.6087\n",
      "Epoch [100/100], Loss: 0.6052\n",
      "Epoch [10/100], Loss: 0.9151\n",
      "Epoch [20/100], Loss: 0.7940\n",
      "Epoch [30/100], Loss: 0.7071\n",
      "Epoch [40/100], Loss: 0.6529\n",
      "Epoch [50/100], Loss: 0.6243\n",
      "Epoch [60/100], Loss: 0.6113\n",
      "Epoch [70/100], Loss: 0.6049\n",
      "Epoch [80/100], Loss: 0.6010\n",
      "Epoch [90/100], Loss: 0.5984\n",
      "Epoch [100/100], Loss: 0.5970\n",
      "Epoch [10/100], Loss: 1.0456\n",
      "Epoch [20/100], Loss: 0.8965\n",
      "Epoch [30/100], Loss: 0.8042\n",
      "Epoch [40/100], Loss: 0.7365\n",
      "Epoch [50/100], Loss: 0.6855\n",
      "Epoch [60/100], Loss: 0.6502\n",
      "Epoch [70/100], Loss: 0.6273\n",
      "Epoch [80/100], Loss: 0.6142\n",
      "Epoch [90/100], Loss: 0.6075\n",
      "Epoch [100/100], Loss: 0.6032\n",
      "Epoch [10/100], Loss: 1.1850\n",
      "Epoch [20/100], Loss: 0.9454\n",
      "Epoch [30/100], Loss: 0.8144\n",
      "Epoch [40/100], Loss: 0.7367\n",
      "Epoch [50/100], Loss: 0.6863\n",
      "Epoch [60/100], Loss: 0.6498\n",
      "Epoch [70/100], Loss: 0.6245\n",
      "Epoch [80/100], Loss: 0.6097\n",
      "Epoch [90/100], Loss: 0.6021\n",
      "Epoch [100/100], Loss: 0.5983\n",
      "Epoch [10/100], Loss: 1.3015\n",
      "Epoch [20/100], Loss: 1.0180\n",
      "Epoch [30/100], Loss: 0.8259\n",
      "Epoch [40/100], Loss: 0.7196\n",
      "Epoch [50/100], Loss: 0.6668\n",
      "Epoch [60/100], Loss: 0.6402\n",
      "Epoch [70/100], Loss: 0.6250\n",
      "Epoch [80/100], Loss: 0.6149\n",
      "Epoch [90/100], Loss: 0.6079\n",
      "Epoch [100/100], Loss: 0.6027\n",
      "Epoch [10/100], Loss: 1.1362\n",
      "Epoch [20/100], Loss: 0.8896\n",
      "Epoch [30/100], Loss: 0.7497\n",
      "Epoch [40/100], Loss: 0.6835\n",
      "Epoch [50/100], Loss: 0.6485\n",
      "Epoch [60/100], Loss: 0.6265\n",
      "Epoch [70/100], Loss: 0.6119\n",
      "Epoch [80/100], Loss: 0.6026\n",
      "Epoch [90/100], Loss: 0.5968\n",
      "Epoch [100/100], Loss: 0.5936\n",
      "Epoch [10/100], Loss: 1.0660\n",
      "Epoch [20/100], Loss: 0.8750\n",
      "Epoch [30/100], Loss: 0.7617\n",
      "Epoch [40/100], Loss: 0.7003\n",
      "Epoch [50/100], Loss: 0.6658\n",
      "Epoch [60/100], Loss: 0.6418\n",
      "Epoch [70/100], Loss: 0.6245\n",
      "Epoch [80/100], Loss: 0.6125\n",
      "Epoch [90/100], Loss: 0.6047\n",
      "Epoch [100/100], Loss: 0.6000\n",
      "Epoch [10/100], Loss: 0.8006\n",
      "Epoch [20/100], Loss: 0.7375\n",
      "Epoch [30/100], Loss: 0.6890\n",
      "Epoch [40/100], Loss: 0.6532\n",
      "Epoch [50/100], Loss: 0.6308\n",
      "Epoch [60/100], Loss: 0.6168\n",
      "Epoch [70/100], Loss: 0.6080\n",
      "Epoch [80/100], Loss: 0.6021\n",
      "Epoch [90/100], Loss: 0.5982\n",
      "Epoch [100/100], Loss: 0.5956\n",
      "Epoch [10/100], Loss: 0.8314\n",
      "Epoch [20/100], Loss: 0.7385\n",
      "Epoch [30/100], Loss: 0.6898\n",
      "Epoch [40/100], Loss: 0.6531\n",
      "Epoch [50/100], Loss: 0.6279\n",
      "Epoch [60/100], Loss: 0.6127\n",
      "Epoch [70/100], Loss: 0.6055\n",
      "Epoch [80/100], Loss: 0.6023\n",
      "Epoch [90/100], Loss: 0.6010\n",
      "Epoch [100/100], Loss: 0.6006\n",
      "Epoch [10/100], Loss: 0.9676\n",
      "Epoch [20/100], Loss: 0.7970\n",
      "Epoch [30/100], Loss: 0.7141\n",
      "Epoch [40/100], Loss: 0.6718\n",
      "Epoch [50/100], Loss: 0.6449\n",
      "Epoch [60/100], Loss: 0.6256\n",
      "Epoch [70/100], Loss: 0.6132\n",
      "Epoch [80/100], Loss: 0.6063\n",
      "Epoch [90/100], Loss: 0.6022\n",
      "Epoch [100/100], Loss: 0.5995\n",
      "Epoch [10/100], Loss: 1.1388\n",
      "Epoch [20/100], Loss: 0.9120\n",
      "Epoch [30/100], Loss: 0.7711\n",
      "Epoch [40/100], Loss: 0.6984\n",
      "Epoch [50/100], Loss: 0.6588\n",
      "Epoch [60/100], Loss: 0.6347\n",
      "Epoch [70/100], Loss: 0.6203\n",
      "Epoch [80/100], Loss: 0.6107\n",
      "Epoch [90/100], Loss: 0.6045\n",
      "Epoch [100/100], Loss: 0.6008\n",
      "Epoch [10/100], Loss: 1.1120\n",
      "Epoch [20/100], Loss: 0.9404\n",
      "Epoch [30/100], Loss: 0.8348\n",
      "Epoch [40/100], Loss: 0.7575\n",
      "Epoch [50/100], Loss: 0.6988\n",
      "Epoch [60/100], Loss: 0.6577\n",
      "Epoch [70/100], Loss: 0.6307\n",
      "Epoch [80/100], Loss: 0.6146\n",
      "Epoch [90/100], Loss: 0.6054\n",
      "Epoch [100/100], Loss: 0.6003\n",
      "Epoch [10/100], Loss: 1.0104\n",
      "Epoch [20/100], Loss: 0.8563\n",
      "Epoch [30/100], Loss: 0.7782\n",
      "Epoch [40/100], Loss: 0.7207\n",
      "Epoch [50/100], Loss: 0.6743\n",
      "Epoch [60/100], Loss: 0.6412\n",
      "Epoch [70/100], Loss: 0.6193\n",
      "Epoch [80/100], Loss: 0.6061\n",
      "Epoch [90/100], Loss: 0.5984\n",
      "Epoch [100/100], Loss: 0.5944\n",
      "Epoch [10/100], Loss: 1.2374\n",
      "Epoch [20/100], Loss: 1.0081\n",
      "Epoch [30/100], Loss: 0.8480\n",
      "Epoch [40/100], Loss: 0.7444\n",
      "Epoch [50/100], Loss: 0.6805\n",
      "Epoch [60/100], Loss: 0.6436\n",
      "Epoch [70/100], Loss: 0.6212\n",
      "Epoch [80/100], Loss: 0.6070\n",
      "Epoch [90/100], Loss: 0.5987\n",
      "Epoch [100/100], Loss: 0.5944\n",
      "Epoch [10/100], Loss: 0.8678\n",
      "Epoch [20/100], Loss: 0.7509\n",
      "Epoch [30/100], Loss: 0.6859\n",
      "Epoch [40/100], Loss: 0.6500\n",
      "Epoch [50/100], Loss: 0.6287\n",
      "Epoch [60/100], Loss: 0.6150\n",
      "Epoch [70/100], Loss: 0.6064\n",
      "Epoch [80/100], Loss: 0.6010\n",
      "Epoch [90/100], Loss: 0.5973\n",
      "Epoch [100/100], Loss: 0.5948\n",
      "Epoch [10/100], Loss: 0.8146\n",
      "Epoch [20/100], Loss: 0.7225\n",
      "Epoch [30/100], Loss: 0.6603\n",
      "Epoch [40/100], Loss: 0.6218\n",
      "Epoch [50/100], Loss: 0.6033\n",
      "Epoch [60/100], Loss: 0.5966\n",
      "Epoch [70/100], Loss: 0.5940\n",
      "Epoch [80/100], Loss: 0.5928\n",
      "Epoch [90/100], Loss: 0.5923\n",
      "Epoch [100/100], Loss: 0.5921\n",
      "Epoch [10/100], Loss: 1.1123\n",
      "Epoch [20/100], Loss: 0.9194\n",
      "Epoch [30/100], Loss: 0.7966\n",
      "Epoch [40/100], Loss: 0.7225\n",
      "Epoch [50/100], Loss: 0.6783\n",
      "Epoch [60/100], Loss: 0.6505\n",
      "Epoch [70/100], Loss: 0.6314\n",
      "Epoch [80/100], Loss: 0.6179\n",
      "Epoch [90/100], Loss: 0.6088\n",
      "Epoch [100/100], Loss: 0.6034\n",
      "Epoch [10/100], Loss: 1.2244\n",
      "Epoch [20/100], Loss: 1.0095\n",
      "Epoch [30/100], Loss: 0.8676\n",
      "Epoch [40/100], Loss: 0.7910\n",
      "Epoch [50/100], Loss: 0.7363\n",
      "Epoch [60/100], Loss: 0.7012\n",
      "Epoch [70/100], Loss: 0.6764\n",
      "Epoch [80/100], Loss: 0.6551\n",
      "Epoch [90/100], Loss: 0.6372\n",
      "Epoch [100/100], Loss: 0.6227\n",
      "Epoch [10/100], Loss: 0.7843\n",
      "Epoch [20/100], Loss: 0.7020\n",
      "Epoch [30/100], Loss: 0.6552\n",
      "Epoch [40/100], Loss: 0.6240\n",
      "Epoch [50/100], Loss: 0.6070\n",
      "Epoch [60/100], Loss: 0.5998\n",
      "Epoch [70/100], Loss: 0.5965\n",
      "Epoch [80/100], Loss: 0.5946\n",
      "Epoch [90/100], Loss: 0.5936\n",
      "Epoch [100/100], Loss: 0.5931\n",
      "Epoch [10/100], Loss: 1.2376\n",
      "Epoch [20/100], Loss: 1.0231\n",
      "Epoch [30/100], Loss: 0.8667\n",
      "Epoch [40/100], Loss: 0.7612\n",
      "Epoch [50/100], Loss: 0.6932\n",
      "Epoch [60/100], Loss: 0.6524\n",
      "Epoch [70/100], Loss: 0.6281\n",
      "Epoch [80/100], Loss: 0.6129\n",
      "Epoch [90/100], Loss: 0.6035\n",
      "Epoch [100/100], Loss: 0.5980\n",
      "Epoch [10/100], Loss: 0.8891\n",
      "Epoch [20/100], Loss: 0.7895\n",
      "Epoch [30/100], Loss: 0.7196\n",
      "Epoch [40/100], Loss: 0.6689\n",
      "Epoch [50/100], Loss: 0.6356\n",
      "Epoch [60/100], Loss: 0.6159\n",
      "Epoch [70/100], Loss: 0.6048\n",
      "Epoch [80/100], Loss: 0.5985\n",
      "Epoch [90/100], Loss: 0.5947\n",
      "Epoch [100/100], Loss: 0.5924\n",
      "Epoch [10/100], Loss: 0.9110\n",
      "Epoch [20/100], Loss: 0.7823\n",
      "Epoch [30/100], Loss: 0.6984\n",
      "Epoch [40/100], Loss: 0.6488\n",
      "Epoch [50/100], Loss: 0.6220\n",
      "Epoch [60/100], Loss: 0.6085\n",
      "Epoch [70/100], Loss: 0.6018\n",
      "Epoch [80/100], Loss: 0.5988\n",
      "Epoch [90/100], Loss: 0.5976\n",
      "Epoch [100/100], Loss: 0.5970\n",
      "Epoch [10/100], Loss: 0.8793\n",
      "Epoch [20/100], Loss: 0.7547\n",
      "Epoch [30/100], Loss: 0.6858\n",
      "Epoch [40/100], Loss: 0.6488\n",
      "Epoch [50/100], Loss: 0.6279\n",
      "Epoch [60/100], Loss: 0.6165\n",
      "Epoch [70/100], Loss: 0.6086\n",
      "Epoch [80/100], Loss: 0.6031\n",
      "Epoch [90/100], Loss: 0.5995\n",
      "Epoch [100/100], Loss: 0.5971\n",
      "Epoch [10/100], Loss: 0.7663\n",
      "Epoch [20/100], Loss: 0.7030\n",
      "Epoch [30/100], Loss: 0.6555\n",
      "Epoch [40/100], Loss: 0.6238\n",
      "Epoch [50/100], Loss: 0.6065\n",
      "Epoch [60/100], Loss: 0.5990\n",
      "Epoch [70/100], Loss: 0.5961\n",
      "Epoch [80/100], Loss: 0.5947\n",
      "Epoch [90/100], Loss: 0.5939\n",
      "Epoch [100/100], Loss: 0.5934\n",
      "Epoch [10/100], Loss: 0.8460\n",
      "Epoch [20/100], Loss: 0.7580\n",
      "Epoch [30/100], Loss: 0.7028\n",
      "Epoch [40/100], Loss: 0.6611\n",
      "Epoch [50/100], Loss: 0.6342\n",
      "Epoch [60/100], Loss: 0.6178\n",
      "Epoch [70/100], Loss: 0.6094\n",
      "Epoch [80/100], Loss: 0.6057\n",
      "Epoch [90/100], Loss: 0.6038\n",
      "Epoch [100/100], Loss: 0.6023\n",
      "Epoch [10/100], Loss: 0.7739\n",
      "Epoch [20/100], Loss: 0.6842\n",
      "Epoch [30/100], Loss: 0.6403\n",
      "Epoch [40/100], Loss: 0.6156\n",
      "Epoch [50/100], Loss: 0.6011\n",
      "Epoch [60/100], Loss: 0.5943\n",
      "Epoch [70/100], Loss: 0.5913\n",
      "Epoch [80/100], Loss: 0.5893\n",
      "Epoch [90/100], Loss: 0.5878\n",
      "Epoch [100/100], Loss: 0.5869\n",
      "Epoch [10/100], Loss: 0.8723\n",
      "Epoch [20/100], Loss: 0.7584\n",
      "Epoch [30/100], Loss: 0.6857\n",
      "Epoch [40/100], Loss: 0.6436\n",
      "Epoch [50/100], Loss: 0.6190\n",
      "Epoch [60/100], Loss: 0.6052\n",
      "Epoch [70/100], Loss: 0.5979\n",
      "Epoch [80/100], Loss: 0.5941\n",
      "Epoch [90/100], Loss: 0.5922\n",
      "Epoch [100/100], Loss: 0.5913\n",
      "Epoch [10/100], Loss: 1.1666\n",
      "Epoch [20/100], Loss: 0.8922\n",
      "Epoch [30/100], Loss: 0.7467\n",
      "Epoch [40/100], Loss: 0.6954\n",
      "Epoch [50/100], Loss: 0.6677\n",
      "Epoch [60/100], Loss: 0.6459\n",
      "Epoch [70/100], Loss: 0.6291\n",
      "Epoch [80/100], Loss: 0.6174\n",
      "Epoch [90/100], Loss: 0.6094\n",
      "Epoch [100/100], Loss: 0.6038\n",
      "Epoch [10/100], Loss: 0.9906\n",
      "Epoch [20/100], Loss: 0.8001\n",
      "Epoch [30/100], Loss: 0.7008\n",
      "Epoch [40/100], Loss: 0.6568\n",
      "Epoch [50/100], Loss: 0.6343\n",
      "Epoch [60/100], Loss: 0.6204\n",
      "Epoch [70/100], Loss: 0.6103\n",
      "Epoch [80/100], Loss: 0.6031\n",
      "Epoch [90/100], Loss: 0.5984\n",
      "Epoch [100/100], Loss: 0.5955\n",
      "Epoch [10/100], Loss: 0.9375\n",
      "Epoch [20/100], Loss: 0.7729\n",
      "Epoch [30/100], Loss: 0.7086\n",
      "Epoch [40/100], Loss: 0.6709\n",
      "Epoch [50/100], Loss: 0.6419\n",
      "Epoch [60/100], Loss: 0.6224\n",
      "Epoch [70/100], Loss: 0.6114\n",
      "Epoch [80/100], Loss: 0.6048\n",
      "Epoch [90/100], Loss: 0.6009\n",
      "Epoch [100/100], Loss: 0.5987\n",
      "Epoch [10/100], Loss: 0.8034\n",
      "Epoch [20/100], Loss: 0.7207\n",
      "Epoch [30/100], Loss: 0.6605\n",
      "Epoch [40/100], Loss: 0.6239\n",
      "Epoch [50/100], Loss: 0.6048\n",
      "Epoch [60/100], Loss: 0.5970\n",
      "Epoch [70/100], Loss: 0.5949\n",
      "Epoch [80/100], Loss: 0.5945\n",
      "Epoch [90/100], Loss: 0.5944\n",
      "Epoch [100/100], Loss: 0.5944\n",
      "Epoch [10/100], Loss: 0.7182\n",
      "Epoch [20/100], Loss: 0.6594\n",
      "Epoch [30/100], Loss: 0.6236\n",
      "Epoch [40/100], Loss: 0.6057\n",
      "Epoch [50/100], Loss: 0.5984\n",
      "Epoch [60/100], Loss: 0.5950\n",
      "Epoch [70/100], Loss: 0.5936\n",
      "Epoch [80/100], Loss: 0.5932\n",
      "Epoch [90/100], Loss: 0.5932\n",
      "Epoch [100/100], Loss: 0.5932\n",
      "Epoch [10/100], Loss: 0.9921\n",
      "Epoch [20/100], Loss: 0.8190\n",
      "Epoch [30/100], Loss: 0.7362\n",
      "Epoch [40/100], Loss: 0.6875\n",
      "Epoch [50/100], Loss: 0.6564\n",
      "Epoch [60/100], Loss: 0.6354\n",
      "Epoch [70/100], Loss: 0.6221\n",
      "Epoch [80/100], Loss: 0.6142\n",
      "Epoch [90/100], Loss: 0.6091\n",
      "Epoch [100/100], Loss: 0.6053\n",
      "Epoch [10/100], Loss: 0.7734\n",
      "Epoch [20/100], Loss: 0.7026\n",
      "Epoch [30/100], Loss: 0.6660\n",
      "Epoch [40/100], Loss: 0.6398\n",
      "Epoch [50/100], Loss: 0.6237\n",
      "Epoch [60/100], Loss: 0.6149\n",
      "Epoch [70/100], Loss: 0.6089\n",
      "Epoch [80/100], Loss: 0.6046\n",
      "Epoch [90/100], Loss: 0.6016\n",
      "Epoch [100/100], Loss: 0.5995\n",
      "Epoch [10/100], Loss: 0.9425\n",
      "Epoch [20/100], Loss: 0.7638\n",
      "Epoch [30/100], Loss: 0.6864\n",
      "Epoch [40/100], Loss: 0.6556\n",
      "Epoch [50/100], Loss: 0.6362\n",
      "Epoch [60/100], Loss: 0.6211\n",
      "Epoch [70/100], Loss: 0.6114\n",
      "Epoch [80/100], Loss: 0.6061\n",
      "Epoch [90/100], Loss: 0.6031\n",
      "Epoch [100/100], Loss: 0.6012\n",
      "Epoch [10/100], Loss: 0.9913\n",
      "Epoch [20/100], Loss: 0.8530\n",
      "Epoch [30/100], Loss: 0.7606\n",
      "Epoch [40/100], Loss: 0.7002\n",
      "Epoch [50/100], Loss: 0.6603\n",
      "Epoch [60/100], Loss: 0.6347\n",
      "Epoch [70/100], Loss: 0.6192\n",
      "Epoch [80/100], Loss: 0.6094\n",
      "Epoch [90/100], Loss: 0.6028\n",
      "Epoch [100/100], Loss: 0.5985\n",
      "Epoch [10/100], Loss: 1.1738\n",
      "Epoch [20/100], Loss: 0.9444\n",
      "Epoch [30/100], Loss: 0.8072\n",
      "Epoch [40/100], Loss: 0.7380\n",
      "Epoch [50/100], Loss: 0.6968\n",
      "Epoch [60/100], Loss: 0.6667\n",
      "Epoch [70/100], Loss: 0.6435\n",
      "Epoch [80/100], Loss: 0.6266\n",
      "Epoch [90/100], Loss: 0.6153\n",
      "Epoch [100/100], Loss: 0.6078\n",
      "Epoch [10/100], Loss: 1.2158\n",
      "Epoch [20/100], Loss: 1.0082\n",
      "Epoch [30/100], Loss: 0.8682\n",
      "Epoch [40/100], Loss: 0.7816\n",
      "Epoch [50/100], Loss: 0.7242\n",
      "Epoch [60/100], Loss: 0.6824\n",
      "Epoch [70/100], Loss: 0.6526\n",
      "Epoch [80/100], Loss: 0.6320\n",
      "Epoch [90/100], Loss: 0.6184\n",
      "Epoch [100/100], Loss: 0.6100\n",
      "Epoch [10/100], Loss: 1.1393\n",
      "Epoch [20/100], Loss: 0.9282\n",
      "Epoch [30/100], Loss: 0.8005\n",
      "Epoch [40/100], Loss: 0.7281\n",
      "Epoch [50/100], Loss: 0.6798\n",
      "Epoch [60/100], Loss: 0.6466\n",
      "Epoch [70/100], Loss: 0.6247\n",
      "Epoch [80/100], Loss: 0.6110\n",
      "Epoch [90/100], Loss: 0.6027\n",
      "Epoch [100/100], Loss: 0.5978\n",
      "Epoch [10/100], Loss: 1.0570\n",
      "Epoch [20/100], Loss: 0.8553\n",
      "Epoch [30/100], Loss: 0.7521\n",
      "Epoch [40/100], Loss: 0.7009\n",
      "Epoch [50/100], Loss: 0.6652\n",
      "Epoch [60/100], Loss: 0.6374\n",
      "Epoch [70/100], Loss: 0.6180\n",
      "Epoch [80/100], Loss: 0.6058\n",
      "Epoch [90/100], Loss: 0.5982\n",
      "Epoch [100/100], Loss: 0.5933\n",
      "Epoch [10/100], Loss: 0.8718\n",
      "Epoch [20/100], Loss: 0.7435\n",
      "Epoch [30/100], Loss: 0.6900\n",
      "Epoch [40/100], Loss: 0.6623\n",
      "Epoch [50/100], Loss: 0.6414\n",
      "Epoch [60/100], Loss: 0.6261\n",
      "Epoch [70/100], Loss: 0.6155\n",
      "Epoch [80/100], Loss: 0.6080\n",
      "Epoch [90/100], Loss: 0.6028\n",
      "Epoch [100/100], Loss: 0.5992\n",
      "Epoch [10/100], Loss: 0.8632\n",
      "Epoch [20/100], Loss: 0.7401\n",
      "Epoch [30/100], Loss: 0.6920\n",
      "Epoch [40/100], Loss: 0.6623\n",
      "Epoch [50/100], Loss: 0.6379\n",
      "Epoch [60/100], Loss: 0.6205\n",
      "Epoch [70/100], Loss: 0.6091\n",
      "Epoch [80/100], Loss: 0.6017\n",
      "Epoch [90/100], Loss: 0.5969\n",
      "Epoch [100/100], Loss: 0.5941\n",
      "Epoch [10/100], Loss: 1.1087\n",
      "Epoch [20/100], Loss: 0.8983\n",
      "Epoch [30/100], Loss: 0.7728\n",
      "Epoch [40/100], Loss: 0.7064\n",
      "Epoch [50/100], Loss: 0.6664\n",
      "Epoch [60/100], Loss: 0.6393\n",
      "Epoch [70/100], Loss: 0.6205\n",
      "Epoch [80/100], Loss: 0.6083\n",
      "Epoch [90/100], Loss: 0.6015\n",
      "Epoch [100/100], Loss: 0.5977\n",
      "Epoch [10/100], Loss: 1.5321\n",
      "Epoch [20/100], Loss: 1.2252\n",
      "Epoch [30/100], Loss: 0.9768\n",
      "Epoch [40/100], Loss: 0.7954\n",
      "Epoch [50/100], Loss: 0.6893\n",
      "Epoch [60/100], Loss: 0.6432\n",
      "Epoch [70/100], Loss: 0.6259\n",
      "Epoch [80/100], Loss: 0.6184\n",
      "Epoch [90/100], Loss: 0.6137\n",
      "Epoch [100/100], Loss: 0.6096\n",
      "Epoch [10/100], Loss: 1.2380\n",
      "Epoch [20/100], Loss: 0.9848\n",
      "Epoch [30/100], Loss: 0.8023\n",
      "Epoch [40/100], Loss: 0.7006\n",
      "Epoch [50/100], Loss: 0.6534\n",
      "Epoch [60/100], Loss: 0.6317\n",
      "Epoch [70/100], Loss: 0.6209\n",
      "Epoch [80/100], Loss: 0.6139\n",
      "Epoch [90/100], Loss: 0.6088\n",
      "Epoch [100/100], Loss: 0.6061\n",
      "Epoch [10/100], Loss: 0.9248\n",
      "Epoch [20/100], Loss: 0.8189\n",
      "Epoch [30/100], Loss: 0.7494\n",
      "Epoch [40/100], Loss: 0.6960\n",
      "Epoch [50/100], Loss: 0.6563\n",
      "Epoch [60/100], Loss: 0.6293\n",
      "Epoch [70/100], Loss: 0.6118\n",
      "Epoch [80/100], Loss: 0.6022\n",
      "Epoch [90/100], Loss: 0.5968\n",
      "Epoch [100/100], Loss: 0.5936\n",
      "Epoch [10/100], Loss: 1.0008\n",
      "Epoch [20/100], Loss: 0.8173\n",
      "Epoch [30/100], Loss: 0.7265\n",
      "Epoch [40/100], Loss: 0.6863\n",
      "Epoch [50/100], Loss: 0.6605\n",
      "Epoch [60/100], Loss: 0.6394\n",
      "Epoch [70/100], Loss: 0.6236\n",
      "Epoch [80/100], Loss: 0.6123\n",
      "Epoch [90/100], Loss: 0.6048\n",
      "Epoch [100/100], Loss: 0.6002\n",
      "Epoch [10/100], Loss: 0.7974\n",
      "Epoch [20/100], Loss: 0.6895\n",
      "Epoch [30/100], Loss: 0.6483\n",
      "Epoch [40/100], Loss: 0.6270\n",
      "Epoch [50/100], Loss: 0.6108\n",
      "Epoch [60/100], Loss: 0.5996\n",
      "Epoch [70/100], Loss: 0.5938\n",
      "Epoch [80/100], Loss: 0.5905\n",
      "Epoch [90/100], Loss: 0.5884\n",
      "Epoch [100/100], Loss: 0.5873\n",
      "Epoch [10/100], Loss: 0.8522\n",
      "Epoch [20/100], Loss: 0.7512\n",
      "Epoch [30/100], Loss: 0.6869\n",
      "Epoch [40/100], Loss: 0.6469\n",
      "Epoch [50/100], Loss: 0.6215\n",
      "Epoch [60/100], Loss: 0.6071\n",
      "Epoch [70/100], Loss: 0.6005\n",
      "Epoch [80/100], Loss: 0.5977\n",
      "Epoch [90/100], Loss: 0.5964\n",
      "Epoch [100/100], Loss: 0.5957\n",
      "Epoch [10/100], Loss: 1.2733\n",
      "Epoch [20/100], Loss: 1.0223\n",
      "Epoch [30/100], Loss: 0.8503\n",
      "Epoch [40/100], Loss: 0.7603\n",
      "Epoch [50/100], Loss: 0.7111\n",
      "Epoch [60/100], Loss: 0.6792\n",
      "Epoch [70/100], Loss: 0.6563\n",
      "Epoch [80/100], Loss: 0.6382\n",
      "Epoch [90/100], Loss: 0.6241\n",
      "Epoch [100/100], Loss: 0.6152\n",
      "Epoch [10/100], Loss: 0.8697\n",
      "Epoch [20/100], Loss: 0.7715\n",
      "Epoch [30/100], Loss: 0.7183\n",
      "Epoch [40/100], Loss: 0.6823\n",
      "Epoch [50/100], Loss: 0.6551\n",
      "Epoch [60/100], Loss: 0.6355\n",
      "Epoch [70/100], Loss: 0.6217\n",
      "Epoch [80/100], Loss: 0.6123\n",
      "Epoch [90/100], Loss: 0.6060\n",
      "Epoch [100/100], Loss: 0.6019\n",
      "Epoch [10/100], Loss: 0.9257\n",
      "Epoch [20/100], Loss: 0.7834\n",
      "Epoch [30/100], Loss: 0.7065\n",
      "Epoch [40/100], Loss: 0.6611\n",
      "Epoch [50/100], Loss: 0.6313\n",
      "Epoch [60/100], Loss: 0.6125\n",
      "Epoch [70/100], Loss: 0.6021\n",
      "Epoch [80/100], Loss: 0.5964\n",
      "Epoch [90/100], Loss: 0.5931\n",
      "Epoch [100/100], Loss: 0.5914\n",
      "Epoch [10/100], Loss: 1.6490\n",
      "Epoch [20/100], Loss: 1.3053\n",
      "Epoch [30/100], Loss: 1.0240\n",
      "Epoch [40/100], Loss: 0.8489\n",
      "Epoch [50/100], Loss: 0.7637\n",
      "Epoch [60/100], Loss: 0.7072\n",
      "Epoch [70/100], Loss: 0.6702\n",
      "Epoch [80/100], Loss: 0.6471\n",
      "Epoch [90/100], Loss: 0.6325\n",
      "Epoch [100/100], Loss: 0.6225\n",
      "Epoch [10/100], Loss: 1.0068\n",
      "Epoch [20/100], Loss: 0.8617\n",
      "Epoch [30/100], Loss: 0.7658\n",
      "Epoch [40/100], Loss: 0.7095\n",
      "Epoch [50/100], Loss: 0.6747\n",
      "Epoch [60/100], Loss: 0.6498\n",
      "Epoch [70/100], Loss: 0.6314\n",
      "Epoch [80/100], Loss: 0.6182\n",
      "Epoch [90/100], Loss: 0.6089\n",
      "Epoch [100/100], Loss: 0.6025\n",
      "Epoch [10/100], Loss: 1.1488\n",
      "Epoch [20/100], Loss: 0.9606\n",
      "Epoch [30/100], Loss: 0.8324\n",
      "Epoch [40/100], Loss: 0.7486\n",
      "Epoch [50/100], Loss: 0.6946\n",
      "Epoch [60/100], Loss: 0.6593\n",
      "Epoch [70/100], Loss: 0.6364\n",
      "Epoch [80/100], Loss: 0.6216\n",
      "Epoch [90/100], Loss: 0.6120\n",
      "Epoch [100/100], Loss: 0.6058\n",
      "Epoch [10/100], Loss: 0.9299\n",
      "Epoch [20/100], Loss: 0.7990\n",
      "Epoch [30/100], Loss: 0.7181\n",
      "Epoch [40/100], Loss: 0.6664\n",
      "Epoch [50/100], Loss: 0.6358\n",
      "Epoch [60/100], Loss: 0.6183\n",
      "Epoch [70/100], Loss: 0.6074\n",
      "Epoch [80/100], Loss: 0.6005\n",
      "Epoch [90/100], Loss: 0.5963\n",
      "Epoch [100/100], Loss: 0.5939\n",
      "Epoch [10/100], Loss: 1.0432\n",
      "Epoch [20/100], Loss: 0.8935\n",
      "Epoch [30/100], Loss: 0.7912\n",
      "Epoch [40/100], Loss: 0.7254\n",
      "Epoch [50/100], Loss: 0.6823\n",
      "Epoch [60/100], Loss: 0.6521\n",
      "Epoch [70/100], Loss: 0.6310\n",
      "Epoch [80/100], Loss: 0.6170\n",
      "Epoch [90/100], Loss: 0.6080\n",
      "Epoch [100/100], Loss: 0.6025\n",
      "Epoch [10/100], Loss: 0.8587\n",
      "Epoch [20/100], Loss: 0.7602\n",
      "Epoch [30/100], Loss: 0.6923\n",
      "Epoch [40/100], Loss: 0.6452\n",
      "Epoch [50/100], Loss: 0.6184\n",
      "Epoch [60/100], Loss: 0.6057\n",
      "Epoch [70/100], Loss: 0.5999\n",
      "Epoch [80/100], Loss: 0.5976\n",
      "Epoch [90/100], Loss: 0.5968\n",
      "Epoch [100/100], Loss: 0.5965\n",
      "Epoch [10/100], Loss: 0.8018\n",
      "Epoch [20/100], Loss: 0.7200\n",
      "Epoch [30/100], Loss: 0.6648\n",
      "Epoch [40/100], Loss: 0.6293\n",
      "Epoch [50/100], Loss: 0.6084\n",
      "Epoch [60/100], Loss: 0.5985\n",
      "Epoch [70/100], Loss: 0.5949\n",
      "Epoch [80/100], Loss: 0.5935\n",
      "Epoch [90/100], Loss: 0.5928\n",
      "Epoch [100/100], Loss: 0.5923\n",
      "Epoch [10/100], Loss: 0.8689\n",
      "Epoch [20/100], Loss: 0.7548\n",
      "Epoch [30/100], Loss: 0.6928\n",
      "Epoch [40/100], Loss: 0.6519\n",
      "Epoch [50/100], Loss: 0.6232\n",
      "Epoch [60/100], Loss: 0.6047\n",
      "Epoch [70/100], Loss: 0.5951\n",
      "Epoch [80/100], Loss: 0.5909\n",
      "Epoch [90/100], Loss: 0.5888\n",
      "Epoch [100/100], Loss: 0.5877\n",
      "Epoch [10/100], Loss: 0.8059\n",
      "Epoch [20/100], Loss: 0.7165\n",
      "Epoch [30/100], Loss: 0.6601\n",
      "Epoch [40/100], Loss: 0.6270\n",
      "Epoch [50/100], Loss: 0.6091\n",
      "Epoch [60/100], Loss: 0.6001\n",
      "Epoch [70/100], Loss: 0.5959\n",
      "Epoch [80/100], Loss: 0.5939\n",
      "Epoch [90/100], Loss: 0.5930\n",
      "Epoch [100/100], Loss: 0.5925\n",
      "Epoch [10/100], Loss: 0.8054\n",
      "Epoch [20/100], Loss: 0.7195\n",
      "Epoch [30/100], Loss: 0.6613\n",
      "Epoch [40/100], Loss: 0.6235\n",
      "Epoch [50/100], Loss: 0.6049\n",
      "Epoch [60/100], Loss: 0.5979\n",
      "Epoch [70/100], Loss: 0.5954\n",
      "Epoch [80/100], Loss: 0.5945\n",
      "Epoch [90/100], Loss: 0.5940\n",
      "Epoch [100/100], Loss: 0.5937\n",
      "Epoch [10/100], Loss: 0.8295\n",
      "Epoch [20/100], Loss: 0.7361\n",
      "Epoch [30/100], Loss: 0.6793\n",
      "Epoch [40/100], Loss: 0.6425\n",
      "Epoch [50/100], Loss: 0.6188\n",
      "Epoch [60/100], Loss: 0.6042\n",
      "Epoch [70/100], Loss: 0.5964\n",
      "Epoch [80/100], Loss: 0.5925\n",
      "Epoch [90/100], Loss: 0.5906\n",
      "Epoch [100/100], Loss: 0.5896\n",
      "Epoch [10/100], Loss: 1.1035\n",
      "Epoch [20/100], Loss: 0.8974\n",
      "Epoch [30/100], Loss: 0.7634\n",
      "Epoch [40/100], Loss: 0.6851\n",
      "Epoch [50/100], Loss: 0.6445\n",
      "Epoch [60/100], Loss: 0.6239\n",
      "Epoch [70/100], Loss: 0.6125\n",
      "Epoch [80/100], Loss: 0.6062\n",
      "Epoch [90/100], Loss: 0.6023\n",
      "Epoch [100/100], Loss: 0.5998\n",
      "Epoch [10/100], Loss: 0.7994\n",
      "Epoch [20/100], Loss: 0.7100\n",
      "Epoch [30/100], Loss: 0.6602\n",
      "Epoch [40/100], Loss: 0.6292\n",
      "Epoch [50/100], Loss: 0.6127\n",
      "Epoch [60/100], Loss: 0.6031\n",
      "Epoch [70/100], Loss: 0.5986\n",
      "Epoch [80/100], Loss: 0.5964\n",
      "Epoch [90/100], Loss: 0.5953\n",
      "Epoch [100/100], Loss: 0.5947\n",
      "Epoch [10/100], Loss: 0.9497\n",
      "Epoch [20/100], Loss: 0.7880\n",
      "Epoch [30/100], Loss: 0.7059\n",
      "Epoch [40/100], Loss: 0.6626\n",
      "Epoch [50/100], Loss: 0.6378\n",
      "Epoch [60/100], Loss: 0.6227\n",
      "Epoch [70/100], Loss: 0.6120\n",
      "Epoch [80/100], Loss: 0.6047\n",
      "Epoch [90/100], Loss: 0.6000\n",
      "Epoch [100/100], Loss: 0.5971\n",
      "Epoch [10/100], Loss: 0.8905\n",
      "Epoch [20/100], Loss: 0.7737\n",
      "Epoch [30/100], Loss: 0.7071\n",
      "Epoch [40/100], Loss: 0.6587\n",
      "Epoch [50/100], Loss: 0.6263\n",
      "Epoch [60/100], Loss: 0.6103\n",
      "Epoch [70/100], Loss: 0.6040\n",
      "Epoch [80/100], Loss: 0.6014\n",
      "Epoch [90/100], Loss: 0.5998\n",
      "Epoch [100/100], Loss: 0.5987\n",
      "Epoch [10/100], Loss: 1.0105\n",
      "Epoch [20/100], Loss: 0.8629\n",
      "Epoch [30/100], Loss: 0.7867\n",
      "Epoch [40/100], Loss: 0.7366\n",
      "Epoch [50/100], Loss: 0.6980\n",
      "Epoch [60/100], Loss: 0.6679\n",
      "Epoch [70/100], Loss: 0.6447\n",
      "Epoch [80/100], Loss: 0.6277\n",
      "Epoch [90/100], Loss: 0.6162\n",
      "Epoch [100/100], Loss: 0.6088\n",
      "Epoch [10/100], Loss: 0.9347\n",
      "Epoch [20/100], Loss: 0.7936\n",
      "Epoch [30/100], Loss: 0.7257\n",
      "Epoch [40/100], Loss: 0.6792\n",
      "Epoch [50/100], Loss: 0.6439\n",
      "Epoch [60/100], Loss: 0.6206\n",
      "Epoch [70/100], Loss: 0.6066\n",
      "Epoch [80/100], Loss: 0.5987\n",
      "Epoch [90/100], Loss: 0.5948\n",
      "Epoch [100/100], Loss: 0.5929\n",
      "Epoch [10/100], Loss: 0.7758\n",
      "Epoch [20/100], Loss: 0.7038\n",
      "Epoch [30/100], Loss: 0.6606\n",
      "Epoch [40/100], Loss: 0.6290\n",
      "Epoch [50/100], Loss: 0.6092\n",
      "Epoch [60/100], Loss: 0.5980\n",
      "Epoch [70/100], Loss: 0.5920\n",
      "Epoch [80/100], Loss: 0.5892\n",
      "Epoch [90/100], Loss: 0.5878\n",
      "Epoch [100/100], Loss: 0.5872\n",
      "Epoch [10/100], Loss: 1.0841\n",
      "Epoch [20/100], Loss: 0.8718\n",
      "Epoch [30/100], Loss: 0.7607\n",
      "Epoch [40/100], Loss: 0.7081\n",
      "Epoch [50/100], Loss: 0.6757\n",
      "Epoch [60/100], Loss: 0.6507\n",
      "Epoch [70/100], Loss: 0.6313\n",
      "Epoch [80/100], Loss: 0.6178\n",
      "Epoch [90/100], Loss: 0.6091\n",
      "Epoch [100/100], Loss: 0.6034\n",
      "Epoch [10/100], Loss: 0.9668\n",
      "Epoch [20/100], Loss: 0.8186\n",
      "Epoch [30/100], Loss: 0.7465\n",
      "Epoch [40/100], Loss: 0.7038\n",
      "Epoch [50/100], Loss: 0.6721\n",
      "Epoch [60/100], Loss: 0.6491\n",
      "Epoch [70/100], Loss: 0.6320\n",
      "Epoch [80/100], Loss: 0.6198\n",
      "Epoch [90/100], Loss: 0.6115\n",
      "Epoch [100/100], Loss: 0.6059\n",
      "Epoch [10/100], Loss: 0.8881\n",
      "Epoch [20/100], Loss: 0.7761\n",
      "Epoch [30/100], Loss: 0.7190\n",
      "Epoch [40/100], Loss: 0.6837\n",
      "Epoch [50/100], Loss: 0.6575\n",
      "Epoch [60/100], Loss: 0.6372\n",
      "Epoch [70/100], Loss: 0.6222\n",
      "Epoch [80/100], Loss: 0.6110\n",
      "Epoch [90/100], Loss: 0.6029\n",
      "Epoch [100/100], Loss: 0.5973\n",
      "Epoch [10/100], Loss: 0.9919\n",
      "Epoch [20/100], Loss: 0.8441\n",
      "Epoch [30/100], Loss: 0.7522\n",
      "Epoch [40/100], Loss: 0.6903\n",
      "Epoch [50/100], Loss: 0.6494\n",
      "Epoch [60/100], Loss: 0.6248\n",
      "Epoch [70/100], Loss: 0.6110\n",
      "Epoch [80/100], Loss: 0.6040\n",
      "Epoch [90/100], Loss: 0.6011\n",
      "Epoch [100/100], Loss: 0.5999\n",
      "Epoch [10/100], Loss: 1.1692\n",
      "Epoch [20/100], Loss: 0.9189\n",
      "Epoch [30/100], Loss: 0.7601\n",
      "Epoch [40/100], Loss: 0.6911\n",
      "Epoch [50/100], Loss: 0.6624\n",
      "Epoch [60/100], Loss: 0.6450\n",
      "Epoch [70/100], Loss: 0.6315\n",
      "Epoch [80/100], Loss: 0.6213\n",
      "Epoch [90/100], Loss: 0.6137\n",
      "Epoch [100/100], Loss: 0.6081\n",
      "Epoch [10/100], Loss: 0.7412\n",
      "Epoch [20/100], Loss: 0.6822\n",
      "Epoch [30/100], Loss: 0.6410\n",
      "Epoch [40/100], Loss: 0.6139\n",
      "Epoch [50/100], Loss: 0.6020\n",
      "Epoch [60/100], Loss: 0.5963\n",
      "Epoch [70/100], Loss: 0.5945\n",
      "Epoch [80/100], Loss: 0.5937\n",
      "Epoch [90/100], Loss: 0.5935\n",
      "Epoch [100/100], Loss: 0.5934\n",
      "Epoch [10/100], Loss: 0.8545\n",
      "Epoch [20/100], Loss: 0.7586\n",
      "Epoch [30/100], Loss: 0.6928\n",
      "Epoch [40/100], Loss: 0.6459\n",
      "Epoch [50/100], Loss: 0.6174\n",
      "Epoch [60/100], Loss: 0.6053\n",
      "Epoch [70/100], Loss: 0.6006\n",
      "Epoch [80/100], Loss: 0.5982\n",
      "Epoch [90/100], Loss: 0.5969\n",
      "Epoch [100/100], Loss: 0.5961\n",
      "Epoch [10/100], Loss: 1.3703\n",
      "Epoch [20/100], Loss: 1.0910\n",
      "Epoch [30/100], Loss: 0.8831\n",
      "Epoch [40/100], Loss: 0.7653\n",
      "Epoch [50/100], Loss: 0.6984\n",
      "Epoch [60/100], Loss: 0.6585\n",
      "Epoch [70/100], Loss: 0.6339\n",
      "Epoch [80/100], Loss: 0.6202\n",
      "Epoch [90/100], Loss: 0.6129\n",
      "Epoch [100/100], Loss: 0.6081\n",
      "Epoch [10/100], Loss: 0.8300\n",
      "Epoch [20/100], Loss: 0.7369\n",
      "Epoch [30/100], Loss: 0.6835\n",
      "Epoch [40/100], Loss: 0.6495\n",
      "Epoch [50/100], Loss: 0.6297\n",
      "Epoch [60/100], Loss: 0.6189\n",
      "Epoch [70/100], Loss: 0.6109\n",
      "Epoch [80/100], Loss: 0.6052\n",
      "Epoch [90/100], Loss: 0.6012\n",
      "Epoch [100/100], Loss: 0.5982\n",
      "Epoch [10/100], Loss: 0.6723\n",
      "Epoch [20/100], Loss: 0.6281\n",
      "Epoch [30/100], Loss: 0.6082\n",
      "Epoch [40/100], Loss: 0.6008\n",
      "Epoch [50/100], Loss: 0.5998\n",
      "Epoch [60/100], Loss: 0.5996\n",
      "Epoch [70/100], Loss: 0.5993\n",
      "Epoch [80/100], Loss: 0.5993\n",
      "Epoch [90/100], Loss: 0.5992\n",
      "Epoch [100/100], Loss: 0.5992\n",
      "Epoch [10/100], Loss: 1.4954\n",
      "Epoch [20/100], Loss: 1.1993\n",
      "Epoch [30/100], Loss: 0.9885\n",
      "Epoch [40/100], Loss: 0.8526\n",
      "Epoch [50/100], Loss: 0.7577\n",
      "Epoch [60/100], Loss: 0.6949\n",
      "Epoch [70/100], Loss: 0.6575\n",
      "Epoch [80/100], Loss: 0.6360\n",
      "Epoch [90/100], Loss: 0.6232\n",
      "Epoch [100/100], Loss: 0.6148\n",
      "Epoch [10/100], Loss: 1.0384\n",
      "Epoch [20/100], Loss: 0.8633\n",
      "Epoch [30/100], Loss: 0.7665\n",
      "Epoch [40/100], Loss: 0.7061\n",
      "Epoch [50/100], Loss: 0.6648\n",
      "Epoch [60/100], Loss: 0.6365\n",
      "Epoch [70/100], Loss: 0.6185\n",
      "Epoch [80/100], Loss: 0.6079\n",
      "Epoch [90/100], Loss: 0.6021\n",
      "Epoch [100/100], Loss: 0.5987\n",
      "Epoch [10/100], Loss: 1.1732\n",
      "Epoch [20/100], Loss: 0.9748\n",
      "Epoch [30/100], Loss: 0.8436\n",
      "Epoch [40/100], Loss: 0.7613\n",
      "Epoch [50/100], Loss: 0.7097\n",
      "Epoch [60/100], Loss: 0.6751\n",
      "Epoch [70/100], Loss: 0.6509\n",
      "Epoch [80/100], Loss: 0.6330\n",
      "Epoch [90/100], Loss: 0.6198\n",
      "Epoch [100/100], Loss: 0.6103\n",
      "Epoch [10/100], Loss: 1.0114\n",
      "Epoch [20/100], Loss: 0.7919\n",
      "Epoch [30/100], Loss: 0.6895\n",
      "Epoch [40/100], Loss: 0.6503\n",
      "Epoch [50/100], Loss: 0.6289\n",
      "Epoch [60/100], Loss: 0.6144\n",
      "Epoch [70/100], Loss: 0.6053\n",
      "Epoch [80/100], Loss: 0.5998\n",
      "Epoch [90/100], Loss: 0.5963\n",
      "Epoch [100/100], Loss: 0.5941\n",
      "Epoch [10/100], Loss: 0.7396\n",
      "Epoch [20/100], Loss: 0.6769\n",
      "Epoch [30/100], Loss: 0.6394\n",
      "Epoch [40/100], Loss: 0.6189\n",
      "Epoch [50/100], Loss: 0.6086\n",
      "Epoch [60/100], Loss: 0.6020\n",
      "Epoch [70/100], Loss: 0.5980\n",
      "Epoch [80/100], Loss: 0.5962\n",
      "Epoch [90/100], Loss: 0.5953\n",
      "Epoch [100/100], Loss: 0.5948\n",
      "Epoch [10/100], Loss: 0.8324\n",
      "Epoch [20/100], Loss: 0.7392\n",
      "Epoch [30/100], Loss: 0.6791\n",
      "Epoch [40/100], Loss: 0.6379\n",
      "Epoch [50/100], Loss: 0.6138\n",
      "Epoch [60/100], Loss: 0.6017\n",
      "Epoch [70/100], Loss: 0.5960\n",
      "Epoch [80/100], Loss: 0.5929\n",
      "Epoch [90/100], Loss: 0.5910\n",
      "Epoch [100/100], Loss: 0.5898\n",
      "Epoch [10/100], Loss: 0.7759\n",
      "Epoch [20/100], Loss: 0.7020\n",
      "Epoch [30/100], Loss: 0.6522\n",
      "Epoch [40/100], Loss: 0.6209\n",
      "Epoch [50/100], Loss: 0.6077\n",
      "Epoch [60/100], Loss: 0.6031\n",
      "Epoch [70/100], Loss: 0.6005\n",
      "Epoch [80/100], Loss: 0.5985\n",
      "Epoch [90/100], Loss: 0.5971\n",
      "Epoch [100/100], Loss: 0.5963\n",
      "Epoch [10/100], Loss: 0.7063\n",
      "Epoch [20/100], Loss: 0.6521\n",
      "Epoch [30/100], Loss: 0.6236\n",
      "Epoch [40/100], Loss: 0.6082\n",
      "Epoch [50/100], Loss: 0.6005\n",
      "Epoch [60/100], Loss: 0.5972\n",
      "Epoch [70/100], Loss: 0.5960\n",
      "Epoch [80/100], Loss: 0.5957\n",
      "Epoch [90/100], Loss: 0.5956\n",
      "Epoch [100/100], Loss: 0.5956\n",
      "Epoch [10/100], Loss: 0.8295\n",
      "Epoch [20/100], Loss: 0.7466\n",
      "Epoch [30/100], Loss: 0.6991\n",
      "Epoch [40/100], Loss: 0.6625\n",
      "Epoch [50/100], Loss: 0.6364\n",
      "Epoch [60/100], Loss: 0.6192\n",
      "Epoch [70/100], Loss: 0.6088\n",
      "Epoch [80/100], Loss: 0.6026\n",
      "Epoch [90/100], Loss: 0.5988\n",
      "Epoch [100/100], Loss: 0.5965\n",
      "Average Accuracy: 0.7380\n",
      "Average Precision: 0.7159\n",
      "Average Sensitivity (Recall): 0.7871\n",
      "Average F1 Score: 0.7498\n",
      "Average ROC AUC: 0.8169\n",
      "Average Confusion Matrix: TP=5548, TN=4886, FP=2204, FN=1501\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Assume data is already preprocessed\n",
    "# Preprocessing code here...\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Load the dataset\n",
    "url = 'https://p19-bot-sign-sg.ciciai.com/tos-alisg-i-b2l6bve69y-sg/ca602a4dd2e04e6ea717f6548ca51fa6.csv~tplv-b2l6bve69y-image.image?rk3s=68e6b6b5&x-expires=1719744177&x-signature=sdxk9fyq5FsGMaG9rN5e5sU%2FFhk%3D'\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Handling Missing Values\n",
    "data.fillna(data.mean(), inplace=True)\n",
    "\n",
    "# Encoding Categorical Variables\n",
    "label_encoder = LabelEncoder()\n",
    "data['Sex'] = label_encoder.fit_transform(data['Sex'])\n",
    "\n",
    "# Normalizing Numerical Features\n",
    "numerical_cols = data.columns\n",
    "scaler = StandardScaler()\n",
    "data[numerical_cols] = scaler.fit_transform(data[numerical_cols])\n",
    "\n",
    "# Sampling\n",
    "X = data.drop(columns=['Diabetes'])\n",
    "y = data['Diabetes']\n",
    "y = y.apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert data to TensorFlow tensors\n",
    "X_train_tensor = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "X_test_tensor = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
    "y_train_tensor = tf.convert_to_tensor(y_train.values, dtype=tf.float32)\n",
    "y_test_tensor = tf.convert_to_tensor(y_test.values, dtype=tf.float32)\n",
    "\n",
    "# Define the SVM model using TensorFlow\n",
    "class SVMModel(tf.keras.Model):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SVMModel, self).__init__()\n",
    "        self.linear = tf.keras.layers.Dense(1, kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return self.linear(inputs)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "input_dim = X_train.shape[1]\n",
    "svm_model = SVMModel(input_dim)\n",
    "hinge_loss = tf.keras.losses.Hinge()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "# Define the training function\n",
    "def train_model(X_train, y_train, epochs=100):\n",
    "    for epoch in range(epochs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = svm_model(X_train)\n",
    "            loss = hinge_loss(2 * y_train - 1, logits)  # Convert 0/1 to -1/1\n",
    "            regularization_loss = tf.reduce_sum(svm_model.losses)\n",
    "            total_loss = loss + regularization_loss\n",
    "        \n",
    "        gradients = tape.gradient(total_loss, svm_model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, svm_model.trainable_variables))\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {total_loss.numpy():.4f}')\n",
    "\n",
    "# Define the evaluation function\n",
    "def evaluate_model(X_test, y_test):\n",
    "    logits = svm_model(X_test)\n",
    "    predictions = tf.sign(logits)\n",
    "    predictions = tf.squeeze(predictions)\n",
    "    y_test_numpy = y_test.numpy()\n",
    "    predictions_numpy = (predictions.numpy() + 1) / 2  # Convert -1/1 to 0/1\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test_numpy, predictions_numpy)\n",
    "    \n",
    "    # Calculate precision\n",
    "    precision = precision_score(y_test_numpy, predictions_numpy)\n",
    "    \n",
    "    # Calculate sensitivity (recall)\n",
    "    recall = recall_score(y_test_numpy, predictions_numpy)\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(y_test_numpy, predictions_numpy)\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test_numpy, predictions_numpy).ravel()\n",
    "\n",
    "    # Calculate ROC AUC\n",
    "    probabilities = tf.sigmoid(logits).numpy()\n",
    "    roc_auc = roc_auc_score(y_test_numpy, probabilities)\n",
    "    \n",
    "    return accuracy, precision, recall, f1, roc_auc, tn, fp, fn, tp\n",
    "\n",
    "# Perform 100 evaluations using bootstrapping\n",
    "num_evaluations = 100\n",
    "metrics = []\n",
    "\n",
    "for _ in range(num_evaluations):\n",
    "    # Bootstrap sampling\n",
    "    X_resampled, y_resampled = resample(X_train, y_train, n_samples=len(X_train), random_state=None)\n",
    "    \n",
    "    # Convert resampled data to TensorFlow tensors\n",
    "    X_resampled_tensor = tf.convert_to_tensor(X_resampled, dtype=tf.float32)\n",
    "    y_resampled_tensor = tf.convert_to_tensor(y_resampled.values, dtype=tf.float32)\n",
    "    \n",
    "    # Reinitialize the model, optimizer, and loss function for each iteration\n",
    "    svm_model = SVMModel(input_dim)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "    \n",
    "    # Train the model\n",
    "    train_model(X_resampled_tensor, y_resampled_tensor)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    metrics.append(evaluate_model(X_test_tensor, y_test_tensor))\n",
    "\n",
    "# Calculate average metrics\n",
    "metrics_array = np.array(metrics)\n",
    "mean_metrics = np.mean(metrics_array, axis=0)\n",
    "\n",
    "# Print the average results\n",
    "print(f'Average Accuracy: {mean_metrics[0]:.4f}')\n",
    "print(f'Average Precision: {mean_metrics[1]:.4f}')\n",
    "print(f'Average Sensitivity (Recall): {mean_metrics[2]:.4f}')\n",
    "print(f'Average F1 Score: {mean_metrics[3]:.4f}')\n",
    "print(f'Average ROC AUC: {mean_metrics[4]:.4f}')\n",
    "print(f'Average Confusion Matrix: TP={mean_metrics[8]:.0f}, TN={mean_metrics[5]:.0f}, FP={mean_metrics[6]:.0f}, FN={mean_metrics[7]:.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#release gpu memory\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#release gpu memory(tensorflow)\n",
    "import tensorflow as tf\n",
    "tf.keras.backend.clear_session()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
