{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ALL LIBRARY REQUIRED**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Descriptors cannot be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, roc_curve, auc\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sem\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jiach\\.conda\\envs\\env1\\lib\\site-packages\\tensorflow\\__init__.py:39\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_typing\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyLoader \u001b[38;5;28;01mas\u001b[39;00m _LazyLoader\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n",
      "File \u001b[1;32mc:\\Users\\jiach\\.conda\\envs\\env1\\lib\\site-packages\\tensorflow\\python\\__init__.py:37\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# We aim to keep this file minimal and ideally remove completely.\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# If you are adding a new file with @tf_export decorators,\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# import it in modules_with_exports.py instead.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# go/tf-wildcard-import\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Bring in subpackages.\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\jiach\\.conda\\envs\\env1\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:29\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msix\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m function_pb2\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_pb2\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m coordination_config_pb2\n",
      "File \u001b[1;32mc:\\Users\\jiach\\.conda\\envs\\env1\\lib\\site-packages\\tensorflow\\core\\framework\\function_pb2.py:16\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# @@protoc_insertion_point(imports)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m _sym_db \u001b[38;5;241m=\u001b[39m _symbol_database\u001b[38;5;241m.\u001b[39mDefault()\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m attr_value_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_attr__value__pb2\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m node_def_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_node__def__pb2\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m op_def_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_op__def__pb2\n",
      "File \u001b[1;32mc:\\Users\\jiach\\.conda\\envs\\env1\\lib\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py:16\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# @@protoc_insertion_point(imports)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m _sym_db \u001b[38;5;241m=\u001b[39m _symbol_database\u001b[38;5;241m.\u001b[39mDefault()\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_tensor__pb2\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_shape_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m types_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_types__pb2\n",
      "File \u001b[1;32mc:\\Users\\jiach\\.conda\\envs\\env1\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py:16\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# @@protoc_insertion_point(imports)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m _sym_db \u001b[38;5;241m=\u001b[39m _symbol_database\u001b[38;5;241m.\u001b[39mDefault()\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m resource_handle_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_resource__handle__pb2\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_shape_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m types_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_types__pb2\n",
      "File \u001b[1;32mc:\\Users\\jiach\\.conda\\envs\\env1\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py:16\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# @@protoc_insertion_point(imports)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m _sym_db \u001b[38;5;241m=\u001b[39m _symbol_database\u001b[38;5;241m.\u001b[39mDefault()\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_shape_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m types_pb2 \u001b[38;5;28;01mas\u001b[39;00m tensorflow_dot_core_dot_framework_dot_types__pb2\n\u001b[0;32m     20\u001b[0m DESCRIPTOR \u001b[38;5;241m=\u001b[39m _descriptor\u001b[38;5;241m.\u001b[39mFileDescriptor(\n\u001b[0;32m     21\u001b[0m   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow/core/framework/resource_handle.proto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     22\u001b[0m   package\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m   ,\n\u001b[0;32m     27\u001b[0m   dependencies\u001b[38;5;241m=\u001b[39m[tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\u001b[38;5;241m.\u001b[39mDESCRIPTOR,tensorflow_dot_core_dot_framework_dot_types__pb2\u001b[38;5;241m.\u001b[39mDESCRIPTOR,])\n",
      "File \u001b[1;32mc:\\Users\\jiach\\.conda\\envs\\env1\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py:36\u001b[0m\n\u001b[0;32m     13\u001b[0m _sym_db \u001b[38;5;241m=\u001b[39m _symbol_database\u001b[38;5;241m.\u001b[39mDefault()\n\u001b[0;32m     18\u001b[0m DESCRIPTOR \u001b[38;5;241m=\u001b[39m _descriptor\u001b[38;5;241m.\u001b[39mFileDescriptor(\n\u001b[0;32m     19\u001b[0m   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow/core/framework/tensor_shape.proto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     20\u001b[0m   package\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m   serialized_pb\u001b[38;5;241m=\u001b[39m_b(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m,tensorflow/core/framework/tensor_shape.proto\u001b[39m\u001b[38;5;130;01m\\x12\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124mz\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\x10\u001b[39;00m\u001b[38;5;124mTensorShapeProto\u001b[39m\u001b[38;5;130;01m\\x12\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\x03\u001b[39;00m\u001b[38;5;130;01m\\x64\u001b[39;00m\u001b[38;5;124mim\u001b[39m\u001b[38;5;130;01m\\x18\u001b[39;00m\u001b[38;5;130;01m\\x02\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\x03\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;130;01m\\x0b\u001b[39;00m\u001b[38;5;130;01m\\x32\u001b[39;00m\u001b[38;5;124m .tensorflow.TensorShapeProto.Dim\u001b[39m\u001b[38;5;130;01m\\x12\u001b[39;00m\u001b[38;5;130;01m\\x14\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\x0c\u001b[39;00m\u001b[38;5;124munknown_rank\u001b[39m\u001b[38;5;130;01m\\x18\u001b[39;00m\u001b[38;5;130;01m\\x03\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\x01\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;130;01m\\x08\u001b[39;00m\u001b[38;5;130;01m\\x1a\u001b[39;00m\u001b[38;5;124m!\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\x03\u001b[39;00m\u001b[38;5;130;01m\\x44\u001b[39;00m\u001b[38;5;124mim\u001b[39m\u001b[38;5;130;01m\\x12\u001b[39;00m\u001b[38;5;130;01m\\x0c\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\x04\u001b[39;00m\u001b[38;5;124msize\u001b[39m\u001b[38;5;130;01m\\x18\u001b[39;00m\u001b[38;5;130;01m\\x01\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\x01\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;130;01m\\x03\u001b[39;00m\u001b[38;5;130;01m\\x12\u001b[39;00m\u001b[38;5;130;01m\\x0c\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\x04\u001b[39;00m\u001b[38;5;124mname\u001b[39m\u001b[38;5;130;01m\\x18\u001b[39;00m\u001b[38;5;130;01m\\x02\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\x01\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mB\u001b[39m\u001b[38;5;130;01m\\x87\u001b[39;00m\u001b[38;5;130;01m\\x01\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\x18\u001b[39;00m\u001b[38;5;124morg.tensorflow.frameworkB\u001b[39m\u001b[38;5;130;01m\\x11\u001b[39;00m\u001b[38;5;124mTensorShapeProtosP\u001b[39m\u001b[38;5;130;01m\\x01\u001b[39;00m\u001b[38;5;124mZSgithub.com/tensorflow/tensorflow/tensorflow/go/core/framework/tensor_shape_go_proto\u001b[39m\u001b[38;5;130;01m\\xf8\u001b[39;00m\u001b[38;5;130;01m\\x01\u001b[39;00m\u001b[38;5;130;01m\\x01\u001b[39;00m\u001b[38;5;130;01m\\x62\u001b[39;00m\u001b[38;5;130;01m\\x06\u001b[39;00m\u001b[38;5;124mproto3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     24\u001b[0m )\n\u001b[0;32m     29\u001b[0m _TENSORSHAPEPROTO_DIM \u001b[38;5;241m=\u001b[39m _descriptor\u001b[38;5;241m.\u001b[39mDescriptor(\n\u001b[0;32m     30\u001b[0m   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDim\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     31\u001b[0m   full_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow.TensorShapeProto.Dim\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     32\u001b[0m   filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     33\u001b[0m   file\u001b[38;5;241m=\u001b[39mDESCRIPTOR,\n\u001b[0;32m     34\u001b[0m   containing_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     35\u001b[0m   fields\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m---> 36\u001b[0m     \u001b[43m_descriptor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFieldDescriptor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtensorflow.TensorShapeProto.Dim.size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnumber\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcpp_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m      \u001b[49m\u001b[43mhas_default_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmessage_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menum_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontaining_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m      \u001b[49m\u001b[43mis_extension\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextension_scope\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m      \u001b[49m\u001b[43mserialized_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDESCRIPTOR\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     43\u001b[0m     _descriptor\u001b[38;5;241m.\u001b[39mFieldDescriptor(\n\u001b[0;32m     44\u001b[0m       name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m, full_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow.TensorShapeProto.Dim.name\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     45\u001b[0m       number\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m9\u001b[39m, cpp_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m9\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     46\u001b[0m       has_default_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, default_value\u001b[38;5;241m=\u001b[39m_b(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     47\u001b[0m       message_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, enum_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, containing_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     48\u001b[0m       is_extension\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, extension_scope\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     49\u001b[0m       serialized_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, file\u001b[38;5;241m=\u001b[39mDESCRIPTOR),\n\u001b[0;32m     50\u001b[0m   ],\n\u001b[0;32m     51\u001b[0m   extensions\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     52\u001b[0m   ],\n\u001b[0;32m     53\u001b[0m   nested_types\u001b[38;5;241m=\u001b[39m[],\n\u001b[0;32m     54\u001b[0m   enum_types\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     55\u001b[0m   ],\n\u001b[0;32m     56\u001b[0m   serialized_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     57\u001b[0m   is_extendable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     58\u001b[0m   syntax\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproto3\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     59\u001b[0m   extension_ranges\u001b[38;5;241m=\u001b[39m[],\n\u001b[0;32m     60\u001b[0m   oneofs\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     61\u001b[0m   ],\n\u001b[0;32m     62\u001b[0m   serialized_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m149\u001b[39m,\n\u001b[0;32m     63\u001b[0m   serialized_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m182\u001b[39m,\n\u001b[0;32m     64\u001b[0m )\n\u001b[0;32m     66\u001b[0m _TENSORSHAPEPROTO \u001b[38;5;241m=\u001b[39m _descriptor\u001b[38;5;241m.\u001b[39mDescriptor(\n\u001b[0;32m     67\u001b[0m   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTensorShapeProto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     68\u001b[0m   full_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow.TensorShapeProto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    100\u001b[0m   serialized_end\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m182\u001b[39m,\n\u001b[0;32m    101\u001b[0m )\n\u001b[0;32m    103\u001b[0m _TENSORSHAPEPROTO_DIM\u001b[38;5;241m.\u001b[39mcontaining_type \u001b[38;5;241m=\u001b[39m _TENSORSHAPEPROTO\n",
      "File \u001b[1;32mc:\\Users\\jiach\\.conda\\envs\\env1\\lib\\site-packages\\google\\protobuf\\descriptor.py:553\u001b[0m, in \u001b[0;36mFieldDescriptor.__new__\u001b[1;34m(cls, name, full_name, index, number, type, cpp_type, label, default_value, message_type, enum_type, containing_type, is_extension, extension_scope, options, serialized_options, has_default_value, containing_oneof, json_name, file, create_key)\u001b[0m\n\u001b[0;32m    547\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, name, full_name, index, number, \u001b[38;5;28mtype\u001b[39m, cpp_type, label,\n\u001b[0;32m    548\u001b[0m             default_value, message_type, enum_type, containing_type,\n\u001b[0;32m    549\u001b[0m             is_extension, extension_scope, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    550\u001b[0m             serialized_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    551\u001b[0m             has_default_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, containing_oneof\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    552\u001b[0m             file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, create_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# pylint: disable=redefined-builtin\u001b[39;00m\n\u001b[1;32m--> 553\u001b[0m   \u001b[43m_message\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMessage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_CheckCalledFromGeneratedFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    554\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m is_extension:\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _message\u001b[38;5;241m.\u001b[39mdefault_pool\u001b[38;5;241m.\u001b[39mFindExtensionByName(full_name)\n",
      "\u001b[1;31mTypeError\u001b[0m: Descriptors cannot be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, roc_curve, auc\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.stats import sem\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***DATA PREPROCESSING***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jiach\\AppData\\Local\\Temp\\ipykernel_33860\\1874909134.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[col].fillna(data[col].mean(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed. Preprocessed data saved as preprocessed_data.csv\n",
      "Preprocessed data preview:\n",
      "        Age       Sex  HighChol  CholCheck       BMI  Smoker  \\\n",
      "0 -1.607237  1.090046       0.0        1.0 -0.542176     0.0   \n",
      "1  1.197681  1.090046       1.0        1.0 -0.542176     1.0   \n",
      "2  1.548296  1.090046       0.0        1.0 -0.542176     0.0   \n",
      "3  0.847066  1.090046       1.0        1.0 -0.261036     1.0   \n",
      "4 -0.204778 -0.917392       0.0        1.0 -0.120466     1.0   \n",
      "\n",
      "   HeartDiseaseorAttack  PhysActivity  Fruits  Veggies  HvyAlcoholConsump  \\\n",
      "0                   0.0           1.0     0.0      1.0                0.0   \n",
      "1                   0.0           0.0     1.0      0.0                0.0   \n",
      "2                   0.0           1.0     1.0      1.0                0.0   \n",
      "3                   0.0           1.0     1.0      1.0                0.0   \n",
      "4                   0.0           1.0     1.0      1.0                0.0   \n",
      "\n",
      "    GenHlth  MentHlth  PhysHlth  DiffWalk  Stroke  HighBP  Diabetes  \n",
      "0  0.146304  0.153020  2.404008       0.0     0.0     1.0       0.0  \n",
      "1  0.146304 -0.460058 -0.577451       0.0     1.0     1.0       0.0  \n",
      "2 -1.649743 -0.460058  0.416369       0.0     0.0     0.0       0.0  \n",
      "3  0.146304 -0.460058 -0.279305       0.0     0.0     1.0       0.0  \n",
      "4 -0.751719 -0.460058 -0.577451       0.0     0.0     0.0       0.0   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "url = 'https://p16-bot-sign-sg.ciciai.com/tos-alisg-i-b2l6bve69y-sg/0a542996a2e74931b9a17cc3fb593feb.csv~tplv-b2l6bve69y-image.image?rk3s=68e6b6b5&x-expires=1719816096&x-signature=hvoff8lKbCLE4lp7Nfk5Cir9yn4%3D'\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Handling Missing Values\n",
    "# Fill missing values with the mean of the column\n",
    "numerical_cols = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "ordinal_binary_cols = ['HighChol', 'CholCheck', 'Smoker', 'HeartDiseaseorAttack', 'PhysActivity', 'Fruits', 'Veggies', 'HvyAlcoholConsump', 'DiffWalk', 'Stroke', 'HighBP', 'Diabetes']\n",
    "\n",
    "for col in numerical_cols:\n",
    "    if col not in ordinal_binary_cols:\n",
    "        data[col].fillna(data[col].mean(), inplace=True)\n",
    "\n",
    "# Normalizing Numerical Features\n",
    "scaler = StandardScaler()\n",
    "for col in numerical_cols:\n",
    "    if col not in ordinal_binary_cols:\n",
    "        data[col] = scaler.fit_transform(data[[col]])\n",
    "\n",
    "# Save the preprocessed dataset\n",
    "data.to_csv('preprocessed_data.csv', index=False)\n",
    "\n",
    "print('Preprocessing completed. Preprocessed data saved as preprocessed_data.csv')\n",
    "print('Preprocessed data preview:')\n",
    "print(data.head(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***SAMPLING***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant features related to eating habits and other relevant variables\n",
    "features = ['Fruits', 'Veggies', 'HvyAlcoholConsump', 'Age', 'BMI', 'Smoker']\n",
    "X = data[features]\n",
    "y = data['Diabetes']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert data to TensorFlow tensors\n",
    "X_train_tensor = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "X_test_tensor = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
    "y_train_tensor = tf.convert_to_tensor(y_train.values, dtype=tf.float32)\n",
    "y_test_tensor = tf.convert_to_tensor(y_test.values, dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***MODEL BUILDING*** ***(LOGITIC REGRESSION)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionModel(tf.keras.Model):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return self.linear(inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***MODEL BUILDING (SVM)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVMModel(tf.keras.Model):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SVMModel, self).__init__()\n",
    "        self.linear = tf.keras.layers.Dense(1, kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return self.linear(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Initialize the models, loss functions, and optimizer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "binary_crossentropy = tf.keras.losses.BinaryCrossentropy()\n",
    "hinge_loss = tf.keras.losses.Hinge()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***MODEL_TRAINING*** ***(LOGITIC REGRESSION)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, loss_fn, epochs=100):\n",
    "    for epoch in range(epochs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(X_train)\n",
    "            loss = loss_fn(y_train, logits)\n",
    "            regularization_loss = tf.reduce_sum(model.losses)\n",
    "            total_loss = loss + regularization_loss\n",
    "        \n",
    "        gradients = tape.gradient(total_loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {total_loss.numpy():.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODEL EVALUATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    logits = model(X_test)\n",
    "    predictions = tf.round(tf.sigmoid(logits)) if isinstance(model, LogisticRegressionModel) else tf.sign(logits)\n",
    "    predictions = tf.squeeze(predictions)\n",
    "    y_test_numpy = y_test.numpy()\n",
    "    predictions_numpy = predictions.numpy() if isinstance(model, LogisticRegressionModel) else (predictions.numpy() + 1) / 2  # Convert -1/1 to 0/1\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test_numpy, predictions_numpy)\n",
    "    \n",
    "    # Calculate precision\n",
    "    precision = precision_score(y_test_numpy, predictions_numpy)\n",
    "    \n",
    "    # Calculate sensitivity (recall)\n",
    "    recall = recall_score(y_test_numpy, predictions_numpy)\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(y_test_numpy, predictions_numpy)\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test_numpy, predictions_numpy).ravel()\n",
    "\n",
    "    # Calculate ROC AUC\n",
    "    probabilities = tf.sigmoid(logits).numpy()\n",
    "    roc_auc = roc_auc_score(y_test_numpy, probabilities)\n",
    "    \n",
    "    return accuracy, precision, recall, f1, roc_auc, tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***100 evaluations using bootstrapping***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.6714\n",
      "Epoch [20/100], Loss: 0.6452\n",
      "Epoch [30/100], Loss: 0.6260\n",
      "Epoch [40/100], Loss: 0.6132\n",
      "Epoch [50/100], Loss: 0.6051\n",
      "Epoch [60/100], Loss: 0.6001\n",
      "Epoch [70/100], Loss: 0.5970\n",
      "Epoch [80/100], Loss: 0.5949\n",
      "Epoch [90/100], Loss: 0.5935\n",
      "Epoch [100/100], Loss: 0.5927\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 1.0259\n",
      "Epoch [20/100], Loss: 0.8732\n",
      "Epoch [30/100], Loss: 0.8006\n",
      "Epoch [40/100], Loss: 0.7663\n",
      "Epoch [50/100], Loss: 0.7461\n",
      "Epoch [60/100], Loss: 0.7355\n",
      "Epoch [70/100], Loss: 0.7308\n",
      "Epoch [80/100], Loss: 0.7284\n",
      "Epoch [90/100], Loss: 0.7272\n",
      "Epoch [100/100], Loss: 0.7267\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.8638\n",
      "Epoch [20/100], Loss: 0.8028\n",
      "Epoch [30/100], Loss: 0.7525\n",
      "Epoch [40/100], Loss: 0.7133\n",
      "Epoch [50/100], Loss: 0.6839\n",
      "Epoch [60/100], Loss: 0.6624\n",
      "Epoch [70/100], Loss: 0.6465\n",
      "Epoch [80/100], Loss: 0.6346\n",
      "Epoch [90/100], Loss: 0.6253\n",
      "Epoch [100/100], Loss: 0.6179\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 1.2297\n",
      "Epoch [20/100], Loss: 1.0383\n",
      "Epoch [30/100], Loss: 0.9180\n",
      "Epoch [40/100], Loss: 0.8463\n",
      "Epoch [50/100], Loss: 0.8028\n",
      "Epoch [60/100], Loss: 0.7745\n",
      "Epoch [70/100], Loss: 0.7560\n",
      "Epoch [80/100], Loss: 0.7444\n",
      "Epoch [90/100], Loss: 0.7374\n",
      "Epoch [100/100], Loss: 0.7331\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.7767\n",
      "Epoch [20/100], Loss: 0.7284\n",
      "Epoch [30/100], Loss: 0.6933\n",
      "Epoch [40/100], Loss: 0.6683\n",
      "Epoch [50/100], Loss: 0.6499\n",
      "Epoch [60/100], Loss: 0.6356\n",
      "Epoch [70/100], Loss: 0.6246\n",
      "Epoch [80/100], Loss: 0.6161\n",
      "Epoch [90/100], Loss: 0.6095\n",
      "Epoch [100/100], Loss: 0.6044\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 0.8593\n",
      "Epoch [20/100], Loss: 0.7661\n",
      "Epoch [30/100], Loss: 0.7357\n",
      "Epoch [40/100], Loss: 0.7257\n",
      "Epoch [50/100], Loss: 0.7233\n",
      "Epoch [60/100], Loss: 0.7223\n",
      "Epoch [70/100], Loss: 0.7217\n",
      "Epoch [80/100], Loss: 0.7215\n",
      "Epoch [90/100], Loss: 0.7215\n",
      "Epoch [100/100], Loss: 0.7215\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.9133\n",
      "Epoch [20/100], Loss: 0.8523\n",
      "Epoch [30/100], Loss: 0.8006\n",
      "Epoch [40/100], Loss: 0.7581\n",
      "Epoch [50/100], Loss: 0.7235\n",
      "Epoch [60/100], Loss: 0.6956\n",
      "Epoch [70/100], Loss: 0.6731\n",
      "Epoch [80/100], Loss: 0.6553\n",
      "Epoch [90/100], Loss: 0.6412\n",
      "Epoch [100/100], Loss: 0.6303\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 1.1098\n",
      "Epoch [20/100], Loss: 0.9856\n",
      "Epoch [30/100], Loss: 0.8957\n",
      "Epoch [40/100], Loss: 0.8335\n",
      "Epoch [50/100], Loss: 0.7919\n",
      "Epoch [60/100], Loss: 0.7664\n",
      "Epoch [70/100], Loss: 0.7504\n",
      "Epoch [80/100], Loss: 0.7406\n",
      "Epoch [90/100], Loss: 0.7346\n",
      "Epoch [100/100], Loss: 0.7308\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.7596\n",
      "Epoch [20/100], Loss: 0.7156\n",
      "Epoch [30/100], Loss: 0.6808\n",
      "Epoch [40/100], Loss: 0.6546\n",
      "Epoch [50/100], Loss: 0.6357\n",
      "Epoch [60/100], Loss: 0.6225\n",
      "Epoch [70/100], Loss: 0.6133\n",
      "Epoch [80/100], Loss: 0.6069\n",
      "Epoch [90/100], Loss: 0.6023\n",
      "Epoch [100/100], Loss: 0.5991\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 0.8878\n",
      "Epoch [20/100], Loss: 0.8030\n",
      "Epoch [30/100], Loss: 0.7652\n",
      "Epoch [40/100], Loss: 0.7467\n",
      "Epoch [50/100], Loss: 0.7372\n",
      "Epoch [60/100], Loss: 0.7319\n",
      "Epoch [70/100], Loss: 0.7294\n",
      "Epoch [80/100], Loss: 0.7284\n",
      "Epoch [90/100], Loss: 0.7280\n",
      "Epoch [100/100], Loss: 0.7279\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.9554\n",
      "Epoch [20/100], Loss: 0.8870\n",
      "Epoch [30/100], Loss: 0.8283\n",
      "Epoch [40/100], Loss: 0.7788\n",
      "Epoch [50/100], Loss: 0.7385\n",
      "Epoch [60/100], Loss: 0.7064\n",
      "Epoch [70/100], Loss: 0.6811\n",
      "Epoch [80/100], Loss: 0.6614\n",
      "Epoch [90/100], Loss: 0.6461\n",
      "Epoch [100/100], Loss: 0.6342\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 0.8308\n",
      "Epoch [20/100], Loss: 0.7660\n",
      "Epoch [30/100], Loss: 0.7350\n",
      "Epoch [40/100], Loss: 0.7241\n",
      "Epoch [50/100], Loss: 0.7222\n",
      "Epoch [60/100], Loss: 0.7221\n",
      "Epoch [70/100], Loss: 0.7220\n",
      "Epoch [80/100], Loss: 0.7219\n",
      "Epoch [90/100], Loss: 0.7218\n",
      "Epoch [100/100], Loss: 0.7218\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.8492\n",
      "Epoch [20/100], Loss: 0.7948\n",
      "Epoch [30/100], Loss: 0.7494\n",
      "Epoch [40/100], Loss: 0.7126\n",
      "Epoch [50/100], Loss: 0.6838\n",
      "Epoch [60/100], Loss: 0.6618\n",
      "Epoch [70/100], Loss: 0.6453\n",
      "Epoch [80/100], Loss: 0.6330\n",
      "Epoch [90/100], Loss: 0.6238\n",
      "Epoch [100/100], Loss: 0.6168\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 0.7916\n",
      "Epoch [20/100], Loss: 0.7504\n",
      "Epoch [30/100], Loss: 0.7343\n",
      "Epoch [40/100], Loss: 0.7304\n",
      "Epoch [50/100], Loss: 0.7300\n",
      "Epoch [60/100], Loss: 0.7301\n",
      "Epoch [70/100], Loss: 0.7300\n",
      "Epoch [80/100], Loss: 0.7299\n",
      "Epoch [90/100], Loss: 0.7299\n",
      "Epoch [100/100], Loss: 0.7299\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.6525\n",
      "Epoch [20/100], Loss: 0.6271\n",
      "Epoch [30/100], Loss: 0.6103\n",
      "Epoch [40/100], Loss: 0.5997\n",
      "Epoch [50/100], Loss: 0.5936\n",
      "Epoch [60/100], Loss: 0.5903\n",
      "Epoch [70/100], Loss: 0.5887\n",
      "Epoch [80/100], Loss: 0.5878\n",
      "Epoch [90/100], Loss: 0.5873\n",
      "Epoch [100/100], Loss: 0.5871\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 0.8587\n",
      "Epoch [20/100], Loss: 0.7804\n",
      "Epoch [30/100], Loss: 0.7413\n",
      "Epoch [40/100], Loss: 0.7262\n",
      "Epoch [50/100], Loss: 0.7218\n",
      "Epoch [60/100], Loss: 0.7204\n",
      "Epoch [70/100], Loss: 0.7201\n",
      "Epoch [80/100], Loss: 0.7200\n",
      "Epoch [90/100], Loss: 0.7200\n",
      "Epoch [100/100], Loss: 0.7200\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.9063\n",
      "Epoch [20/100], Loss: 0.8398\n",
      "Epoch [30/100], Loss: 0.7834\n",
      "Epoch [40/100], Loss: 0.7375\n",
      "Epoch [50/100], Loss: 0.7012\n",
      "Epoch [60/100], Loss: 0.6731\n",
      "Epoch [70/100], Loss: 0.6519\n",
      "Epoch [80/100], Loss: 0.6361\n",
      "Epoch [90/100], Loss: 0.6245\n",
      "Epoch [100/100], Loss: 0.6161\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 0.8347\n",
      "Epoch [20/100], Loss: 0.7760\n",
      "Epoch [30/100], Loss: 0.7401\n",
      "Epoch [40/100], Loss: 0.7281\n",
      "Epoch [50/100], Loss: 0.7260\n",
      "Epoch [60/100], Loss: 0.7259\n",
      "Epoch [70/100], Loss: 0.7259\n",
      "Epoch [80/100], Loss: 0.7258\n",
      "Epoch [90/100], Loss: 0.7257\n",
      "Epoch [100/100], Loss: 0.7257\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.9229\n",
      "Epoch [20/100], Loss: 0.8596\n",
      "Epoch [30/100], Loss: 0.8056\n",
      "Epoch [40/100], Loss: 0.7607\n",
      "Epoch [50/100], Loss: 0.7251\n",
      "Epoch [60/100], Loss: 0.6974\n",
      "Epoch [70/100], Loss: 0.6756\n",
      "Epoch [80/100], Loss: 0.6583\n",
      "Epoch [90/100], Loss: 0.6445\n",
      "Epoch [100/100], Loss: 0.6334\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 0.8598\n",
      "Epoch [20/100], Loss: 0.7851\n",
      "Epoch [30/100], Loss: 0.7456\n",
      "Epoch [40/100], Loss: 0.7329\n",
      "Epoch [50/100], Loss: 0.7297\n",
      "Epoch [60/100], Loss: 0.7285\n",
      "Epoch [70/100], Loss: 0.7282\n",
      "Epoch [80/100], Loss: 0.7281\n",
      "Epoch [90/100], Loss: 0.7281\n",
      "Epoch [100/100], Loss: 0.7281\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.8333\n",
      "Epoch [20/100], Loss: 0.7924\n",
      "Epoch [30/100], Loss: 0.7555\n",
      "Epoch [40/100], Loss: 0.7237\n",
      "Epoch [50/100], Loss: 0.6970\n",
      "Epoch [60/100], Loss: 0.6752\n",
      "Epoch [70/100], Loss: 0.6578\n",
      "Epoch [80/100], Loss: 0.6441\n",
      "Epoch [90/100], Loss: 0.6331\n",
      "Epoch [100/100], Loss: 0.6243\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 1.0334\n",
      "Epoch [20/100], Loss: 0.8737\n",
      "Epoch [30/100], Loss: 0.7912\n",
      "Epoch [40/100], Loss: 0.7517\n",
      "Epoch [50/100], Loss: 0.7385\n",
      "Epoch [60/100], Loss: 0.7303\n",
      "Epoch [70/100], Loss: 0.7257\n",
      "Epoch [80/100], Loss: 0.7236\n",
      "Epoch [90/100], Loss: 0.7224\n",
      "Epoch [100/100], Loss: 0.7218\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 1.1466\n",
      "Epoch [20/100], Loss: 1.0383\n",
      "Epoch [30/100], Loss: 0.9402\n",
      "Epoch [40/100], Loss: 0.8550\n",
      "Epoch [50/100], Loss: 0.7845\n",
      "Epoch [60/100], Loss: 0.7294\n",
      "Epoch [70/100], Loss: 0.6888\n",
      "Epoch [80/100], Loss: 0.6603\n",
      "Epoch [90/100], Loss: 0.6409\n",
      "Epoch [100/100], Loss: 0.6277\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 1.2866\n",
      "Epoch [20/100], Loss: 1.1206\n",
      "Epoch [30/100], Loss: 0.9901\n",
      "Epoch [40/100], Loss: 0.8912\n",
      "Epoch [50/100], Loss: 0.8222\n",
      "Epoch [60/100], Loss: 0.7806\n",
      "Epoch [70/100], Loss: 0.7568\n",
      "Epoch [80/100], Loss: 0.7438\n",
      "Epoch [90/100], Loss: 0.7363\n",
      "Epoch [100/100], Loss: 0.7317\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 1.0352\n",
      "Epoch [20/100], Loss: 0.9438\n",
      "Epoch [30/100], Loss: 0.8654\n",
      "Epoch [40/100], Loss: 0.8012\n",
      "Epoch [50/100], Loss: 0.7510\n",
      "Epoch [60/100], Loss: 0.7129\n",
      "Epoch [70/100], Loss: 0.6846\n",
      "Epoch [80/100], Loss: 0.6634\n",
      "Epoch [90/100], Loss: 0.6475\n",
      "Epoch [100/100], Loss: 0.6354\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 0.9134\n",
      "Epoch [20/100], Loss: 0.8029\n",
      "Epoch [30/100], Loss: 0.7493\n",
      "Epoch [40/100], Loss: 0.7299\n",
      "Epoch [50/100], Loss: 0.7245\n",
      "Epoch [60/100], Loss: 0.7236\n",
      "Epoch [70/100], Loss: 0.7236\n",
      "Epoch [80/100], Loss: 0.7236\n",
      "Epoch [90/100], Loss: 0.7235\n",
      "Epoch [100/100], Loss: 0.7234\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.8046\n",
      "Epoch [20/100], Loss: 0.7575\n",
      "Epoch [30/100], Loss: 0.7199\n",
      "Epoch [40/100], Loss: 0.6909\n",
      "Epoch [50/100], Loss: 0.6683\n",
      "Epoch [60/100], Loss: 0.6507\n",
      "Epoch [70/100], Loss: 0.6373\n",
      "Epoch [80/100], Loss: 0.6269\n",
      "Epoch [90/100], Loss: 0.6189\n",
      "Epoch [100/100], Loss: 0.6127\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 1.0210\n",
      "Epoch [20/100], Loss: 0.8633\n",
      "Epoch [30/100], Loss: 0.7701\n",
      "Epoch [40/100], Loss: 0.7388\n",
      "Epoch [50/100], Loss: 0.7336\n",
      "Epoch [60/100], Loss: 0.7328\n",
      "Epoch [70/100], Loss: 0.7322\n",
      "Epoch [80/100], Loss: 0.7319\n",
      "Epoch [90/100], Loss: 0.7318\n",
      "Epoch [100/100], Loss: 0.7318\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.8097\n",
      "Epoch [20/100], Loss: 0.7560\n",
      "Epoch [30/100], Loss: 0.7151\n",
      "Epoch [40/100], Loss: 0.6844\n",
      "Epoch [50/100], Loss: 0.6610\n",
      "Epoch [60/100], Loss: 0.6433\n",
      "Epoch [70/100], Loss: 0.6300\n",
      "Epoch [80/100], Loss: 0.6201\n",
      "Epoch [90/100], Loss: 0.6128\n",
      "Epoch [100/100], Loss: 0.6074\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 0.7733\n",
      "Epoch [20/100], Loss: 0.7365\n",
      "Epoch [30/100], Loss: 0.7284\n",
      "Epoch [40/100], Loss: 0.7273\n",
      "Epoch [50/100], Loss: 0.7269\n",
      "Epoch [60/100], Loss: 0.7267\n",
      "Epoch [70/100], Loss: 0.7266\n",
      "Epoch [80/100], Loss: 0.7265\n",
      "Epoch [90/100], Loss: 0.7265\n",
      "Epoch [100/100], Loss: 0.7265\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.9359\n",
      "Epoch [20/100], Loss: 0.8628\n",
      "Epoch [30/100], Loss: 0.8006\n",
      "Epoch [40/100], Loss: 0.7493\n",
      "Epoch [50/100], Loss: 0.7084\n",
      "Epoch [60/100], Loss: 0.6770\n",
      "Epoch [70/100], Loss: 0.6536\n",
      "Epoch [80/100], Loss: 0.6364\n",
      "Epoch [90/100], Loss: 0.6238\n",
      "Epoch [100/100], Loss: 0.6145\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 1.0717\n",
      "Epoch [20/100], Loss: 0.9541\n",
      "Epoch [30/100], Loss: 0.8649\n",
      "Epoch [40/100], Loss: 0.8121\n",
      "Epoch [50/100], Loss: 0.7774\n",
      "Epoch [60/100], Loss: 0.7553\n",
      "Epoch [70/100], Loss: 0.7423\n",
      "Epoch [80/100], Loss: 0.7347\n",
      "Epoch [90/100], Loss: 0.7302\n",
      "Epoch [100/100], Loss: 0.7273\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.7233\n",
      "Epoch [20/100], Loss: 0.6838\n",
      "Epoch [30/100], Loss: 0.6543\n",
      "Epoch [40/100], Loss: 0.6328\n",
      "Epoch [50/100], Loss: 0.6178\n",
      "Epoch [60/100], Loss: 0.6079\n",
      "Epoch [70/100], Loss: 0.6015\n",
      "Epoch [80/100], Loss: 0.5976\n",
      "Epoch [90/100], Loss: 0.5952\n",
      "Epoch [100/100], Loss: 0.5938\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 0.9609\n",
      "Epoch [20/100], Loss: 0.8324\n",
      "Epoch [30/100], Loss: 0.7655\n",
      "Epoch [40/100], Loss: 0.7377\n",
      "Epoch [50/100], Loss: 0.7303\n",
      "Epoch [60/100], Loss: 0.7295\n",
      "Epoch [70/100], Loss: 0.7295\n",
      "Epoch [80/100], Loss: 0.7294\n",
      "Epoch [90/100], Loss: 0.7293\n",
      "Epoch [100/100], Loss: 0.7293\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.6306\n",
      "Epoch [20/100], Loss: 0.6158\n",
      "Epoch [30/100], Loss: 0.6062\n",
      "Epoch [40/100], Loss: 0.5995\n",
      "Epoch [50/100], Loss: 0.5951\n",
      "Epoch [60/100], Loss: 0.5921\n",
      "Epoch [70/100], Loss: 0.5901\n",
      "Epoch [80/100], Loss: 0.5890\n",
      "Epoch [90/100], Loss: 0.5883\n",
      "Epoch [100/100], Loss: 0.5880\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 0.9736\n",
      "Epoch [20/100], Loss: 0.8526\n",
      "Epoch [30/100], Loss: 0.7825\n",
      "Epoch [40/100], Loss: 0.7490\n",
      "Epoch [50/100], Loss: 0.7329\n",
      "Epoch [60/100], Loss: 0.7257\n",
      "Epoch [70/100], Loss: 0.7231\n",
      "Epoch [80/100], Loss: 0.7223\n",
      "Epoch [90/100], Loss: 0.7221\n",
      "Epoch [100/100], Loss: 0.7220\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 1.0584\n",
      "Epoch [20/100], Loss: 0.9811\n",
      "Epoch [30/100], Loss: 0.9102\n",
      "Epoch [40/100], Loss: 0.8472\n",
      "Epoch [50/100], Loss: 0.7927\n",
      "Epoch [60/100], Loss: 0.7470\n",
      "Epoch [70/100], Loss: 0.7099\n",
      "Epoch [80/100], Loss: 0.6810\n",
      "Epoch [90/100], Loss: 0.6589\n",
      "Epoch [100/100], Loss: 0.6425\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 0.7666\n",
      "Epoch [20/100], Loss: 0.7429\n",
      "Epoch [30/100], Loss: 0.7308\n",
      "Epoch [40/100], Loss: 0.7272\n",
      "Epoch [50/100], Loss: 0.7268\n",
      "Epoch [60/100], Loss: 0.7268\n",
      "Epoch [70/100], Loss: 0.7267\n",
      "Epoch [80/100], Loss: 0.7267\n",
      "Epoch [90/100], Loss: 0.7267\n",
      "Epoch [100/100], Loss: 0.7267\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.9237\n",
      "Epoch [20/100], Loss: 0.8695\n",
      "Epoch [30/100], Loss: 0.8218\n",
      "Epoch [40/100], Loss: 0.7800\n",
      "Epoch [50/100], Loss: 0.7446\n",
      "Epoch [60/100], Loss: 0.7156\n",
      "Epoch [70/100], Loss: 0.6920\n",
      "Epoch [80/100], Loss: 0.6730\n",
      "Epoch [90/100], Loss: 0.6576\n",
      "Epoch [100/100], Loss: 0.6450\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 0.9787\n",
      "Epoch [20/100], Loss: 0.8549\n",
      "Epoch [30/100], Loss: 0.7803\n",
      "Epoch [40/100], Loss: 0.7478\n",
      "Epoch [50/100], Loss: 0.7332\n",
      "Epoch [60/100], Loss: 0.7259\n",
      "Epoch [70/100], Loss: 0.7226\n",
      "Epoch [80/100], Loss: 0.7211\n",
      "Epoch [90/100], Loss: 0.7204\n",
      "Epoch [100/100], Loss: 0.7201\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.9157\n",
      "Epoch [20/100], Loss: 0.8525\n",
      "Epoch [30/100], Loss: 0.7985\n",
      "Epoch [40/100], Loss: 0.7541\n",
      "Epoch [50/100], Loss: 0.7187\n",
      "Epoch [60/100], Loss: 0.6907\n",
      "Epoch [70/100], Loss: 0.6687\n",
      "Epoch [80/100], Loss: 0.6516\n",
      "Epoch [90/100], Loss: 0.6382\n",
      "Epoch [100/100], Loss: 0.6277\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 1.0639\n",
      "Epoch [20/100], Loss: 0.9269\n",
      "Epoch [30/100], Loss: 0.8351\n",
      "Epoch [40/100], Loss: 0.7848\n",
      "Epoch [50/100], Loss: 0.7575\n",
      "Epoch [60/100], Loss: 0.7431\n",
      "Epoch [70/100], Loss: 0.7346\n",
      "Epoch [80/100], Loss: 0.7294\n",
      "Epoch [90/100], Loss: 0.7264\n",
      "Epoch [100/100], Loss: 0.7247\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.8343\n",
      "Epoch [20/100], Loss: 0.7956\n",
      "Epoch [30/100], Loss: 0.7640\n",
      "Epoch [40/100], Loss: 0.7376\n",
      "Epoch [50/100], Loss: 0.7147\n",
      "Epoch [60/100], Loss: 0.6946\n",
      "Epoch [70/100], Loss: 0.6772\n",
      "Epoch [80/100], Loss: 0.6624\n",
      "Epoch [90/100], Loss: 0.6497\n",
      "Epoch [100/100], Loss: 0.6391\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 0.9921\n",
      "Epoch [20/100], Loss: 0.8616\n",
      "Epoch [30/100], Loss: 0.7814\n",
      "Epoch [40/100], Loss: 0.7465\n",
      "Epoch [50/100], Loss: 0.7328\n",
      "Epoch [60/100], Loss: 0.7279\n",
      "Epoch [70/100], Loss: 0.7266\n",
      "Epoch [80/100], Loss: 0.7263\n",
      "Epoch [90/100], Loss: 0.7261\n",
      "Epoch [100/100], Loss: 0.7261\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.8828\n",
      "Epoch [20/100], Loss: 0.8242\n",
      "Epoch [30/100], Loss: 0.7758\n",
      "Epoch [40/100], Loss: 0.7359\n",
      "Epoch [50/100], Loss: 0.7030\n",
      "Epoch [60/100], Loss: 0.6762\n",
      "Epoch [70/100], Loss: 0.6552\n",
      "Epoch [80/100], Loss: 0.6393\n",
      "Epoch [90/100], Loss: 0.6276\n",
      "Epoch [100/100], Loss: 0.6191\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 1.1995\n",
      "Epoch [20/100], Loss: 1.0480\n",
      "Epoch [30/100], Loss: 0.9329\n",
      "Epoch [40/100], Loss: 0.8541\n",
      "Epoch [50/100], Loss: 0.8069\n",
      "Epoch [60/100], Loss: 0.7791\n",
      "Epoch [70/100], Loss: 0.7612\n",
      "Epoch [80/100], Loss: 0.7494\n",
      "Epoch [90/100], Loss: 0.7418\n",
      "Epoch [100/100], Loss: 0.7368\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.6157\n",
      "Epoch [20/100], Loss: 0.6062\n",
      "Epoch [30/100], Loss: 0.5999\n",
      "Epoch [40/100], Loss: 0.5956\n",
      "Epoch [50/100], Loss: 0.5928\n",
      "Epoch [60/100], Loss: 0.5912\n",
      "Epoch [70/100], Loss: 0.5903\n",
      "Epoch [80/100], Loss: 0.5900\n",
      "Epoch [90/100], Loss: 0.5900\n",
      "Epoch [100/100], Loss: 0.5900\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 0.8296\n",
      "Epoch [20/100], Loss: 0.7657\n",
      "Epoch [30/100], Loss: 0.7360\n",
      "Epoch [40/100], Loss: 0.7271\n",
      "Epoch [50/100], Loss: 0.7253\n",
      "Epoch [60/100], Loss: 0.7252\n",
      "Epoch [70/100], Loss: 0.7253\n",
      "Epoch [80/100], Loss: 0.7252\n",
      "Epoch [90/100], Loss: 0.7252\n",
      "Epoch [100/100], Loss: 0.7252\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.7518\n",
      "Epoch [20/100], Loss: 0.7091\n",
      "Epoch [30/100], Loss: 0.6753\n",
      "Epoch [40/100], Loss: 0.6488\n",
      "Epoch [50/100], Loss: 0.6293\n",
      "Epoch [60/100], Loss: 0.6157\n",
      "Epoch [70/100], Loss: 0.6066\n",
      "Epoch [80/100], Loss: 0.6005\n",
      "Epoch [90/100], Loss: 0.5965\n",
      "Epoch [100/100], Loss: 0.5939\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 1.2179\n",
      "Epoch [20/100], Loss: 1.0483\n",
      "Epoch [30/100], Loss: 0.9281\n",
      "Epoch [40/100], Loss: 0.8566\n",
      "Epoch [50/100], Loss: 0.8136\n",
      "Epoch [60/100], Loss: 0.7853\n",
      "Epoch [70/100], Loss: 0.7658\n",
      "Epoch [80/100], Loss: 0.7525\n",
      "Epoch [90/100], Loss: 0.7436\n",
      "Epoch [100/100], Loss: 0.7376\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.8768\n",
      "Epoch [20/100], Loss: 0.8304\n",
      "Epoch [30/100], Loss: 0.7900\n",
      "Epoch [40/100], Loss: 0.7556\n",
      "Epoch [50/100], Loss: 0.7265\n",
      "Epoch [60/100], Loss: 0.7021\n",
      "Epoch [70/100], Loss: 0.6817\n",
      "Epoch [80/100], Loss: 0.6648\n",
      "Epoch [90/100], Loss: 0.6507\n",
      "Epoch [100/100], Loss: 0.6392\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 0.7958\n",
      "Epoch [20/100], Loss: 0.7550\n",
      "Epoch [30/100], Loss: 0.7362\n",
      "Epoch [40/100], Loss: 0.7273\n",
      "Epoch [50/100], Loss: 0.7240\n",
      "Epoch [60/100], Loss: 0.7235\n",
      "Epoch [70/100], Loss: 0.7233\n",
      "Epoch [80/100], Loss: 0.7234\n",
      "Epoch [90/100], Loss: 0.7234\n",
      "Epoch [100/100], Loss: 0.7233\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.7201\n",
      "Epoch [20/100], Loss: 0.6775\n",
      "Epoch [30/100], Loss: 0.6492\n",
      "Epoch [40/100], Loss: 0.6308\n",
      "Epoch [50/100], Loss: 0.6185\n",
      "Epoch [60/100], Loss: 0.6102\n",
      "Epoch [70/100], Loss: 0.6046\n",
      "Epoch [80/100], Loss: 0.6007\n",
      "Epoch [90/100], Loss: 0.5980\n",
      "Epoch [100/100], Loss: 0.5961\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 1.3075\n",
      "Epoch [20/100], Loss: 1.1310\n",
      "Epoch [30/100], Loss: 0.9917\n",
      "Epoch [40/100], Loss: 0.8962\n",
      "Epoch [50/100], Loss: 0.8339\n",
      "Epoch [60/100], Loss: 0.7906\n",
      "Epoch [70/100], Loss: 0.7651\n",
      "Epoch [80/100], Loss: 0.7512\n",
      "Epoch [90/100], Loss: 0.7427\n",
      "Epoch [100/100], Loss: 0.7373\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.7585\n",
      "Epoch [20/100], Loss: 0.7113\n",
      "Epoch [30/100], Loss: 0.6745\n",
      "Epoch [40/100], Loss: 0.6475\n",
      "Epoch [50/100], Loss: 0.6288\n",
      "Epoch [60/100], Loss: 0.6161\n",
      "Epoch [70/100], Loss: 0.6077\n",
      "Epoch [80/100], Loss: 0.6020\n",
      "Epoch [90/100], Loss: 0.5983\n",
      "Epoch [100/100], Loss: 0.5958\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 1.2512\n",
      "Epoch [20/100], Loss: 1.0954\n",
      "Epoch [30/100], Loss: 0.9783\n",
      "Epoch [40/100], Loss: 0.8890\n",
      "Epoch [50/100], Loss: 0.8280\n",
      "Epoch [60/100], Loss: 0.7869\n",
      "Epoch [70/100], Loss: 0.7605\n",
      "Epoch [80/100], Loss: 0.7464\n",
      "Epoch [90/100], Loss: 0.7387\n",
      "Epoch [100/100], Loss: 0.7345\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.9101\n",
      "Epoch [20/100], Loss: 0.8482\n",
      "Epoch [30/100], Loss: 0.7960\n",
      "Epoch [40/100], Loss: 0.7531\n",
      "Epoch [50/100], Loss: 0.7190\n",
      "Epoch [60/100], Loss: 0.6928\n",
      "Epoch [70/100], Loss: 0.6728\n",
      "Epoch [80/100], Loss: 0.6574\n",
      "Epoch [90/100], Loss: 0.6452\n",
      "Epoch [100/100], Loss: 0.6352\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 1.0341\n",
      "Epoch [20/100], Loss: 0.8919\n",
      "Epoch [30/100], Loss: 0.8006\n",
      "Epoch [40/100], Loss: 0.7545\n",
      "Epoch [50/100], Loss: 0.7356\n",
      "Epoch [60/100], Loss: 0.7293\n",
      "Epoch [70/100], Loss: 0.7276\n",
      "Epoch [80/100], Loss: 0.7274\n",
      "Epoch [90/100], Loss: 0.7274\n",
      "Epoch [100/100], Loss: 0.7273\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.7411\n",
      "Epoch [20/100], Loss: 0.6962\n",
      "Epoch [30/100], Loss: 0.6628\n",
      "Epoch [40/100], Loss: 0.6394\n",
      "Epoch [50/100], Loss: 0.6232\n",
      "Epoch [60/100], Loss: 0.6119\n",
      "Epoch [70/100], Loss: 0.6041\n",
      "Epoch [80/100], Loss: 0.5989\n",
      "Epoch [90/100], Loss: 0.5954\n",
      "Epoch [100/100], Loss: 0.5931\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 1.0639\n",
      "Epoch [20/100], Loss: 0.9180\n",
      "Epoch [30/100], Loss: 0.8310\n",
      "Epoch [40/100], Loss: 0.7823\n",
      "Epoch [50/100], Loss: 0.7545\n",
      "Epoch [60/100], Loss: 0.7405\n",
      "Epoch [70/100], Loss: 0.7338\n",
      "Epoch [80/100], Loss: 0.7303\n",
      "Epoch [90/100], Loss: 0.7282\n",
      "Epoch [100/100], Loss: 0.7269\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.6945\n",
      "Epoch [20/100], Loss: 0.6638\n",
      "Epoch [30/100], Loss: 0.6432\n",
      "Epoch [40/100], Loss: 0.6286\n",
      "Epoch [50/100], Loss: 0.6179\n",
      "Epoch [60/100], Loss: 0.6099\n",
      "Epoch [70/100], Loss: 0.6041\n",
      "Epoch [80/100], Loss: 0.5999\n",
      "Epoch [90/100], Loss: 0.5969\n",
      "Epoch [100/100], Loss: 0.5948\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 0.9468\n",
      "Epoch [20/100], Loss: 0.8322\n",
      "Epoch [30/100], Loss: 0.7745\n",
      "Epoch [40/100], Loss: 0.7477\n",
      "Epoch [50/100], Loss: 0.7337\n",
      "Epoch [60/100], Loss: 0.7280\n",
      "Epoch [70/100], Loss: 0.7261\n",
      "Epoch [80/100], Loss: 0.7254\n",
      "Epoch [90/100], Loss: 0.7252\n",
      "Epoch [100/100], Loss: 0.7250\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.6744\n",
      "Epoch [20/100], Loss: 0.6436\n",
      "Epoch [30/100], Loss: 0.6234\n",
      "Epoch [40/100], Loss: 0.6105\n",
      "Epoch [50/100], Loss: 0.6022\n",
      "Epoch [60/100], Loss: 0.5972\n",
      "Epoch [70/100], Loss: 0.5942\n",
      "Epoch [80/100], Loss: 0.5924\n",
      "Epoch [90/100], Loss: 0.5914\n",
      "Epoch [100/100], Loss: 0.5908\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 1.5245\n",
      "Epoch [20/100], Loss: 1.2712\n",
      "Epoch [30/100], Loss: 1.0768\n",
      "Epoch [40/100], Loss: 0.9343\n",
      "Epoch [50/100], Loss: 0.8457\n",
      "Epoch [60/100], Loss: 0.7928\n",
      "Epoch [70/100], Loss: 0.7644\n",
      "Epoch [80/100], Loss: 0.7503\n",
      "Epoch [90/100], Loss: 0.7417\n",
      "Epoch [100/100], Loss: 0.7359\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.7768\n",
      "Epoch [20/100], Loss: 0.7364\n",
      "Epoch [30/100], Loss: 0.7059\n",
      "Epoch [40/100], Loss: 0.6827\n",
      "Epoch [50/100], Loss: 0.6643\n",
      "Epoch [60/100], Loss: 0.6496\n",
      "Epoch [70/100], Loss: 0.6377\n",
      "Epoch [80/100], Loss: 0.6279\n",
      "Epoch [90/100], Loss: 0.6200\n",
      "Epoch [100/100], Loss: 0.6137\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 1.4108\n",
      "Epoch [20/100], Loss: 1.2370\n",
      "Epoch [30/100], Loss: 1.0885\n",
      "Epoch [40/100], Loss: 0.9685\n",
      "Epoch [50/100], Loss: 0.8820\n",
      "Epoch [60/100], Loss: 0.8318\n",
      "Epoch [70/100], Loss: 0.7936\n",
      "Epoch [80/100], Loss: 0.7661\n",
      "Epoch [90/100], Loss: 0.7482\n",
      "Epoch [100/100], Loss: 0.7379\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.7567\n",
      "Epoch [20/100], Loss: 0.7065\n",
      "Epoch [30/100], Loss: 0.6684\n",
      "Epoch [40/100], Loss: 0.6412\n",
      "Epoch [50/100], Loss: 0.6226\n",
      "Epoch [60/100], Loss: 0.6103\n",
      "Epoch [70/100], Loss: 0.6025\n",
      "Epoch [80/100], Loss: 0.5975\n",
      "Epoch [90/100], Loss: 0.5943\n",
      "Epoch [100/100], Loss: 0.5923\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 1.2235\n",
      "Epoch [20/100], Loss: 1.0272\n",
      "Epoch [30/100], Loss: 0.8954\n",
      "Epoch [40/100], Loss: 0.8084\n",
      "Epoch [50/100], Loss: 0.7677\n",
      "Epoch [60/100], Loss: 0.7484\n",
      "Epoch [70/100], Loss: 0.7374\n",
      "Epoch [80/100], Loss: 0.7316\n",
      "Epoch [90/100], Loss: 0.7283\n",
      "Epoch [100/100], Loss: 0.7262\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.8479\n",
      "Epoch [20/100], Loss: 0.7818\n",
      "Epoch [30/100], Loss: 0.7296\n",
      "Epoch [40/100], Loss: 0.6898\n",
      "Epoch [50/100], Loss: 0.6600\n",
      "Epoch [60/100], Loss: 0.6382\n",
      "Epoch [70/100], Loss: 0.6229\n",
      "Epoch [80/100], Loss: 0.6124\n",
      "Epoch [90/100], Loss: 0.6053\n",
      "Epoch [100/100], Loss: 0.6006\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 0.9237\n",
      "Epoch [20/100], Loss: 0.8228\n",
      "Epoch [30/100], Loss: 0.7650\n",
      "Epoch [40/100], Loss: 0.7405\n",
      "Epoch [50/100], Loss: 0.7314\n",
      "Epoch [60/100], Loss: 0.7280\n",
      "Epoch [70/100], Loss: 0.7267\n",
      "Epoch [80/100], Loss: 0.7262\n",
      "Epoch [90/100], Loss: 0.7261\n",
      "Epoch [100/100], Loss: 0.7261\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.9243\n",
      "Epoch [20/100], Loss: 0.8557\n",
      "Epoch [30/100], Loss: 0.7986\n",
      "Epoch [40/100], Loss: 0.7531\n",
      "Epoch [50/100], Loss: 0.7179\n",
      "Epoch [60/100], Loss: 0.6909\n",
      "Epoch [70/100], Loss: 0.6702\n",
      "Epoch [80/100], Loss: 0.6543\n",
      "Epoch [90/100], Loss: 0.6419\n",
      "Epoch [100/100], Loss: 0.6321\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 1.3165\n",
      "Epoch [20/100], Loss: 1.1719\n",
      "Epoch [30/100], Loss: 1.0486\n",
      "Epoch [40/100], Loss: 0.9477\n",
      "Epoch [50/100], Loss: 0.8714\n",
      "Epoch [60/100], Loss: 0.8231\n",
      "Epoch [70/100], Loss: 0.7851\n",
      "Epoch [80/100], Loss: 0.7582\n",
      "Epoch [90/100], Loss: 0.7440\n",
      "Epoch [100/100], Loss: 0.7363\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.7804\n",
      "Epoch [20/100], Loss: 0.7391\n",
      "Epoch [30/100], Loss: 0.7051\n",
      "Epoch [40/100], Loss: 0.6788\n",
      "Epoch [50/100], Loss: 0.6591\n",
      "Epoch [60/100], Loss: 0.6444\n",
      "Epoch [70/100], Loss: 0.6330\n",
      "Epoch [80/100], Loss: 0.6238\n",
      "Epoch [90/100], Loss: 0.6163\n",
      "Epoch [100/100], Loss: 0.6101\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 0.9462\n",
      "Epoch [20/100], Loss: 0.8641\n",
      "Epoch [30/100], Loss: 0.8066\n",
      "Epoch [40/100], Loss: 0.7714\n",
      "Epoch [50/100], Loss: 0.7499\n",
      "Epoch [60/100], Loss: 0.7372\n",
      "Epoch [70/100], Loss: 0.7298\n",
      "Epoch [80/100], Loss: 0.7255\n",
      "Epoch [90/100], Loss: 0.7231\n",
      "Epoch [100/100], Loss: 0.7216\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.9929\n",
      "Epoch [20/100], Loss: 0.9088\n",
      "Epoch [30/100], Loss: 0.8369\n",
      "Epoch [40/100], Loss: 0.7785\n",
      "Epoch [50/100], Loss: 0.7335\n",
      "Epoch [60/100], Loss: 0.7001\n",
      "Epoch [70/100], Loss: 0.6758\n",
      "Epoch [80/100], Loss: 0.6581\n",
      "Epoch [90/100], Loss: 0.6448\n",
      "Epoch [100/100], Loss: 0.6348\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 0.9970\n",
      "Epoch [20/100], Loss: 0.8847\n",
      "Epoch [30/100], Loss: 0.8074\n",
      "Epoch [40/100], Loss: 0.7726\n",
      "Epoch [50/100], Loss: 0.7536\n",
      "Epoch [60/100], Loss: 0.7431\n",
      "Epoch [70/100], Loss: 0.7377\n",
      "Epoch [80/100], Loss: 0.7350\n",
      "Epoch [90/100], Loss: 0.7335\n",
      "Epoch [100/100], Loss: 0.7327\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.8025\n",
      "Epoch [20/100], Loss: 0.7531\n",
      "Epoch [30/100], Loss: 0.7139\n",
      "Epoch [40/100], Loss: 0.6842\n",
      "Epoch [50/100], Loss: 0.6618\n",
      "Epoch [60/100], Loss: 0.6446\n",
      "Epoch [70/100], Loss: 0.6311\n",
      "Epoch [80/100], Loss: 0.6207\n",
      "Epoch [90/100], Loss: 0.6128\n",
      "Epoch [100/100], Loss: 0.6068\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 0.9124\n",
      "Epoch [20/100], Loss: 0.8348\n",
      "Epoch [30/100], Loss: 0.7821\n",
      "Epoch [40/100], Loss: 0.7540\n",
      "Epoch [50/100], Loss: 0.7407\n",
      "Epoch [60/100], Loss: 0.7329\n",
      "Epoch [70/100], Loss: 0.7291\n",
      "Epoch [80/100], Loss: 0.7272\n",
      "Epoch [90/100], Loss: 0.7263\n",
      "Epoch [100/100], Loss: 0.7259\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.7696\n",
      "Epoch [20/100], Loss: 0.7194\n",
      "Epoch [30/100], Loss: 0.6821\n",
      "Epoch [40/100], Loss: 0.6552\n",
      "Epoch [50/100], Loss: 0.6360\n",
      "Epoch [60/100], Loss: 0.6223\n",
      "Epoch [70/100], Loss: 0.6126\n",
      "Epoch [80/100], Loss: 0.6058\n",
      "Epoch [90/100], Loss: 0.6013\n",
      "Epoch [100/100], Loss: 0.5982\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 1.1636\n",
      "Epoch [20/100], Loss: 1.0111\n",
      "Epoch [30/100], Loss: 0.8929\n",
      "Epoch [40/100], Loss: 0.8192\n",
      "Epoch [50/100], Loss: 0.7735\n",
      "Epoch [60/100], Loss: 0.7512\n",
      "Epoch [70/100], Loss: 0.7405\n",
      "Epoch [80/100], Loss: 0.7349\n",
      "Epoch [90/100], Loss: 0.7323\n",
      "Epoch [100/100], Loss: 0.7311\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.8407\n",
      "Epoch [20/100], Loss: 0.7894\n",
      "Epoch [30/100], Loss: 0.7468\n",
      "Epoch [40/100], Loss: 0.7113\n",
      "Epoch [50/100], Loss: 0.6828\n",
      "Epoch [60/100], Loss: 0.6605\n",
      "Epoch [70/100], Loss: 0.6432\n",
      "Epoch [80/100], Loss: 0.6299\n",
      "Epoch [90/100], Loss: 0.6197\n",
      "Epoch [100/100], Loss: 0.6121\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 1.2777\n",
      "Epoch [20/100], Loss: 1.0750\n",
      "Epoch [30/100], Loss: 0.9339\n",
      "Epoch [40/100], Loss: 0.8476\n",
      "Epoch [50/100], Loss: 0.7939\n",
      "Epoch [60/100], Loss: 0.7638\n",
      "Epoch [70/100], Loss: 0.7479\n",
      "Epoch [80/100], Loss: 0.7392\n",
      "Epoch [90/100], Loss: 0.7341\n",
      "Epoch [100/100], Loss: 0.7312\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.8581\n",
      "Epoch [20/100], Loss: 0.7927\n",
      "Epoch [30/100], Loss: 0.7378\n",
      "Epoch [40/100], Loss: 0.6942\n",
      "Epoch [50/100], Loss: 0.6611\n",
      "Epoch [60/100], Loss: 0.6373\n",
      "Epoch [70/100], Loss: 0.6209\n",
      "Epoch [80/100], Loss: 0.6098\n",
      "Epoch [90/100], Loss: 0.6025\n",
      "Epoch [100/100], Loss: 0.5976\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 1.2030\n",
      "Epoch [20/100], Loss: 0.9705\n",
      "Epoch [30/100], Loss: 0.8230\n",
      "Epoch [40/100], Loss: 0.7516\n",
      "Epoch [50/100], Loss: 0.7288\n",
      "Epoch [60/100], Loss: 0.7247\n",
      "Epoch [70/100], Loss: 0.7243\n",
      "Epoch [80/100], Loss: 0.7242\n",
      "Epoch [90/100], Loss: 0.7240\n",
      "Epoch [100/100], Loss: 0.7239\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.9208\n",
      "Epoch [20/100], Loss: 0.8514\n",
      "Epoch [30/100], Loss: 0.7913\n",
      "Epoch [40/100], Loss: 0.7417\n",
      "Epoch [50/100], Loss: 0.7025\n",
      "Epoch [60/100], Loss: 0.6730\n",
      "Epoch [70/100], Loss: 0.6513\n",
      "Epoch [80/100], Loss: 0.6356\n",
      "Epoch [90/100], Loss: 0.6243\n",
      "Epoch [100/100], Loss: 0.6160\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 0.9082\n",
      "Epoch [20/100], Loss: 0.7878\n",
      "Epoch [30/100], Loss: 0.7389\n",
      "Epoch [40/100], Loss: 0.7298\n",
      "Epoch [50/100], Loss: 0.7291\n",
      "Epoch [60/100], Loss: 0.7286\n",
      "Epoch [70/100], Loss: 0.7282\n",
      "Epoch [80/100], Loss: 0.7280\n",
      "Epoch [90/100], Loss: 0.7280\n",
      "Epoch [100/100], Loss: 0.7279\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.8786\n",
      "Epoch [20/100], Loss: 0.8259\n",
      "Epoch [30/100], Loss: 0.7792\n",
      "Epoch [40/100], Loss: 0.7387\n",
      "Epoch [50/100], Loss: 0.7049\n",
      "Epoch [60/100], Loss: 0.6777\n",
      "Epoch [70/100], Loss: 0.6567\n",
      "Epoch [80/100], Loss: 0.6408\n",
      "Epoch [90/100], Loss: 0.6291\n",
      "Epoch [100/100], Loss: 0.6203\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 0.9343\n",
      "Epoch [20/100], Loss: 0.8089\n",
      "Epoch [30/100], Loss: 0.7451\n",
      "Epoch [40/100], Loss: 0.7280\n",
      "Epoch [50/100], Loss: 0.7268\n",
      "Epoch [60/100], Loss: 0.7269\n",
      "Epoch [70/100], Loss: 0.7265\n",
      "Epoch [80/100], Loss: 0.7262\n",
      "Epoch [90/100], Loss: 0.7261\n",
      "Epoch [100/100], Loss: 0.7261\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.6827\n",
      "Epoch [20/100], Loss: 0.6524\n",
      "Epoch [30/100], Loss: 0.6295\n",
      "Epoch [40/100], Loss: 0.6131\n",
      "Epoch [50/100], Loss: 0.6026\n",
      "Epoch [60/100], Loss: 0.5963\n",
      "Epoch [70/100], Loss: 0.5929\n",
      "Epoch [80/100], Loss: 0.5910\n",
      "Epoch [90/100], Loss: 0.5900\n",
      "Epoch [100/100], Loss: 0.5895\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 0.9703\n",
      "Epoch [20/100], Loss: 0.8738\n",
      "Epoch [30/100], Loss: 0.8122\n",
      "Epoch [40/100], Loss: 0.7762\n",
      "Epoch [50/100], Loss: 0.7552\n",
      "Epoch [60/100], Loss: 0.7423\n",
      "Epoch [70/100], Loss: 0.7345\n",
      "Epoch [80/100], Loss: 0.7299\n",
      "Epoch [90/100], Loss: 0.7273\n",
      "Epoch [100/100], Loss: 0.7258\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.7427\n",
      "Epoch [20/100], Loss: 0.6998\n",
      "Epoch [30/100], Loss: 0.6665\n",
      "Epoch [40/100], Loss: 0.6423\n",
      "Epoch [50/100], Loss: 0.6256\n",
      "Epoch [60/100], Loss: 0.6144\n",
      "Epoch [70/100], Loss: 0.6069\n",
      "Epoch [80/100], Loss: 0.6018\n",
      "Epoch [90/100], Loss: 0.5982\n",
      "Epoch [100/100], Loss: 0.5956\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 0.8963\n",
      "Epoch [20/100], Loss: 0.8034\n",
      "Epoch [30/100], Loss: 0.7535\n",
      "Epoch [40/100], Loss: 0.7365\n",
      "Epoch [50/100], Loss: 0.7303\n",
      "Epoch [60/100], Loss: 0.7268\n",
      "Epoch [70/100], Loss: 0.7252\n",
      "Epoch [80/100], Loss: 0.7245\n",
      "Epoch [90/100], Loss: 0.7242\n",
      "Epoch [100/100], Loss: 0.7241\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.9493\n",
      "Epoch [20/100], Loss: 0.8959\n",
      "Epoch [30/100], Loss: 0.8473\n",
      "Epoch [40/100], Loss: 0.8044\n",
      "Epoch [50/100], Loss: 0.7673\n",
      "Epoch [60/100], Loss: 0.7356\n",
      "Epoch [70/100], Loss: 0.7089\n",
      "Epoch [80/100], Loss: 0.6868\n",
      "Epoch [90/100], Loss: 0.6686\n",
      "Epoch [100/100], Loss: 0.6537\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 0.9474\n",
      "Epoch [20/100], Loss: 0.8281\n",
      "Epoch [30/100], Loss: 0.7606\n",
      "Epoch [40/100], Loss: 0.7384\n",
      "Epoch [50/100], Loss: 0.7324\n",
      "Epoch [60/100], Loss: 0.7305\n",
      "Epoch [70/100], Loss: 0.7297\n",
      "Epoch [80/100], Loss: 0.7293\n",
      "Epoch [90/100], Loss: 0.7292\n",
      "Epoch [100/100], Loss: 0.7292\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.9280\n",
      "Epoch [20/100], Loss: 0.8475\n",
      "Epoch [30/100], Loss: 0.7790\n",
      "Epoch [40/100], Loss: 0.7239\n",
      "Epoch [50/100], Loss: 0.6823\n",
      "Epoch [60/100], Loss: 0.6527\n",
      "Epoch [70/100], Loss: 0.6326\n",
      "Epoch [80/100], Loss: 0.6191\n",
      "Epoch [90/100], Loss: 0.6100\n",
      "Epoch [100/100], Loss: 0.6037\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 1.3398\n",
      "Epoch [20/100], Loss: 1.1508\n",
      "Epoch [30/100], Loss: 1.0047\n",
      "Epoch [40/100], Loss: 0.8996\n",
      "Epoch [50/100], Loss: 0.8224\n",
      "Epoch [60/100], Loss: 0.7741\n",
      "Epoch [70/100], Loss: 0.7483\n",
      "Epoch [80/100], Loss: 0.7352\n",
      "Epoch [90/100], Loss: 0.7276\n",
      "Epoch [100/100], Loss: 0.7236\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.7378\n",
      "Epoch [20/100], Loss: 0.6971\n",
      "Epoch [30/100], Loss: 0.6654\n",
      "Epoch [40/100], Loss: 0.6411\n",
      "Epoch [50/100], Loss: 0.6229\n",
      "Epoch [60/100], Loss: 0.6099\n",
      "Epoch [70/100], Loss: 0.6009\n",
      "Epoch [80/100], Loss: 0.5949\n",
      "Epoch [90/100], Loss: 0.5911\n",
      "Epoch [100/100], Loss: 0.5887\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 0.8523\n",
      "Epoch [20/100], Loss: 0.7676\n",
      "Epoch [30/100], Loss: 0.7326\n",
      "Epoch [40/100], Loss: 0.7217\n",
      "Epoch [50/100], Loss: 0.7183\n",
      "Epoch [60/100], Loss: 0.7175\n",
      "Epoch [70/100], Loss: 0.7174\n",
      "Epoch [80/100], Loss: 0.7173\n",
      "Epoch [90/100], Loss: 0.7173\n",
      "Epoch [100/100], Loss: 0.7173\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.7333\n",
      "Epoch [20/100], Loss: 0.6912\n",
      "Epoch [30/100], Loss: 0.6584\n",
      "Epoch [40/100], Loss: 0.6345\n",
      "Epoch [50/100], Loss: 0.6183\n",
      "Epoch [60/100], Loss: 0.6082\n",
      "Epoch [70/100], Loss: 0.6022\n",
      "Epoch [80/100], Loss: 0.5988\n",
      "Epoch [90/100], Loss: 0.5967\n",
      "Epoch [100/100], Loss: 0.5954\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 1.1167\n",
      "Epoch [20/100], Loss: 0.9203\n",
      "Epoch [30/100], Loss: 0.7916\n",
      "Epoch [40/100], Loss: 0.7470\n",
      "Epoch [50/100], Loss: 0.7384\n",
      "Epoch [60/100], Loss: 0.7339\n",
      "Epoch [70/100], Loss: 0.7313\n",
      "Epoch [80/100], Loss: 0.7301\n",
      "Epoch [90/100], Loss: 0.7296\n",
      "Epoch [100/100], Loss: 0.7294\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.8767\n",
      "Epoch [20/100], Loss: 0.8104\n",
      "Epoch [30/100], Loss: 0.7553\n",
      "Epoch [40/100], Loss: 0.7120\n",
      "Epoch [50/100], Loss: 0.6799\n",
      "Epoch [60/100], Loss: 0.6568\n",
      "Epoch [70/100], Loss: 0.6402\n",
      "Epoch [80/100], Loss: 0.6279\n",
      "Epoch [90/100], Loss: 0.6187\n",
      "Epoch [100/100], Loss: 0.6117\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 1.1410\n",
      "Epoch [20/100], Loss: 1.0015\n",
      "Epoch [30/100], Loss: 0.9056\n",
      "Epoch [40/100], Loss: 0.8435\n",
      "Epoch [50/100], Loss: 0.8027\n",
      "Epoch [60/100], Loss: 0.7757\n",
      "Epoch [70/100], Loss: 0.7581\n",
      "Epoch [80/100], Loss: 0.7469\n",
      "Epoch [90/100], Loss: 0.7392\n",
      "Epoch [100/100], Loss: 0.7341\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.7751\n",
      "Epoch [20/100], Loss: 0.7310\n",
      "Epoch [30/100], Loss: 0.6974\n",
      "Epoch [40/100], Loss: 0.6708\n",
      "Epoch [50/100], Loss: 0.6496\n",
      "Epoch [60/100], Loss: 0.6331\n",
      "Epoch [70/100], Loss: 0.6206\n",
      "Epoch [80/100], Loss: 0.6114\n",
      "Epoch [90/100], Loss: 0.6048\n",
      "Epoch [100/100], Loss: 0.6003\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 1.0897\n",
      "Epoch [20/100], Loss: 0.9597\n",
      "Epoch [30/100], Loss: 0.8687\n",
      "Epoch [40/100], Loss: 0.8070\n",
      "Epoch [50/100], Loss: 0.7700\n",
      "Epoch [60/100], Loss: 0.7501\n",
      "Epoch [70/100], Loss: 0.7391\n",
      "Epoch [80/100], Loss: 0.7332\n",
      "Epoch [90/100], Loss: 0.7301\n",
      "Epoch [100/100], Loss: 0.7283\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.8214\n",
      "Epoch [20/100], Loss: 0.7644\n",
      "Epoch [30/100], Loss: 0.7182\n",
      "Epoch [40/100], Loss: 0.6816\n",
      "Epoch [50/100], Loss: 0.6537\n",
      "Epoch [60/100], Loss: 0.6335\n",
      "Epoch [70/100], Loss: 0.6195\n",
      "Epoch [80/100], Loss: 0.6100\n",
      "Epoch [90/100], Loss: 0.6037\n",
      "Epoch [100/100], Loss: 0.5996\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 0.8951\n",
      "Epoch [20/100], Loss: 0.7958\n",
      "Epoch [30/100], Loss: 0.7540\n",
      "Epoch [40/100], Loss: 0.7362\n",
      "Epoch [50/100], Loss: 0.7296\n",
      "Epoch [60/100], Loss: 0.7282\n",
      "Epoch [70/100], Loss: 0.7279\n",
      "Epoch [80/100], Loss: 0.7278\n",
      "Epoch [90/100], Loss: 0.7278\n",
      "Epoch [100/100], Loss: 0.7278\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.6996\n",
      "Epoch [20/100], Loss: 0.6684\n",
      "Epoch [30/100], Loss: 0.6443\n",
      "Epoch [40/100], Loss: 0.6259\n",
      "Epoch [50/100], Loss: 0.6124\n",
      "Epoch [60/100], Loss: 0.6031\n",
      "Epoch [70/100], Loss: 0.5971\n",
      "Epoch [80/100], Loss: 0.5934\n",
      "Epoch [90/100], Loss: 0.5911\n",
      "Epoch [100/100], Loss: 0.5897\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 1.2330\n",
      "Epoch [20/100], Loss: 1.0532\n",
      "Epoch [30/100], Loss: 0.9240\n",
      "Epoch [40/100], Loss: 0.8361\n",
      "Epoch [50/100], Loss: 0.7844\n",
      "Epoch [60/100], Loss: 0.7564\n",
      "Epoch [70/100], Loss: 0.7420\n",
      "Epoch [80/100], Loss: 0.7338\n",
      "Epoch [90/100], Loss: 0.7288\n",
      "Epoch [100/100], Loss: 0.7256\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.8268\n",
      "Epoch [20/100], Loss: 0.7788\n",
      "Epoch [30/100], Loss: 0.7405\n",
      "Epoch [40/100], Loss: 0.7098\n",
      "Epoch [50/100], Loss: 0.6850\n",
      "Epoch [60/100], Loss: 0.6652\n",
      "Epoch [70/100], Loss: 0.6495\n",
      "Epoch [80/100], Loss: 0.6371\n",
      "Epoch [90/100], Loss: 0.6271\n",
      "Epoch [100/100], Loss: 0.6193\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 1.0388\n",
      "Epoch [20/100], Loss: 0.8855\n",
      "Epoch [30/100], Loss: 0.7944\n",
      "Epoch [40/100], Loss: 0.7588\n",
      "Epoch [50/100], Loss: 0.7443\n",
      "Epoch [60/100], Loss: 0.7356\n",
      "Epoch [70/100], Loss: 0.7308\n",
      "Epoch [80/100], Loss: 0.7286\n",
      "Epoch [90/100], Loss: 0.7275\n",
      "Epoch [100/100], Loss: 0.7269\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.6831\n",
      "Epoch [20/100], Loss: 0.6445\n",
      "Epoch [30/100], Loss: 0.6189\n",
      "Epoch [40/100], Loss: 0.6037\n",
      "Epoch [50/100], Loss: 0.5953\n",
      "Epoch [60/100], Loss: 0.5909\n",
      "Epoch [70/100], Loss: 0.5886\n",
      "Epoch [80/100], Loss: 0.5874\n",
      "Epoch [90/100], Loss: 0.5868\n",
      "Epoch [100/100], Loss: 0.5865\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 1.2260\n",
      "Epoch [20/100], Loss: 1.0654\n",
      "Epoch [30/100], Loss: 0.9549\n",
      "Epoch [40/100], Loss: 0.8756\n",
      "Epoch [50/100], Loss: 0.8198\n",
      "Epoch [60/100], Loss: 0.7834\n",
      "Epoch [70/100], Loss: 0.7608\n",
      "Epoch [80/100], Loss: 0.7463\n",
      "Epoch [90/100], Loss: 0.7370\n",
      "Epoch [100/100], Loss: 0.7310\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.9293\n",
      "Epoch [20/100], Loss: 0.8747\n",
      "Epoch [30/100], Loss: 0.8276\n",
      "Epoch [40/100], Loss: 0.7862\n",
      "Epoch [50/100], Loss: 0.7499\n",
      "Epoch [60/100], Loss: 0.7191\n",
      "Epoch [70/100], Loss: 0.6937\n",
      "Epoch [80/100], Loss: 0.6729\n",
      "Epoch [90/100], Loss: 0.6561\n",
      "Epoch [100/100], Loss: 0.6426\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 1.0609\n",
      "Epoch [20/100], Loss: 0.9076\n",
      "Epoch [30/100], Loss: 0.8074\n",
      "Epoch [40/100], Loss: 0.7588\n",
      "Epoch [50/100], Loss: 0.7372\n",
      "Epoch [60/100], Loss: 0.7292\n",
      "Epoch [70/100], Loss: 0.7273\n",
      "Epoch [80/100], Loss: 0.7270\n",
      "Epoch [90/100], Loss: 0.7269\n",
      "Epoch [100/100], Loss: 0.7269\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.6622\n",
      "Epoch [20/100], Loss: 0.6361\n",
      "Epoch [30/100], Loss: 0.6183\n",
      "Epoch [40/100], Loss: 0.6070\n",
      "Epoch [50/100], Loss: 0.6006\n",
      "Epoch [60/100], Loss: 0.5972\n",
      "Epoch [70/100], Loss: 0.5954\n",
      "Epoch [80/100], Loss: 0.5946\n",
      "Epoch [90/100], Loss: 0.5942\n",
      "Epoch [100/100], Loss: 0.5941\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 0.9640\n",
      "Epoch [20/100], Loss: 0.8528\n",
      "Epoch [30/100], Loss: 0.7961\n",
      "Epoch [40/100], Loss: 0.7664\n",
      "Epoch [50/100], Loss: 0.7495\n",
      "Epoch [60/100], Loss: 0.7404\n",
      "Epoch [70/100], Loss: 0.7359\n",
      "Epoch [80/100], Loss: 0.7338\n",
      "Epoch [90/100], Loss: 0.7328\n",
      "Epoch [100/100], Loss: 0.7323\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.8598\n",
      "Epoch [20/100], Loss: 0.8125\n",
      "Epoch [30/100], Loss: 0.7730\n",
      "Epoch [40/100], Loss: 0.7404\n",
      "Epoch [50/100], Loss: 0.7137\n",
      "Epoch [60/100], Loss: 0.6922\n",
      "Epoch [70/100], Loss: 0.6746\n",
      "Epoch [80/100], Loss: 0.6601\n",
      "Epoch [90/100], Loss: 0.6478\n",
      "Epoch [100/100], Loss: 0.6375\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 1.2602\n",
      "Epoch [20/100], Loss: 1.0873\n",
      "Epoch [30/100], Loss: 0.9571\n",
      "Epoch [40/100], Loss: 0.8663\n",
      "Epoch [50/100], Loss: 0.8075\n",
      "Epoch [60/100], Loss: 0.7740\n",
      "Epoch [70/100], Loss: 0.7530\n",
      "Epoch [80/100], Loss: 0.7405\n",
      "Epoch [90/100], Loss: 0.7335\n",
      "Epoch [100/100], Loss: 0.7298\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.8541\n",
      "Epoch [20/100], Loss: 0.7975\n",
      "Epoch [30/100], Loss: 0.7483\n",
      "Epoch [40/100], Loss: 0.7076\n",
      "Epoch [50/100], Loss: 0.6753\n",
      "Epoch [60/100], Loss: 0.6507\n",
      "Epoch [70/100], Loss: 0.6323\n",
      "Epoch [80/100], Loss: 0.6191\n",
      "Epoch [90/100], Loss: 0.6097\n",
      "Epoch [100/100], Loss: 0.6033\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 1.0141\n",
      "Epoch [20/100], Loss: 0.9112\n",
      "Epoch [30/100], Loss: 0.8338\n",
      "Epoch [40/100], Loss: 0.7843\n",
      "Epoch [50/100], Loss: 0.7561\n",
      "Epoch [60/100], Loss: 0.7410\n",
      "Epoch [70/100], Loss: 0.7328\n",
      "Epoch [80/100], Loss: 0.7285\n",
      "Epoch [90/100], Loss: 0.7262\n",
      "Epoch [100/100], Loss: 0.7250\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.6685\n",
      "Epoch [20/100], Loss: 0.6383\n",
      "Epoch [30/100], Loss: 0.6180\n",
      "Epoch [40/100], Loss: 0.6055\n",
      "Epoch [50/100], Loss: 0.5979\n",
      "Epoch [60/100], Loss: 0.5933\n",
      "Epoch [70/100], Loss: 0.5906\n",
      "Epoch [80/100], Loss: 0.5893\n",
      "Epoch [90/100], Loss: 0.5887\n",
      "Epoch [100/100], Loss: 0.5884\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 1.2798\n",
      "Epoch [20/100], Loss: 1.1208\n",
      "Epoch [30/100], Loss: 0.9862\n",
      "Epoch [40/100], Loss: 0.8856\n",
      "Epoch [50/100], Loss: 0.8304\n",
      "Epoch [60/100], Loss: 0.7893\n",
      "Epoch [70/100], Loss: 0.7594\n",
      "Epoch [80/100], Loss: 0.7420\n",
      "Epoch [90/100], Loss: 0.7327\n",
      "Epoch [100/100], Loss: 0.7281\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.9826\n",
      "Epoch [20/100], Loss: 0.8978\n",
      "Epoch [30/100], Loss: 0.8252\n",
      "Epoch [40/100], Loss: 0.7663\n",
      "Epoch [50/100], Loss: 0.7207\n",
      "Epoch [60/100], Loss: 0.6864\n",
      "Epoch [70/100], Loss: 0.6610\n",
      "Epoch [80/100], Loss: 0.6422\n",
      "Epoch [90/100], Loss: 0.6284\n",
      "Epoch [100/100], Loss: 0.6182\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 0.9205\n",
      "Epoch [20/100], Loss: 0.7976\n",
      "Epoch [30/100], Loss: 0.7453\n",
      "Epoch [40/100], Loss: 0.7286\n",
      "Epoch [50/100], Loss: 0.7253\n",
      "Epoch [60/100], Loss: 0.7240\n",
      "Epoch [70/100], Loss: 0.7236\n",
      "Epoch [80/100], Loss: 0.7235\n",
      "Epoch [90/100], Loss: 0.7235\n",
      "Epoch [100/100], Loss: 0.7234\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.6952\n",
      "Epoch [20/100], Loss: 0.6628\n",
      "Epoch [30/100], Loss: 0.6402\n",
      "Epoch [40/100], Loss: 0.6244\n",
      "Epoch [50/100], Loss: 0.6136\n",
      "Epoch [60/100], Loss: 0.6062\n",
      "Epoch [70/100], Loss: 0.6011\n",
      "Epoch [80/100], Loss: 0.5976\n",
      "Epoch [90/100], Loss: 0.5950\n",
      "Epoch [100/100], Loss: 0.5932\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 1.0368\n",
      "Epoch [20/100], Loss: 0.8747\n",
      "Epoch [30/100], Loss: 0.7789\n",
      "Epoch [40/100], Loss: 0.7377\n",
      "Epoch [50/100], Loss: 0.7254\n",
      "Epoch [60/100], Loss: 0.7231\n",
      "Epoch [70/100], Loss: 0.7231\n",
      "Epoch [80/100], Loss: 0.7230\n",
      "Epoch [90/100], Loss: 0.7228\n",
      "Epoch [100/100], Loss: 0.7227\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.7023\n",
      "Epoch [20/100], Loss: 0.6731\n",
      "Epoch [30/100], Loss: 0.6506\n",
      "Epoch [40/100], Loss: 0.6331\n",
      "Epoch [50/100], Loss: 0.6195\n",
      "Epoch [60/100], Loss: 0.6095\n",
      "Epoch [70/100], Loss: 0.6024\n",
      "Epoch [80/100], Loss: 0.5977\n",
      "Epoch [90/100], Loss: 0.5945\n",
      "Epoch [100/100], Loss: 0.5925\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 0.8809\n",
      "Epoch [20/100], Loss: 0.7783\n",
      "Epoch [30/100], Loss: 0.7348\n",
      "Epoch [40/100], Loss: 0.7259\n",
      "Epoch [50/100], Loss: 0.7252\n",
      "Epoch [60/100], Loss: 0.7239\n",
      "Epoch [70/100], Loss: 0.7235\n",
      "Epoch [80/100], Loss: 0.7235\n",
      "Epoch [90/100], Loss: 0.7235\n",
      "Epoch [100/100], Loss: 0.7234\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.9805\n",
      "Epoch [20/100], Loss: 0.9196\n",
      "Epoch [30/100], Loss: 0.8645\n",
      "Epoch [40/100], Loss: 0.8157\n",
      "Epoch [50/100], Loss: 0.7734\n",
      "Epoch [60/100], Loss: 0.7375\n",
      "Epoch [70/100], Loss: 0.7077\n",
      "Epoch [80/100], Loss: 0.6833\n",
      "Epoch [90/100], Loss: 0.6638\n",
      "Epoch [100/100], Loss: 0.6483\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 0.8506\n",
      "Epoch [20/100], Loss: 0.7850\n",
      "Epoch [30/100], Loss: 0.7508\n",
      "Epoch [40/100], Loss: 0.7329\n",
      "Epoch [50/100], Loss: 0.7260\n",
      "Epoch [60/100], Loss: 0.7240\n",
      "Epoch [70/100], Loss: 0.7235\n",
      "Epoch [80/100], Loss: 0.7233\n",
      "Epoch [90/100], Loss: 0.7233\n",
      "Epoch [100/100], Loss: 0.7233\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.9252\n",
      "Epoch [20/100], Loss: 0.8515\n",
      "Epoch [30/100], Loss: 0.7882\n",
      "Epoch [40/100], Loss: 0.7370\n",
      "Epoch [50/100], Loss: 0.6976\n",
      "Epoch [60/100], Loss: 0.6686\n",
      "Epoch [70/100], Loss: 0.6477\n",
      "Epoch [80/100], Loss: 0.6326\n",
      "Epoch [90/100], Loss: 0.6215\n",
      "Epoch [100/100], Loss: 0.6132\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 1.0736\n",
      "Epoch [20/100], Loss: 0.9015\n",
      "Epoch [30/100], Loss: 0.7808\n",
      "Epoch [40/100], Loss: 0.7346\n",
      "Epoch [50/100], Loss: 0.7255\n",
      "Epoch [60/100], Loss: 0.7221\n",
      "Epoch [70/100], Loss: 0.7202\n",
      "Epoch [80/100], Loss: 0.7195\n",
      "Epoch [90/100], Loss: 0.7192\n",
      "Epoch [100/100], Loss: 0.7190\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.7486\n",
      "Epoch [20/100], Loss: 0.7109\n",
      "Epoch [30/100], Loss: 0.6810\n",
      "Epoch [40/100], Loss: 0.6579\n",
      "Epoch [50/100], Loss: 0.6402\n",
      "Epoch [60/100], Loss: 0.6268\n",
      "Epoch [70/100], Loss: 0.6167\n",
      "Epoch [80/100], Loss: 0.6094\n",
      "Epoch [90/100], Loss: 0.6042\n",
      "Epoch [100/100], Loss: 0.6006\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 0.9459\n",
      "Epoch [20/100], Loss: 0.8269\n",
      "Epoch [30/100], Loss: 0.7626\n",
      "Epoch [40/100], Loss: 0.7395\n",
      "Epoch [50/100], Loss: 0.7335\n",
      "Epoch [60/100], Loss: 0.7322\n",
      "Epoch [70/100], Loss: 0.7318\n",
      "Epoch [80/100], Loss: 0.7316\n",
      "Epoch [90/100], Loss: 0.7315\n",
      "Epoch [100/100], Loss: 0.7315\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.9460\n",
      "Epoch [20/100], Loss: 0.8765\n",
      "Epoch [30/100], Loss: 0.8174\n",
      "Epoch [40/100], Loss: 0.7686\n",
      "Epoch [50/100], Loss: 0.7289\n",
      "Epoch [60/100], Loss: 0.6970\n",
      "Epoch [70/100], Loss: 0.6717\n",
      "Epoch [80/100], Loss: 0.6522\n",
      "Epoch [90/100], Loss: 0.6373\n",
      "Epoch [100/100], Loss: 0.6262\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 1.2921\n",
      "Epoch [20/100], Loss: 1.0941\n",
      "Epoch [30/100], Loss: 0.9414\n",
      "Epoch [40/100], Loss: 0.8525\n",
      "Epoch [50/100], Loss: 0.7878\n",
      "Epoch [60/100], Loss: 0.7545\n",
      "Epoch [70/100], Loss: 0.7385\n",
      "Epoch [80/100], Loss: 0.7297\n",
      "Epoch [90/100], Loss: 0.7256\n",
      "Epoch [100/100], Loss: 0.7235\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.8496\n",
      "Epoch [20/100], Loss: 0.7886\n",
      "Epoch [30/100], Loss: 0.7393\n",
      "Epoch [40/100], Loss: 0.7014\n",
      "Epoch [50/100], Loss: 0.6733\n",
      "Epoch [60/100], Loss: 0.6524\n",
      "Epoch [70/100], Loss: 0.6370\n",
      "Epoch [80/100], Loss: 0.6256\n",
      "Epoch [90/100], Loss: 0.6172\n",
      "Epoch [100/100], Loss: 0.6110\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 1.0590\n",
      "Epoch [20/100], Loss: 0.9161\n",
      "Epoch [30/100], Loss: 0.8266\n",
      "Epoch [40/100], Loss: 0.7698\n",
      "Epoch [50/100], Loss: 0.7415\n",
      "Epoch [60/100], Loss: 0.7302\n",
      "Epoch [70/100], Loss: 0.7266\n",
      "Epoch [80/100], Loss: 0.7257\n",
      "Epoch [90/100], Loss: 0.7255\n",
      "Epoch [100/100], Loss: 0.7255\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.9559\n",
      "Epoch [20/100], Loss: 0.8945\n",
      "Epoch [30/100], Loss: 0.8426\n",
      "Epoch [40/100], Loss: 0.7990\n",
      "Epoch [50/100], Loss: 0.7622\n",
      "Epoch [60/100], Loss: 0.7310\n",
      "Epoch [70/100], Loss: 0.7047\n",
      "Epoch [80/100], Loss: 0.6827\n",
      "Epoch [90/100], Loss: 0.6645\n",
      "Epoch [100/100], Loss: 0.6496\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 1.0897\n",
      "Epoch [20/100], Loss: 0.9415\n",
      "Epoch [30/100], Loss: 0.8276\n",
      "Epoch [40/100], Loss: 0.7609\n",
      "Epoch [50/100], Loss: 0.7338\n",
      "Epoch [60/100], Loss: 0.7237\n",
      "Epoch [70/100], Loss: 0.7207\n",
      "Epoch [80/100], Loss: 0.7199\n",
      "Epoch [90/100], Loss: 0.7196\n",
      "Epoch [100/100], Loss: 0.7195\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.6728\n",
      "Epoch [20/100], Loss: 0.6397\n",
      "Epoch [30/100], Loss: 0.6181\n",
      "Epoch [40/100], Loss: 0.6050\n",
      "Epoch [50/100], Loss: 0.5972\n",
      "Epoch [60/100], Loss: 0.5929\n",
      "Epoch [70/100], Loss: 0.5907\n",
      "Epoch [80/100], Loss: 0.5896\n",
      "Epoch [90/100], Loss: 0.5891\n",
      "Epoch [100/100], Loss: 0.5889\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 1.2054\n",
      "Epoch [20/100], Loss: 1.0453\n",
      "Epoch [30/100], Loss: 0.9205\n",
      "Epoch [40/100], Loss: 0.8366\n",
      "Epoch [50/100], Loss: 0.7875\n",
      "Epoch [60/100], Loss: 0.7575\n",
      "Epoch [70/100], Loss: 0.7413\n",
      "Epoch [80/100], Loss: 0.7327\n",
      "Epoch [90/100], Loss: 0.7280\n",
      "Epoch [100/100], Loss: 0.7253\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.8155\n",
      "Epoch [20/100], Loss: 0.7530\n",
      "Epoch [30/100], Loss: 0.7051\n",
      "Epoch [40/100], Loss: 0.6708\n",
      "Epoch [50/100], Loss: 0.6471\n",
      "Epoch [60/100], Loss: 0.6307\n",
      "Epoch [70/100], Loss: 0.6193\n",
      "Epoch [80/100], Loss: 0.6113\n",
      "Epoch [90/100], Loss: 0.6054\n",
      "Epoch [100/100], Loss: 0.6012\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 1.0203\n",
      "Epoch [20/100], Loss: 0.8681\n",
      "Epoch [30/100], Loss: 0.7848\n",
      "Epoch [40/100], Loss: 0.7497\n",
      "Epoch [50/100], Loss: 0.7348\n",
      "Epoch [60/100], Loss: 0.7287\n",
      "Epoch [70/100], Loss: 0.7263\n",
      "Epoch [80/100], Loss: 0.7253\n",
      "Epoch [90/100], Loss: 0.7250\n",
      "Epoch [100/100], Loss: 0.7248\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.7268\n",
      "Epoch [20/100], Loss: 0.6876\n",
      "Epoch [30/100], Loss: 0.6590\n",
      "Epoch [40/100], Loss: 0.6386\n",
      "Epoch [50/100], Loss: 0.6239\n",
      "Epoch [60/100], Loss: 0.6132\n",
      "Epoch [70/100], Loss: 0.6053\n",
      "Epoch [80/100], Loss: 0.5999\n",
      "Epoch [90/100], Loss: 0.5961\n",
      "Epoch [100/100], Loss: 0.5937\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 1.0928\n",
      "Epoch [20/100], Loss: 0.9467\n",
      "Epoch [30/100], Loss: 0.8482\n",
      "Epoch [40/100], Loss: 0.7908\n",
      "Epoch [50/100], Loss: 0.7561\n",
      "Epoch [60/100], Loss: 0.7413\n",
      "Epoch [70/100], Loss: 0.7336\n",
      "Epoch [80/100], Loss: 0.7297\n",
      "Epoch [90/100], Loss: 0.7277\n",
      "Epoch [100/100], Loss: 0.7265\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.7778\n",
      "Epoch [20/100], Loss: 0.7215\n",
      "Epoch [30/100], Loss: 0.6781\n",
      "Epoch [40/100], Loss: 0.6469\n",
      "Epoch [50/100], Loss: 0.6254\n",
      "Epoch [60/100], Loss: 0.6111\n",
      "Epoch [70/100], Loss: 0.6018\n",
      "Epoch [80/100], Loss: 0.5960\n",
      "Epoch [90/100], Loss: 0.5923\n",
      "Epoch [100/100], Loss: 0.5901\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 1.0988\n",
      "Epoch [20/100], Loss: 0.9374\n",
      "Epoch [30/100], Loss: 0.8234\n",
      "Epoch [40/100], Loss: 0.7619\n",
      "Epoch [50/100], Loss: 0.7332\n",
      "Epoch [60/100], Loss: 0.7243\n",
      "Epoch [70/100], Loss: 0.7217\n",
      "Epoch [80/100], Loss: 0.7209\n",
      "Epoch [90/100], Loss: 0.7206\n",
      "Epoch [100/100], Loss: 0.7204\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.7277\n",
      "Epoch [20/100], Loss: 0.6877\n",
      "Epoch [30/100], Loss: 0.6562\n",
      "Epoch [40/100], Loss: 0.6332\n",
      "Epoch [50/100], Loss: 0.6177\n",
      "Epoch [60/100], Loss: 0.6078\n",
      "Epoch [70/100], Loss: 0.6017\n",
      "Epoch [80/100], Loss: 0.5978\n",
      "Epoch [90/100], Loss: 0.5953\n",
      "Epoch [100/100], Loss: 0.5937\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 0.8822\n",
      "Epoch [20/100], Loss: 0.7837\n",
      "Epoch [30/100], Loss: 0.7450\n",
      "Epoch [40/100], Loss: 0.7312\n",
      "Epoch [50/100], Loss: 0.7287\n",
      "Epoch [60/100], Loss: 0.7282\n",
      "Epoch [70/100], Loss: 0.7279\n",
      "Epoch [80/100], Loss: 0.7279\n",
      "Epoch [90/100], Loss: 0.7278\n",
      "Epoch [100/100], Loss: 0.7278\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.8828\n",
      "Epoch [20/100], Loss: 0.8130\n",
      "Epoch [30/100], Loss: 0.7542\n",
      "Epoch [40/100], Loss: 0.7060\n",
      "Epoch [50/100], Loss: 0.6689\n",
      "Epoch [60/100], Loss: 0.6422\n",
      "Epoch [70/100], Loss: 0.6241\n",
      "Epoch [80/100], Loss: 0.6121\n",
      "Epoch [90/100], Loss: 0.6044\n",
      "Epoch [100/100], Loss: 0.5994\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 0.9605\n",
      "Epoch [20/100], Loss: 0.8291\n",
      "Epoch [30/100], Loss: 0.7646\n",
      "Epoch [40/100], Loss: 0.7428\n",
      "Epoch [50/100], Loss: 0.7341\n",
      "Epoch [60/100], Loss: 0.7284\n",
      "Epoch [70/100], Loss: 0.7256\n",
      "Epoch [80/100], Loss: 0.7246\n",
      "Epoch [90/100], Loss: 0.7244\n",
      "Epoch [100/100], Loss: 0.7243\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.8140\n",
      "Epoch [20/100], Loss: 0.7603\n",
      "Epoch [30/100], Loss: 0.7189\n",
      "Epoch [40/100], Loss: 0.6880\n",
      "Epoch [50/100], Loss: 0.6650\n",
      "Epoch [60/100], Loss: 0.6477\n",
      "Epoch [70/100], Loss: 0.6344\n",
      "Epoch [80/100], Loss: 0.6241\n",
      "Epoch [90/100], Loss: 0.6161\n",
      "Epoch [100/100], Loss: 0.6101\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 0.9592\n",
      "Epoch [20/100], Loss: 0.8854\n",
      "Epoch [30/100], Loss: 0.8262\n",
      "Epoch [40/100], Loss: 0.7854\n",
      "Epoch [50/100], Loss: 0.7606\n",
      "Epoch [60/100], Loss: 0.7463\n",
      "Epoch [70/100], Loss: 0.7383\n",
      "Epoch [80/100], Loss: 0.7336\n",
      "Epoch [90/100], Loss: 0.7309\n",
      "Epoch [100/100], Loss: 0.7293\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.9374\n",
      "Epoch [20/100], Loss: 0.8721\n",
      "Epoch [30/100], Loss: 0.8157\n",
      "Epoch [40/100], Loss: 0.7685\n",
      "Epoch [50/100], Loss: 0.7297\n",
      "Epoch [60/100], Loss: 0.6982\n",
      "Epoch [70/100], Loss: 0.6731\n",
      "Epoch [80/100], Loss: 0.6536\n",
      "Epoch [90/100], Loss: 0.6386\n",
      "Epoch [100/100], Loss: 0.6273\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 1.2588\n",
      "Epoch [20/100], Loss: 1.0986\n",
      "Epoch [30/100], Loss: 0.9799\n",
      "Epoch [40/100], Loss: 0.8839\n",
      "Epoch [50/100], Loss: 0.8241\n",
      "Epoch [60/100], Loss: 0.7854\n",
      "Epoch [70/100], Loss: 0.7590\n",
      "Epoch [80/100], Loss: 0.7412\n",
      "Epoch [90/100], Loss: 0.7322\n",
      "Epoch [100/100], Loss: 0.7275\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.8222\n",
      "Epoch [20/100], Loss: 0.7650\n",
      "Epoch [30/100], Loss: 0.7179\n",
      "Epoch [40/100], Loss: 0.6803\n",
      "Epoch [50/100], Loss: 0.6521\n",
      "Epoch [60/100], Loss: 0.6319\n",
      "Epoch [70/100], Loss: 0.6180\n",
      "Epoch [80/100], Loss: 0.6084\n",
      "Epoch [90/100], Loss: 0.6018\n",
      "Epoch [100/100], Loss: 0.5972\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 1.2328\n",
      "Epoch [20/100], Loss: 1.0834\n",
      "Epoch [30/100], Loss: 0.9737\n",
      "Epoch [40/100], Loss: 0.8894\n",
      "Epoch [50/100], Loss: 0.8289\n",
      "Epoch [60/100], Loss: 0.7887\n",
      "Epoch [70/100], Loss: 0.7645\n",
      "Epoch [80/100], Loss: 0.7496\n",
      "Epoch [90/100], Loss: 0.7399\n",
      "Epoch [100/100], Loss: 0.7337\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 1.0070\n",
      "Epoch [20/100], Loss: 0.9141\n",
      "Epoch [30/100], Loss: 0.8351\n",
      "Epoch [40/100], Loss: 0.7715\n",
      "Epoch [50/100], Loss: 0.7233\n",
      "Epoch [60/100], Loss: 0.6883\n",
      "Epoch [70/100], Loss: 0.6635\n",
      "Epoch [80/100], Loss: 0.6459\n",
      "Epoch [90/100], Loss: 0.6332\n",
      "Epoch [100/100], Loss: 0.6238\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 0.9228\n",
      "Epoch [20/100], Loss: 0.8189\n",
      "Epoch [30/100], Loss: 0.7592\n",
      "Epoch [40/100], Loss: 0.7352\n",
      "Epoch [50/100], Loss: 0.7263\n",
      "Epoch [60/100], Loss: 0.7232\n",
      "Epoch [70/100], Loss: 0.7224\n",
      "Epoch [80/100], Loss: 0.7223\n",
      "Epoch [90/100], Loss: 0.7223\n",
      "Epoch [100/100], Loss: 0.7222\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.8580\n",
      "Epoch [20/100], Loss: 0.8078\n",
      "Epoch [30/100], Loss: 0.7659\n",
      "Epoch [40/100], Loss: 0.7316\n",
      "Epoch [50/100], Loss: 0.7037\n",
      "Epoch [60/100], Loss: 0.6815\n",
      "Epoch [70/100], Loss: 0.6641\n",
      "Epoch [80/100], Loss: 0.6504\n",
      "Epoch [90/100], Loss: 0.6394\n",
      "Epoch [100/100], Loss: 0.6304\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 0.8847\n",
      "Epoch [20/100], Loss: 0.7896\n",
      "Epoch [30/100], Loss: 0.7509\n",
      "Epoch [40/100], Loss: 0.7351\n",
      "Epoch [50/100], Loss: 0.7305\n",
      "Epoch [60/100], Loss: 0.7284\n",
      "Epoch [70/100], Loss: 0.7280\n",
      "Epoch [80/100], Loss: 0.7279\n",
      "Epoch [90/100], Loss: 0.7279\n",
      "Epoch [100/100], Loss: 0.7279\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.6506\n",
      "Epoch [20/100], Loss: 0.6286\n",
      "Epoch [30/100], Loss: 0.6134\n",
      "Epoch [40/100], Loss: 0.6035\n",
      "Epoch [50/100], Loss: 0.5975\n",
      "Epoch [60/100], Loss: 0.5939\n",
      "Epoch [70/100], Loss: 0.5917\n",
      "Epoch [80/100], Loss: 0.5905\n",
      "Epoch [90/100], Loss: 0.5898\n",
      "Epoch [100/100], Loss: 0.5895\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 1.0186\n",
      "Epoch [20/100], Loss: 0.8748\n",
      "Epoch [30/100], Loss: 0.8007\n",
      "Epoch [40/100], Loss: 0.7608\n",
      "Epoch [50/100], Loss: 0.7381\n",
      "Epoch [60/100], Loss: 0.7279\n",
      "Epoch [70/100], Loss: 0.7248\n",
      "Epoch [80/100], Loss: 0.7241\n",
      "Epoch [90/100], Loss: 0.7240\n",
      "Epoch [100/100], Loss: 0.7240\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.6694\n",
      "Epoch [20/100], Loss: 0.6397\n",
      "Epoch [30/100], Loss: 0.6194\n",
      "Epoch [40/100], Loss: 0.6066\n",
      "Epoch [50/100], Loss: 0.5992\n",
      "Epoch [60/100], Loss: 0.5950\n",
      "Epoch [70/100], Loss: 0.5928\n",
      "Epoch [80/100], Loss: 0.5916\n",
      "Epoch [90/100], Loss: 0.5910\n",
      "Epoch [100/100], Loss: 0.5906\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 1.1145\n",
      "Epoch [20/100], Loss: 0.9917\n",
      "Epoch [30/100], Loss: 0.9060\n",
      "Epoch [40/100], Loss: 0.8427\n",
      "Epoch [50/100], Loss: 0.8001\n",
      "Epoch [60/100], Loss: 0.7721\n",
      "Epoch [70/100], Loss: 0.7542\n",
      "Epoch [80/100], Loss: 0.7431\n",
      "Epoch [90/100], Loss: 0.7364\n",
      "Epoch [100/100], Loss: 0.7322\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.8693\n",
      "Epoch [20/100], Loss: 0.8100\n",
      "Epoch [30/100], Loss: 0.7594\n",
      "Epoch [40/100], Loss: 0.7181\n",
      "Epoch [50/100], Loss: 0.6855\n",
      "Epoch [60/100], Loss: 0.6604\n",
      "Epoch [70/100], Loss: 0.6417\n",
      "Epoch [80/100], Loss: 0.6280\n",
      "Epoch [90/100], Loss: 0.6180\n",
      "Epoch [100/100], Loss: 0.6107\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 0.8617\n",
      "Epoch [20/100], Loss: 0.7698\n",
      "Epoch [30/100], Loss: 0.7356\n",
      "Epoch [40/100], Loss: 0.7274\n",
      "Epoch [50/100], Loss: 0.7243\n",
      "Epoch [60/100], Loss: 0.7232\n",
      "Epoch [70/100], Loss: 0.7230\n",
      "Epoch [80/100], Loss: 0.7229\n",
      "Epoch [90/100], Loss: 0.7229\n",
      "Epoch [100/100], Loss: 0.7229\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.8772\n",
      "Epoch [20/100], Loss: 0.8115\n",
      "Epoch [30/100], Loss: 0.7612\n",
      "Epoch [40/100], Loss: 0.7248\n",
      "Epoch [50/100], Loss: 0.6983\n",
      "Epoch [60/100], Loss: 0.6783\n",
      "Epoch [70/100], Loss: 0.6624\n",
      "Epoch [80/100], Loss: 0.6494\n",
      "Epoch [90/100], Loss: 0.6386\n",
      "Epoch [100/100], Loss: 0.6296\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 0.9167\n",
      "Epoch [20/100], Loss: 0.8261\n",
      "Epoch [30/100], Loss: 0.7690\n",
      "Epoch [40/100], Loss: 0.7429\n",
      "Epoch [50/100], Loss: 0.7329\n",
      "Epoch [60/100], Loss: 0.7289\n",
      "Epoch [70/100], Loss: 0.7271\n",
      "Epoch [80/100], Loss: 0.7262\n",
      "Epoch [90/100], Loss: 0.7257\n",
      "Epoch [100/100], Loss: 0.7256\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.6745\n",
      "Epoch [20/100], Loss: 0.6434\n",
      "Epoch [30/100], Loss: 0.6241\n",
      "Epoch [40/100], Loss: 0.6119\n",
      "Epoch [50/100], Loss: 0.6036\n",
      "Epoch [60/100], Loss: 0.5979\n",
      "Epoch [70/100], Loss: 0.5943\n",
      "Epoch [80/100], Loss: 0.5920\n",
      "Epoch [90/100], Loss: 0.5907\n",
      "Epoch [100/100], Loss: 0.5899\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 0.8359\n",
      "Epoch [20/100], Loss: 0.7686\n",
      "Epoch [30/100], Loss: 0.7344\n",
      "Epoch [40/100], Loss: 0.7262\n",
      "Epoch [50/100], Loss: 0.7244\n",
      "Epoch [60/100], Loss: 0.7243\n",
      "Epoch [70/100], Loss: 0.7243\n",
      "Epoch [80/100], Loss: 0.7242\n",
      "Epoch [90/100], Loss: 0.7242\n",
      "Epoch [100/100], Loss: 0.7242\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.7601\n",
      "Epoch [20/100], Loss: 0.7086\n",
      "Epoch [30/100], Loss: 0.6687\n",
      "Epoch [40/100], Loss: 0.6404\n",
      "Epoch [50/100], Loss: 0.6216\n",
      "Epoch [60/100], Loss: 0.6098\n",
      "Epoch [70/100], Loss: 0.6026\n",
      "Epoch [80/100], Loss: 0.5983\n",
      "Epoch [90/100], Loss: 0.5957\n",
      "Epoch [100/100], Loss: 0.5941\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 1.2302\n",
      "Epoch [20/100], Loss: 1.0473\n",
      "Epoch [30/100], Loss: 0.9129\n",
      "Epoch [40/100], Loss: 0.8303\n",
      "Epoch [50/100], Loss: 0.7815\n",
      "Epoch [60/100], Loss: 0.7551\n",
      "Epoch [70/100], Loss: 0.7416\n",
      "Epoch [80/100], Loss: 0.7348\n",
      "Epoch [90/100], Loss: 0.7315\n",
      "Epoch [100/100], Loss: 0.7299\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.8574\n",
      "Epoch [20/100], Loss: 0.8064\n",
      "Epoch [30/100], Loss: 0.7632\n",
      "Epoch [40/100], Loss: 0.7276\n",
      "Epoch [50/100], Loss: 0.6984\n",
      "Epoch [60/100], Loss: 0.6748\n",
      "Epoch [70/100], Loss: 0.6562\n",
      "Epoch [80/100], Loss: 0.6415\n",
      "Epoch [90/100], Loss: 0.6301\n",
      "Epoch [100/100], Loss: 0.6212\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 0.9938\n",
      "Epoch [20/100], Loss: 0.8578\n",
      "Epoch [30/100], Loss: 0.7741\n",
      "Epoch [40/100], Loss: 0.7348\n",
      "Epoch [50/100], Loss: 0.7242\n",
      "Epoch [60/100], Loss: 0.7218\n",
      "Epoch [70/100], Loss: 0.7208\n",
      "Epoch [80/100], Loss: 0.7206\n",
      "Epoch [90/100], Loss: 0.7206\n",
      "Epoch [100/100], Loss: 0.7206\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.7423\n",
      "Epoch [20/100], Loss: 0.6970\n",
      "Epoch [30/100], Loss: 0.6622\n",
      "Epoch [40/100], Loss: 0.6371\n",
      "Epoch [50/100], Loss: 0.6203\n",
      "Epoch [60/100], Loss: 0.6095\n",
      "Epoch [70/100], Loss: 0.6025\n",
      "Epoch [80/100], Loss: 0.5979\n",
      "Epoch [90/100], Loss: 0.5949\n",
      "Epoch [100/100], Loss: 0.5928\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 1.1337\n",
      "Epoch [20/100], Loss: 0.9872\n",
      "Epoch [30/100], Loss: 0.8870\n",
      "Epoch [40/100], Loss: 0.8173\n",
      "Epoch [50/100], Loss: 0.7736\n",
      "Epoch [60/100], Loss: 0.7504\n",
      "Epoch [70/100], Loss: 0.7389\n",
      "Epoch [80/100], Loss: 0.7330\n",
      "Epoch [90/100], Loss: 0.7295\n",
      "Epoch [100/100], Loss: 0.7274\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.8387\n",
      "Epoch [20/100], Loss: 0.7905\n",
      "Epoch [30/100], Loss: 0.7503\n",
      "Epoch [40/100], Loss: 0.7174\n",
      "Epoch [50/100], Loss: 0.6906\n",
      "Epoch [60/100], Loss: 0.6690\n",
      "Epoch [70/100], Loss: 0.6517\n",
      "Epoch [80/100], Loss: 0.6382\n",
      "Epoch [90/100], Loss: 0.6276\n",
      "Epoch [100/100], Loss: 0.6194\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 1.0374\n",
      "Epoch [20/100], Loss: 0.9259\n",
      "Epoch [30/100], Loss: 0.8469\n",
      "Epoch [40/100], Loss: 0.7900\n",
      "Epoch [50/100], Loss: 0.7579\n",
      "Epoch [60/100], Loss: 0.7427\n",
      "Epoch [70/100], Loss: 0.7359\n",
      "Epoch [80/100], Loss: 0.7326\n",
      "Epoch [90/100], Loss: 0.7308\n",
      "Epoch [100/100], Loss: 0.7297\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.8643\n",
      "Epoch [20/100], Loss: 0.8133\n",
      "Epoch [30/100], Loss: 0.7685\n",
      "Epoch [40/100], Loss: 0.7305\n",
      "Epoch [50/100], Loss: 0.6997\n",
      "Epoch [60/100], Loss: 0.6754\n",
      "Epoch [70/100], Loss: 0.6566\n",
      "Epoch [80/100], Loss: 0.6421\n",
      "Epoch [90/100], Loss: 0.6307\n",
      "Epoch [100/100], Loss: 0.6218\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 1.0181\n",
      "Epoch [20/100], Loss: 0.8553\n",
      "Epoch [30/100], Loss: 0.7835\n",
      "Epoch [40/100], Loss: 0.7502\n",
      "Epoch [50/100], Loss: 0.7339\n",
      "Epoch [60/100], Loss: 0.7268\n",
      "Epoch [70/100], Loss: 0.7242\n",
      "Epoch [80/100], Loss: 0.7233\n",
      "Epoch [90/100], Loss: 0.7231\n",
      "Epoch [100/100], Loss: 0.7230\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.7408\n",
      "Epoch [20/100], Loss: 0.6982\n",
      "Epoch [30/100], Loss: 0.6664\n",
      "Epoch [40/100], Loss: 0.6431\n",
      "Epoch [50/100], Loss: 0.6264\n",
      "Epoch [60/100], Loss: 0.6147\n",
      "Epoch [70/100], Loss: 0.6066\n",
      "Epoch [80/100], Loss: 0.6012\n",
      "Epoch [90/100], Loss: 0.5975\n",
      "Epoch [100/100], Loss: 0.5951\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 1.0848\n",
      "Epoch [20/100], Loss: 0.9381\n",
      "Epoch [30/100], Loss: 0.8295\n",
      "Epoch [40/100], Loss: 0.7777\n",
      "Epoch [50/100], Loss: 0.7494\n",
      "Epoch [60/100], Loss: 0.7378\n",
      "Epoch [70/100], Loss: 0.7311\n",
      "Epoch [80/100], Loss: 0.7283\n",
      "Epoch [90/100], Loss: 0.7270\n",
      "Epoch [100/100], Loss: 0.7266\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.7515\n",
      "Epoch [20/100], Loss: 0.7054\n",
      "Epoch [30/100], Loss: 0.6685\n",
      "Epoch [40/100], Loss: 0.6416\n",
      "Epoch [50/100], Loss: 0.6231\n",
      "Epoch [60/100], Loss: 0.6110\n",
      "Epoch [70/100], Loss: 0.6032\n",
      "Epoch [80/100], Loss: 0.5982\n",
      "Epoch [90/100], Loss: 0.5948\n",
      "Epoch [100/100], Loss: 0.5926\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 0.9829\n",
      "Epoch [20/100], Loss: 0.8347\n",
      "Epoch [30/100], Loss: 0.7611\n",
      "Epoch [40/100], Loss: 0.7332\n",
      "Epoch [50/100], Loss: 0.7245\n",
      "Epoch [60/100], Loss: 0.7225\n",
      "Epoch [70/100], Loss: 0.7219\n",
      "Epoch [80/100], Loss: 0.7214\n",
      "Epoch [90/100], Loss: 0.7212\n",
      "Epoch [100/100], Loss: 0.7212\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.6839\n",
      "Epoch [20/100], Loss: 0.6517\n",
      "Epoch [30/100], Loss: 0.6293\n",
      "Epoch [40/100], Loss: 0.6149\n",
      "Epoch [50/100], Loss: 0.6060\n",
      "Epoch [60/100], Loss: 0.6007\n",
      "Epoch [70/100], Loss: 0.5974\n",
      "Epoch [80/100], Loss: 0.5953\n",
      "Epoch [90/100], Loss: 0.5939\n",
      "Epoch [100/100], Loss: 0.5929\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 0.8136\n",
      "Epoch [20/100], Loss: 0.7547\n",
      "Epoch [30/100], Loss: 0.7322\n",
      "Epoch [40/100], Loss: 0.7288\n",
      "Epoch [50/100], Loss: 0.7288\n",
      "Epoch [60/100], Loss: 0.7280\n",
      "Epoch [70/100], Loss: 0.7278\n",
      "Epoch [80/100], Loss: 0.7277\n",
      "Epoch [90/100], Loss: 0.7277\n",
      "Epoch [100/100], Loss: 0.7277\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.7453\n",
      "Epoch [20/100], Loss: 0.7064\n",
      "Epoch [30/100], Loss: 0.6776\n",
      "Epoch [40/100], Loss: 0.6557\n",
      "Epoch [50/100], Loss: 0.6389\n",
      "Epoch [60/100], Loss: 0.6261\n",
      "Epoch [70/100], Loss: 0.6167\n",
      "Epoch [80/100], Loss: 0.6098\n",
      "Epoch [90/100], Loss: 0.6047\n",
      "Epoch [100/100], Loss: 0.6011\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 1.4140\n",
      "Epoch [20/100], Loss: 1.1570\n",
      "Epoch [30/100], Loss: 0.9630\n",
      "Epoch [40/100], Loss: 0.8280\n",
      "Epoch [50/100], Loss: 0.7588\n",
      "Epoch [60/100], Loss: 0.7360\n",
      "Epoch [70/100], Loss: 0.7300\n",
      "Epoch [80/100], Loss: 0.7281\n",
      "Epoch [90/100], Loss: 0.7273\n",
      "Epoch [100/100], Loss: 0.7269\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.8330\n",
      "Epoch [20/100], Loss: 0.7778\n",
      "Epoch [30/100], Loss: 0.7331\n",
      "Epoch [40/100], Loss: 0.6970\n",
      "Epoch [50/100], Loss: 0.6690\n",
      "Epoch [60/100], Loss: 0.6480\n",
      "Epoch [70/100], Loss: 0.6324\n",
      "Epoch [80/100], Loss: 0.6211\n",
      "Epoch [90/100], Loss: 0.6129\n",
      "Epoch [100/100], Loss: 0.6069\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 1.1439\n",
      "Epoch [20/100], Loss: 0.9963\n",
      "Epoch [30/100], Loss: 0.9020\n",
      "Epoch [40/100], Loss: 0.8380\n",
      "Epoch [50/100], Loss: 0.7935\n",
      "Epoch [60/100], Loss: 0.7669\n",
      "Epoch [70/100], Loss: 0.7510\n",
      "Epoch [80/100], Loss: 0.7412\n",
      "Epoch [90/100], Loss: 0.7352\n",
      "Epoch [100/100], Loss: 0.7314\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.9161\n",
      "Epoch [20/100], Loss: 0.8595\n",
      "Epoch [30/100], Loss: 0.8103\n",
      "Epoch [40/100], Loss: 0.7675\n",
      "Epoch [50/100], Loss: 0.7311\n",
      "Epoch [60/100], Loss: 0.7011\n",
      "Epoch [70/100], Loss: 0.6771\n",
      "Epoch [80/100], Loss: 0.6583\n",
      "Epoch [90/100], Loss: 0.6436\n",
      "Epoch [100/100], Loss: 0.6323\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 1.1549\n",
      "Epoch [20/100], Loss: 0.9784\n",
      "Epoch [30/100], Loss: 0.8576\n",
      "Epoch [40/100], Loss: 0.7904\n",
      "Epoch [50/100], Loss: 0.7589\n",
      "Epoch [60/100], Loss: 0.7460\n",
      "Epoch [70/100], Loss: 0.7386\n",
      "Epoch [80/100], Loss: 0.7340\n",
      "Epoch [90/100], Loss: 0.7313\n",
      "Epoch [100/100], Loss: 0.7296\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.7385\n",
      "Epoch [20/100], Loss: 0.6943\n",
      "Epoch [30/100], Loss: 0.6614\n",
      "Epoch [40/100], Loss: 0.6383\n",
      "Epoch [50/100], Loss: 0.6226\n",
      "Epoch [60/100], Loss: 0.6120\n",
      "Epoch [70/100], Loss: 0.6049\n",
      "Epoch [80/100], Loss: 0.6002\n",
      "Epoch [90/100], Loss: 0.5969\n",
      "Epoch [100/100], Loss: 0.5947\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 0.8929\n",
      "Epoch [20/100], Loss: 0.8224\n",
      "Epoch [30/100], Loss: 0.7739\n",
      "Epoch [40/100], Loss: 0.7463\n",
      "Epoch [50/100], Loss: 0.7343\n",
      "Epoch [60/100], Loss: 0.7297\n",
      "Epoch [70/100], Loss: 0.7272\n",
      "Epoch [80/100], Loss: 0.7262\n",
      "Epoch [90/100], Loss: 0.7259\n",
      "Epoch [100/100], Loss: 0.7258\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.8418\n",
      "Epoch [20/100], Loss: 0.8018\n",
      "Epoch [30/100], Loss: 0.7682\n",
      "Epoch [40/100], Loss: 0.7390\n",
      "Epoch [50/100], Loss: 0.7142\n",
      "Epoch [60/100], Loss: 0.6930\n",
      "Epoch [70/100], Loss: 0.6750\n",
      "Epoch [80/100], Loss: 0.6600\n",
      "Epoch [90/100], Loss: 0.6475\n",
      "Epoch [100/100], Loss: 0.6370\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 0.9051\n",
      "Epoch [20/100], Loss: 0.7953\n",
      "Epoch [30/100], Loss: 0.7514\n",
      "Epoch [40/100], Loss: 0.7380\n",
      "Epoch [50/100], Loss: 0.7315\n",
      "Epoch [60/100], Loss: 0.7279\n",
      "Epoch [70/100], Loss: 0.7265\n",
      "Epoch [80/100], Loss: 0.7261\n",
      "Epoch [90/100], Loss: 0.7260\n",
      "Epoch [100/100], Loss: 0.7260\n",
      "Training Logistic Regression Model\n",
      "Epoch [10/100], Loss: 0.7724\n",
      "Epoch [20/100], Loss: 0.7240\n",
      "Epoch [30/100], Loss: 0.6864\n",
      "Epoch [40/100], Loss: 0.6585\n",
      "Epoch [50/100], Loss: 0.6385\n",
      "Epoch [60/100], Loss: 0.6243\n",
      "Epoch [70/100], Loss: 0.6142\n",
      "Epoch [80/100], Loss: 0.6070\n",
      "Epoch [90/100], Loss: 0.6017\n",
      "Epoch [100/100], Loss: 0.5978\n",
      "Training SVM Model\n",
      "Epoch [10/100], Loss: 0.9498\n",
      "Epoch [20/100], Loss: 0.8347\n",
      "Epoch [30/100], Loss: 0.7771\n",
      "Epoch [40/100], Loss: 0.7456\n",
      "Epoch [50/100], Loss: 0.7329\n",
      "Epoch [60/100], Loss: 0.7277\n",
      "Epoch [70/100], Loss: 0.7251\n",
      "Epoch [80/100], Loss: 0.7238\n",
      "Epoch [90/100], Loss: 0.7231\n",
      "Epoch [100/100], Loss: 0.7227\n",
      "INFO:tensorflow:Assets written to: ram://e3f8ce09-763e-49df-9490-664ff15cd26f/assets\n",
      "INFO:tensorflow:Assets written to: ram://1d1dacfa-55a9-4895-87e7-bfbd9437816b/assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['model/(bmi_age)_diabetes_svm_model.pkl']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_evaluations = 100\n",
    "logistic_metrics = []\n",
    "svm_metrics = []\n",
    "best_logistic_model = None\n",
    "best_svm_model = None\n",
    "best_logistic_score = -np.inf\n",
    "best_svm_score = -np.inf\n",
    "\n",
    "for _ in range(num_evaluations):\n",
    "    # Bootstrap sampling\n",
    "    X_resampled, y_resampled = resample(X_train, y_train, n_samples=len(X_train), random_state=None)\n",
    "    \n",
    "    # Convert resampled data to TensorFlow tensors\n",
    "    X_resampled_tensor = tf.convert_to_tensor(X_resampled, dtype=tf.float32)\n",
    "    y_resampled_tensor = tf.convert_to_tensor(y_resampled.values, dtype=tf.float32)\n",
    "    \n",
    "    # Reinitialize the models and optimizer for each iteration\n",
    "    lr_model = LogisticRegressionModel(input_dim)\n",
    "    svm_model = SVMModel(input_dim)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "    \n",
    "    # Train the logistic regression model\n",
    "    print(\"Training Logistic Regression Model\")\n",
    "    train_model(lr_model, X_resampled_tensor, y_resampled_tensor, binary_crossentropy)\n",
    "    \n",
    "    # Evaluate the logistic regression model\n",
    "    lr_metrics = evaluate_model(lr_model, X_test_tensor, y_test_tensor)\n",
    "    logistic_metrics.append(lr_metrics)\n",
    "    \n",
    "    # Save the best logistic regression model\n",
    "    if lr_metrics[0] > best_logistic_score:\n",
    "        best_logistic_score = lr_metrics[0]\n",
    "        best_logistic_model = lr_model\n",
    "    \n",
    "    # Train the SVM model\n",
    "    print(\"Training SVM Model\")\n",
    "    train_model(svm_model, X_resampled_tensor, y_resampled_tensor, hinge_loss)\n",
    "    \n",
    "    # Evaluate the SVM model\n",
    "    svm_metrics_evaluation = evaluate_model(svm_model, X_test_tensor, y_test_tensor)\n",
    "    svm_metrics.append(svm_metrics_evaluation)\n",
    "    \n",
    "    # Save the best SVM model\n",
    "    if svm_metrics_evaluation[0] > best_svm_score:\n",
    "        best_svm_score = svm_metrics_evaluation[0]\n",
    "        best_svm_model = svm_model\n",
    "\n",
    "# Save the best models\n",
    "joblib.dump(best_logistic_model, 'model/(bmi_age)_diabetes_logistic_model.pkl')\n",
    "joblib.dump(best_svm_model, 'model/(bmi_age)_diabetes_svm_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Confidence Interval (95%)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_confidence_interval(data, confidence=0.95):\n",
    "    n = len(data)\n",
    "    mean = np.mean(data)\n",
    "    se = sem(data)\n",
    "    margin_of_error = se * 1.96  # For 95% confidence interval\n",
    "    return mean, margin_of_error\n",
    "\n",
    "# Calculate average metrics for logistic regression\n",
    "logistic_metrics_array = np.array(logistic_metrics)\n",
    "logistic_mean_metrics = np.mean(logistic_metrics_array, axis=0)\n",
    "\n",
    "# Calculate average metrics for SVM\n",
    "svm_metrics_array = np.array(svm_metrics)\n",
    "svm_mean_metrics = np.mean(svm_metrics_array, axis=0)\n",
    "\n",
    "logistic_cis = [calculate_confidence_interval(logistic_metrics_array[:, i]) for i in range(logistic_metrics_array.shape[1])]\n",
    "svm_cis = [calculate_confidence_interval(svm_metrics_array[:, i]) for i in range(svm_metrics_array.shape[1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***EVALUTION RESULT***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Metric  Logistic Regression Mean Logistic Regression CI  \\\n",
      "0              Accuracy                  0.498550        0.4986  0.0000   \n",
      "1             Precision                  0.498550        0.4986  0.0000   \n",
      "2  Sensitivity (Recall)                  1.000000        1.0000  0.0000   \n",
      "3              F1 Score                  0.665377        0.6654  0.0000   \n",
      "4               ROC AUC                  0.732750        0.7328  0.0048   \n",
      "5    True Positive (TP)                  0.000000        0.0000  0.0000   \n",
      "6    True Negative (TN)               7090.000000     7090.0000  0.0000   \n",
      "7   False Positive (FP)                  0.000000        0.0000  0.0000   \n",
      "8   False Negative (FN)               7049.000000     7049.0000  0.0000   \n",
      "\n",
      "      SVM Mean               SVM CI  \n",
      "0     0.688844      0.6888  0.0007  \n",
      "1     0.668808      0.6688  0.0014  \n",
      "2     0.745188      0.7452  0.0026  \n",
      "3     0.704807      0.7048  0.0007  \n",
      "4     0.752392      0.7524  0.0008  \n",
      "5  4486.740000  4486.7400  24.3867  \n",
      "6  2603.260000  2603.2600  24.3867  \n",
      "7  1796.170000  1796.1700  18.1819  \n",
      "8  5252.830000  5252.8300  18.1819  \n"
     ]
    }
   ],
   "source": [
    "# Create a performance comparison table\n",
    "metrics_names = ['Accuracy', 'Precision', 'Sensitivity (Recall)', 'F1 Score', 'ROC AUC', 'TN', 'FP', 'FN', 'TP']\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Metric': metrics_names,\n",
    "    'Logistic Regression Mean': logistic_mean_metrics,\n",
    "    'Logistic Regression CI': [f'{ci[0]:.4f}  {ci[1]:.4f}' for ci in logistic_cis],\n",
    "    'SVM Mean': svm_mean_metrics,\n",
    "    'SVM CI': [f'{ci[0]:.4f}  {ci[1]:.4f}' for ci in svm_cis]\n",
    "})\n",
    "print(comparison_df)\n",
    "#save the comparison table to a csv file\n",
    "comparison_df.to_csv('performance/performance_comparison_[1].csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Cost Benefit Analysis (CBA)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost-Benefit Analysis for Logistic Regression: -41\n",
      "Cost-Benefit Analysis for SVM: -6681\n"
     ]
    }
   ],
   "source": [
    "# Define the cost-benefit analysis function\n",
    "def cost_benefit_analysis(tn, fp, fn, tp, cost_benefit_matrix):\n",
    "    return tn * cost_benefit_matrix[0][0] + fp * cost_benefit_matrix[0][1] + fn * cost_benefit_matrix[1][0] + tp * cost_benefit_matrix[1][1]\n",
    "\n",
    "\n",
    "# Define a hypothetical cost-benefit matrix\n",
    "cost_benefit_matrix = np.array([[0, -1], [-5, 1]])\n",
    "# Perform cost-benefit analysis for the best models\n",
    "best_lr_metrics = evaluate_model(best_logistic_model, X_test_tensor, y_test_tensor)\n",
    "best_svm_metrics = evaluate_model(best_svm_model, X_test_tensor, y_test_tensor)\n",
    "\n",
    "lr_cost_benefit = cost_benefit_analysis(*best_lr_metrics[5:], cost_benefit_matrix)\n",
    "svm_cost_benefit = cost_benefit_analysis(*best_svm_metrics[5:], cost_benefit_matrix)\n",
    "\n",
    "print(f'Cost-Benefit Analysis for Logistic Regression: {lr_cost_benefit}')\n",
    "print(f'Cost-Benefit Analysis for SVM: {svm_cost_benefit}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***ROC Plot***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logistic_probabilities = tf.sigmoid(best_logistic_model(X_test_tensor)).numpy()\n",
    "svm_probabilities = tf.sigmoid(best_svm_model(X_test_tensor)).numpy()\n",
    "\n",
    "logistic_fpr, logistic_tpr, _ = roc_curve(y_test_tensor, logistic_probabilities)\n",
    "svm_fpr, svm_tpr, _ = roc_curve(y_test_tensor, svm_probabilities)\n",
    "\n",
    "logistic_auc = auc(logistic_fpr, logistic_tpr)\n",
    "svm_auc = auc(svm_fpr, svm_tpr)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(logistic_fpr, logistic_tpr, label=f'Logistic Regression (AUC = {logistic_auc:.2f})')\n",
    "plt.plot(svm_fpr, svm_tpr, label=f'SVM (AUC = {svm_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#end of code\n",
    "#release gpu memory\n",
    "from numba import cuda\n",
    "cuda.select_device(0)\n",
    "cuda.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
